[{"content":"unattended-upgrades 란? unattended-upgrades 는 Ubuntu system 의 최신 보안 패치 및 기타 업데이트를 자동으로 수행하고 시스템을 유지, 관리 하는 것에 목적이 있는 서비스 입니다.\n설치 Ubuntu 를 설치하면 기본적으로 해당 서비스는 설치되어 작동하고 있습니다.\n만약 설치가 안되어 있다면 아래와 같이 설치를 진행합니다.\n$ sudo apt install unattended-upgrades -y 구동중인 daemon 정보는 아래와 같습니다.\n$ sudo systemctl status unattended-upgrades ● unattended-upgrades.service - Unattended Upgrades Shutdown  Loaded: loaded (/lib/systemd/system/unattended-upgrades.service; enabled; vendor preset: enabled)  Active: active (running) since Thu 2022-04-28 14:26:19 KST; 3 days ago  Docs: man:unattended-upgrade(8)  Main PID: 681 (unattended-upgr)  Tasks: 2 (limit: 9495)  Memory: 14.5M  CPU: 65ms  CGroup: /system.slice/unattended-upgrades.service  └─681 /usr/bin/python3 /usr/share/unattended-upgrades/unattended-upgrade-shutdown --wait-for-signal  Apr 28 14:26:19 u-node-1 systemd[1]: Started Unattended Upgrades Shutdown. 설정 unattended-upgrades 에 Default 로 들어가는 설정은 아래와 같습니다.\n$ cat /etc/apt/apt.conf.d/50unattended-upgrades | egrep -v \u0026#34;//|^$\u0026#34; Unattended-Upgrade::Allowed-Origins {  \u0026#34;${distro_id}:${distro_codename}\u0026#34;;  \u0026#34;${distro_id}:${distro_codename}-security\u0026#34;;  \u0026#34;${distro_id}ESMApps:${distro_codename}-apps-security\u0026#34;;  \u0026#34;${distro_id}ESM:${distro_codename}-infra-security\u0026#34;; }; Unattended-Upgrade::Package-Blacklist { }; Unattended-Upgrade::DevRelease \u0026#34;auto\u0026#34;; 기본적으로 설정 되있는 것을 보면 4개의 Repository 에 대해서는 Update 가 Allow 되어 있는 것을 볼 수 있습니다.\n특별히 추가한 Repository 가 없다면 Update 대상이 되는 REpository 는 jammy, jammy-security 가 될 것 입니다. (Ubuntu 22.04 기준)\n서비스 Enable / Disable 안정적인 운영을 요구하는 시스템이라면 해당 서비스를 Disable 하는 것이 좋습니다.\nPackage 의 업데이트가 운영중인 서비스에 영향이 가는 것을 막기 위함입니다.\nDisable 을 위해서는 아래 명령을 이용하면 됩니다.\n$ sudo dpkg-reconfigure unattended-upgrades 위 명령을 입력하면 아래와 같은 Text UI 가 확인이 되고 \u0026lt;No\u0026gt; 를 선택하면 서비스가 Disable 됩니다.\n 상세 설정은 아래와 같이 0 로 변경됩니다.\nubuntu@chhan-u2204:~$ sudo dpkg-reconfigure unattended-upgrades Replacing config file /etc/apt/apt.conf.d/20auto-upgrades with new version  ubuntu@chhan-u2204:~$ cat /etc/apt/apt.conf.d/20auto-upgrades APT::Periodic::Update-Package-Lists \u0026#34;0\u0026#34;; APT::Periodic::Unattended-Upgrade \u0026#34;0\u0026#34;; Enable 을 위해서는 Disable 에서 사용한 명령을 그대로 사용하고 \u0026lt;Yes\u0026gt; 를 선택하면 서비스가 Enable 됩니다.\n상세 설정은 아래와 같이 1 로 변경됩니다.\nubuntu@chhan-u2204:~$ sudo dpkg-reconfigure unattended-upgrades Replacing config file /etc/apt/apt.conf.d/20auto-upgrades with new version  ubuntu@chhan-u2204:~$ cat /etc/apt/apt.conf.d/20auto-upgrades APT::Periodic::Update-Package-Lists \u0026#34;1\u0026#34;; APT::Periodic::Unattended-Upgrade \u0026#34;1\u0026#34;; 주요 기능 unattended-upgrades 에 포함된 기능 중, 활용하기 좋은 설정 부분에 대해 알아보도록 하겠습니다.\nBlacklist 기능 /etc/apt/apt.conf.d/50unattended-upgrades 의 Unattended-Upgrade::Package-Blacklist 영역에 내용을 추가하여 unattended-upgrades 서비스가 특정 Package 를 Update 못하도록 하는 설정을 할 수 있습니다.\nUnattended-Upgrade::Package-Blacklist {  // The following matches all packages starting with linux- // \u0026#34;linux-\u0026#34;;   // Use $ to explicitely define the end of a package name. Without  // the $, \u0026#34;libc6\u0026#34; would match all of them. // \u0026#34;libc6$\u0026#34;; // \u0026#34;libc6-dev$\u0026#34;; // \u0026#34;libc6-i686$\u0026#34;;   // Special characters need escaping // \u0026#34;libstdc\\+\\+6$\u0026#34;;   // The following matches packages like xen-system-amd64, xen-utils-4.1,  // xenstore-utils and libxenstore3.0 // \u0026#34;(lib)?xen(store)?\u0026#34;;   // For more information about Python regular expressions, see  // https://docs.python.org/3/howto/regex.html }; 한가지 예로 nginx Package 에 대해 자동으로 update 가 안되도록 하기 위해서는 아래와 같이 설정하면 됩니다.\nUnattended-Upgrade::Package-Blacklist {  \u0026#34;nginx\u0026#34;; }; Auto-Reboot 주기적으로 시스템을 Update 하고 자동으로 재부팅을 진행하고 싶은 경우, 아래 설정을 이용 할 수 있습니다.\n// Automatically reboot *WITHOUT CONFIRMATION* if // the file /var/run/reboot-required is found after the upgrade //Unattended-Upgrade::Automatic-Reboot \u0026#34;false\u0026#34;;  // Automatically reboot even if there are users currently logged in // when Unattended-Upgrade::Automatic-Reboot is set to true //Unattended-Upgrade::Automatic-Reboot-WithUsers \u0026#34;true\u0026#34;;  // If automatic reboot is enabled and needed, reboot at the specific // time instead of immediately // Default: \u0026#34;now\u0026#34; //Unattended-Upgrade::Automatic-Reboot-Time \u0026#34;02:00\u0026#34;; Unattended-Upgrade::Automatic-Reboot 재부팅 여부를 설정 할 수 있고,\nUnattended-Upgrade::Automatic-Reboot-WithUsers 로그인한 사용자에 대한 재부팅 여부 설정,\nUnattended-Upgrade::Automatic-Reboot-Time 재부팅 시간을 설정 할 수 있습니다.\n작동 확인 기본적으로 unattended-upgrades 는 매일 06:25 분에 수행되는 cron.daily 에 등록 되어 있습니다.\nubuntu@chhan-u2204:~$ cat /etc/crontab | grep daily 25 6 * * * root test -x /usr/sbin/anacron || ( cd / \u0026amp;\u0026amp; run-parts --report /etc/cron.daily ) (위 시간은 각 시스템에 설정에 따라 차이가 있을 수 있습니다.)\n수행하는 script 는 아래 부분입니다.\nubuntu@chhan-u2204:~$ cat /etc/cron.daily/apt-compat | tail -n1 exec /usr/lib/apt/apt.systemd.daily 작동 로그 자동으로 update 가 완료되는 아래와 경로에 로그가 생성되고 확인이 가능합니다.\n$ cat /var/log/unattended-upgrades/unattended-upgrades-dpkg.log | more Log started: 2022-05-04 06:29:22 (Reading database ... Preparing to unpack .../libinput-bin_1.20.0-1ubuntu0.1_amd64.deb ... Unpacking libinput-bin (1.20.0-1ubuntu0.1) over (1.20.0-1) ... Preparing to unpack .../libinput10_1.20.0-1ubuntu0.1_amd64.deb ... Unpacking libinput10:amd64 (1.20.0-1ubuntu0.1) over (1.20.0-1) ... Setting up libinput-bin (1.20.0-1ubuntu0.1) ... Setting up libinput10:amd64 (1.20.0-1ubuntu0.1) ... Processing triggers for libc-bin (2.35-0ubuntu3) ... NEEDRESTART-VER: 3.5 NEEDRESTART-KCUR: 5.15.0-25-generic NEEDRESTART-KEXP: 5.15.0-27-generic NEEDRESTART-KSTA: 3 NEEDRESTART-SVC: NetworkManager.service ... 생략 apt history.log 에도 해당 내용이 기록됩니다.\n$ cat /var/log/apt/history.log ... 생략 Start-Date: 2022-05-04 06:29:23 Commandline: /usr/bin/unattended-upgrade Upgrade: libinput10:amd64 (1.20.0-1, 1.20.0-1ubuntu0.1), libinput-bin:amd64 (1.20.0-1, 1.20.0-1ubuntu0.1) End-Date: 2022-05-04 06:29:23 ... 생략 참고 자료  https://linuxhint.com/enable-disable-unattended-upgrades-ubuntu/  ","permalink":"https://chhanz88.github.io/post/2022-05-11-ubuntu-unattended-upgrades/","summary":"unattended-upgrades 란? unattended-upgrades 는 Ubuntu system 의 최신 보안 패치 및 기타 업데이트를 자동으로 수행하고 시스템을 유지, 관리 하는 것에 목적이 있는 서비스 입니다.\n설치 Ubuntu 를 설치하면 기본적으로 해당 서비스는 설치되어 작동하고 있습니다.\n만약 설치가 안되어 있다면 아래와 같이 설치를 진행합니다.\n$ sudo apt install unattended-upgrades -y 구동중인 daemon 정보는 아래와 같습니다.\n$ sudo systemctl status unattended-upgrades ● unattended-upgrades.service - Unattended Upgrades Shutdown  Loaded: loaded (/lib/systemd/system/unattended-upgrades.service; enabled; vendor preset: enabled)  Active: active (running) since Thu 2022-04-28 14:26:19 KST; 3 days ago  Docs: man:unattended-upgrade(8)  Main PID: 681 (unattended-upgr)  Tasks: 2 (limit: 9495)  Memory: 14.","title":"[Ubuntu] unattended-upgrades 설정 (자동 업데이트)"},{"content":"Ubuntu 22.04 - APT Repository 사용법 Repository Component 종류 Ubuntu 에서 사용되는 Repository Component 는 아래와 같습니다.\n  Main : Ubuntu 무료 및 오픈소스 소프트웨어 Universe : Linux 커뮤니티의 무료 및 오픈소스 소프트웨어 Restricted : Vender 장치 드라이버 및 소프트웨어 Multiverse : 법적 제한(저작권 등)이 있는 소프트웨어   Default Repository Ubuntu 가 설치되면 기본적으로 아래와 같이 /etc/apt/sources.list 파일로 Repository 가 관리됩니다.\nroot@u-node-1:/etc/apt/sources.list.d# cat /etc/apt/sources.list | egrep -v \u0026#34;#|^$\u0026#34; deb http://archive.ubuntu.com/ubuntu jammy main restricted deb http://archive.ubuntu.com/ubuntu jammy-updates main restricted deb http://archive.ubuntu.com/ubuntu jammy universe deb http://archive.ubuntu.com/ubuntu jammy-updates universe deb http://archive.ubuntu.com/ubuntu jammy multiverse deb http://archive.ubuntu.com/ubuntu jammy-updates multiverse deb http://archive.ubuntu.com/ubuntu jammy-backports main restricted universe multiverse deb http://security.ubuntu.com/ubuntu jammy-security main restricted deb http://security.ubuntu.com/ubuntu jammy-security universe deb http://security.ubuntu.com/ubuntu jammy-security multiverse sources.list 의 구조 sources.list 는 아래와 같은 구조로 작성됩니다.\n\u0026lt;package version\u0026gt; \u0026lt;mirror site url\u0026gt; \u0026lt;release code\u0026gt; \u0026lt;repository component, component, component..........\u0026gt; Package Version 은 deb , deb-i386 , deb-amd64 3가지 Version 이 있습니다.\ndeb 는 현재 OS Bit (ex. 64bit) 와 일치하는 Package version 을 뜻하며,\ndeb-i386, deb-amd64 는 각각 32bit, 64bit 를 뜻합니다.\nsources.list manual configuration /etc/apt/sources.list 혹은 /etc/apt/sources.list.d/ 에 위 구조를 참고하여 수동으로 Repository 를 추가하면 됩니다.\n아래는 수동으로 카카오 Repository 를 추가한 상태입니다.\nroot@u-node-1:/etc/apt/sources.list.d# cat /etc/apt/sources.list.d/kr.list deb https://mirror.kakao.com/ubuntu/ jammy main restricted apt-add-repository command 를 이용한 설정 아래 내용은 Command 를 이용하여 Repository 를 관리하는 방법에 대하여 설명하고 있습니다.\n현재 Repository 목록 apt-add-repository or add-apt-repository 명령어를 이용하여 확인이 가능합니다.\n목록을 확인하는 옵션은 -L 입니다. (Ubuntu 22.04)\nroot@u-node-1:~# apt-add-repository -L deb http://archive.ubuntu.com/ubuntu jammy universe main restricted multiverse deb http://archive.ubuntu.com/ubuntu jammy-updates universe main restricted multiverse deb http://archive.ubuntu.com/ubuntu jammy-backports universe main restricted multiverse deb http://security.ubuntu.com/ubuntu jammy-security universe main restricted multiverse deb https://mirror.kakao.com/ubuntu/ jammy main restricted Repository 추가 -U 옵션을 통해 Repositoty 를 추가 및 업데이트가 가능합니다. (Ubuntu 22.04)\nroot@u-node-1:~# apt-add-repository -U https://ftp.lanet.kr/ubuntu/ Repository: \u0026#39;deb https://ftp.lanet.kr/ubuntu/ jammy main\u0026#39; Description: Archive for codename: jammy components: main More info: https://ftp.lanet.kr/ubuntu/ Adding repository. Press [ENTER] to continue or Ctrl-c to cancel. Adding deb entry to /etc/apt/sources.list.d/archive_uri-https_ftp_lanet_kr_ubuntu_-jammy.list Adding disabled deb-src entry to /etc/apt/sources.list.d/archive_uri-https_ftp_lanet_kr_ubuntu_-jammy.list Hit:1 https://mirror.kakao.com/ubuntu jammy InRelease Get:2 https://ftp.lanet.kr/ubuntu jammy InRelease [270 kB] Get:3 https://ftp.lanet.kr/ubuntu jammy/main amd64 Packages [1395 kB] Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease Get:5 https://ftp.lanet.kr/ubuntu jammy/main Translation-en [510 kB] Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease Get:7 https://ftp.lanet.kr/ubuntu jammy/main amd64 DEP-11 Metadata [423 kB] Get:8 https://ftp.lanet.kr/ubuntu jammy/main DEP-11 48x48 Icons [100.0 kB] Hit:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease Get:10 https://ftp.lanet.kr/ubuntu jammy/main DEP-11 64x64 Icons [148 kB] Get:11 https://ftp.lanet.kr/ubuntu jammy/main amd64 c-n-f Metadata [30.3 kB] Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease Fetched 2876 kB in 1s (2090 kB/s) Reading package lists... Done  root@u-node-1:~# apt-add-repository -L deb http://archive.ubuntu.com/ubuntu jammy universe restricted multiverse main deb http://archive.ubuntu.com/ubuntu jammy-updates universe restricted multiverse main deb http://archive.ubuntu.com/ubuntu jammy-backports universe restricted multiverse main deb http://security.ubuntu.com/ubuntu jammy-security universe restricted multiverse main deb https://mirror.kakao.com/ubuntu/ jammy restricted main deb https://ftp.lanet.kr/ubuntu/ jammy main \u0026lt;\u0026lt; \u0026#34;추가된 Repository\u0026#34; 특정 Component 추가 -U 옵션과 -c 옵션을 추가하여 Component 를 추가 할 수 있습니다.\nroot@u-node-1:~# apt-add-repository -U https://ftp.lanet.kr/ubuntu/ -c multiverse Repository: \u0026#39;deb https://ftp.lanet.kr/ubuntu/ jammy multiverse\u0026#39; Description: Archive for codename: jammy components: multiverse More info: https://ftp.lanet.kr/ubuntu/ Adding repository. Press [ENTER] to continue or Ctrl-c to cancel. Archive has template, updating /etc/apt/sources.list Updating existing entry instead of using /etc/apt/sources.list Hit:1 https://mirror.kakao.com/ubuntu jammy InRelease Hit:2 https://ftp.lanet.kr/ubuntu jammy InRelease Get:3 https://ftp.lanet.kr/ubuntu jammy/multiverse amd64 Packages [217 kB] Get:4 https://ftp.lanet.kr/ubuntu jammy/multiverse Translation-en [112 kB] Get:5 https://ftp.lanet.kr/ubuntu jammy/multiverse amd64 DEP-11 Metadata [42.1 kB] Get:6 https://ftp.lanet.kr/ubuntu jammy/multiverse DEP-11 48x48 Icons [42.7 kB] Get:7 https://ftp.lanet.kr/ubuntu jammy/multiverse DEP-11 64x64 Icons [193 kB] Get:8 https://ftp.lanet.kr/ubuntu jammy/multiverse amd64 c-n-f Metadata [8372 B] Hit:9 http://security.ubuntu.com/ubuntu jammy-security InRelease Hit:10 http://archive.ubuntu.com/ubuntu jammy InRelease Hit:11 http://archive.ubuntu.com/ubuntu jammy-updates InRelease Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease Fetched 615 kB in 2s (395 kB/s) Reading package lists... Done 이후 아래와 같이 Component 가 추가 된 것을 확인 할 수 있습니다.\nroot@u-node-1:~# cat /etc/apt/sources.list | tail -n1 deb https://ftp.lanet.kr/ubuntu/ jammy main multiverse Repository 제거 (특정 component 제거) 아래와 같이 -r 옵션을 이용하여 Repository 를 제거 할 수 있습니다.\nroot@u-node-1:~# apt-add-repository -r https://mirror.kakao.com/ubuntu -c restricted Repository: \u0026#39;deb https://mirror.kakao.com/ubuntu jammy restricted\u0026#39; Description: Archive for codename: jammy components: restricted More info: https://mirror.kakao.com/ubuntu Removing repository. Press [ENTER] to continue or Ctrl-c to cancel. Disabling deb entry in /etc/apt/sources.list.d/archive_uri-https_mirror_kakao_com_ubuntu-jammy.list Hit:1 https://mirror.kakao.com/ubuntu jammy InRelease Hit:2 https://ftp.lanet.kr/ubuntu jammy InRelease Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease Reading package lists... Done -c 옵션을 추가하면 특정 component 만 제거가 가능합니다.\n","permalink":"https://chhanz88.github.io/post/2022-05-09-ubuntu-apt/","summary":"Ubuntu 22.04 - APT Repository 사용법 Repository Component 종류 Ubuntu 에서 사용되는 Repository Component 는 아래와 같습니다.\n  Main : Ubuntu 무료 및 오픈소스 소프트웨어 Universe : Linux 커뮤니티의 무료 및 오픈소스 소프트웨어 Restricted : Vender 장치 드라이버 및 소프트웨어 Multiverse : 법적 제한(저작권 등)이 있는 소프트웨어   Default Repository Ubuntu 가 설치되면 기본적으로 아래와 같이 /etc/apt/sources.list 파일로 Repository 가 관리됩니다.\nroot@u-node-1:/etc/apt/sources.list.d# cat /etc/apt/sources.list | egrep -v \u0026#34;#|^$\u0026#34; deb http://archive.","title":"[Ubuntu] APT Repository 사용법"},{"content":"apt-mirror 를 이용하여 sync 후, apt-get error 발생 apt-mirror 를 이용하여 Repository 를 sync 하고 아래와 같은 에러가 발생했습니다.\n... 생략 Ign:22 http://localhost:8000/ubuntu jammy/multiverse amd64 c-n-f Metadata Err:7 http://localhost:8000/ubuntu jammy/main amd64 c-n-f Metadata  404 File not found [IP: 127.0.0.1 8000] \u0026lt;\u0026lt;\u0026lt; Ign:10 http://localhost:8000/ubuntu jammy/restricted amd64 c-n-f Metadata Ign:16 http://localhost:8000/ubuntu jammy/universe amd64 c-n-f Metadata Ign:22 http://localhost:8000/ubuntu jammy/multiverse amd64 c-n-f Metadata Fetched 38.0 MB in 0s (128 MB/s) Reading package lists... Done E: Failed to fetch http://localhost:8000/ubuntu/dists/jammy/main/cnf/Commands-amd64 404 File not found [IP: 127.0.0.1 8000] \u0026lt;\u0026lt;\u0026lt; E: Some index files failed to download. They have been ignored, or old ones used instead. ... 생략 위와 같이 file not found 가 발생되는데 이런 원인은 i386 및 amd64 패키지가 sync 안되는 경우에도 발생 할 수 있습니다.\n하지만 i386 및 amd64 모든 Version 을 sync 해도 위와 같이 File not found 가 발생되는 경우는 아래와 같습니다.\n발생 원인 우리가 Repository 를 sync 할 때, 사용하는 apt-mirror 의 명령어(https://github.com/apt-mirror/apt-mirror)는 Last commit 이 3 year 전 입니다.\n새로운 maintainer 를 구한다고 readme 에 작성이 되어 있습니다.\n이런 이유로 신규 Ubuntu Release 에 해당 명령어가 정상적으로 작동을 안하는 이슈가 있습니다.\n해결 방안 공식 apt-mirror 를 fork 하여 수정한 https://github.com/Stifler6996/apt-mirror 를 이용하면 이러한 이슈들이 해결됩니다.\nfork apt-mirror 사용법 아래와 같이 진행하여 명령어를 수정합니다.\n\u0026#34;official apt-mirror 설치\u0026#34; $ sudo apt install apt-mirror  \u0026#34;git clone\u0026#34; $ git clone https://github.com/Stifler6996/apt-mirror  \u0026#34;backup original command\u0026#34; $ sudo mv /usr/bin/apt-mirror /usr/bin/apt-mirror.backup  \u0026#34;install fork apt-mirror\u0026#34; $ sudo cp apt-mirror/apt-mirror /usr/bin/apt-mirror $ sudo chmod 755 /usr/bin/apt-mirror ; sudo chown root:root /usr/bin/apt-mirror 이슈 해결 확인 fork apt-mirror 를 사용하여 추가로 repository 를 sync 받고 확인하면 신규 Ubuntu Release 도 정상적으로 repository 가 sync 된 것을 확인 할 수 있습니다.\nroot@u-node-1:~# apt-get update Get:1 http://localhost:8000/ubuntu jammy InRelease [270 kB] Get:2 http://localhost:8000/ubuntu jammy/main amd64 Packages [1395 kB] Get:3 http://localhost:8000/ubuntu jammy/main Translation-en [510 kB] Get:4 http://localhost:8000/ubuntu jammy/main amd64 DEP-11 Metadata [423 kB] Get:5 http://localhost:8000/ubuntu jammy/main DEP-11 48x48 Icons [100.0 kB] Get:6 http://localhost:8000/ubuntu jammy/main DEP-11 64x64 Icons [148 kB] Ign:7 http://localhost:8000/ubuntu jammy/main amd64 c-n-f Metadata Get:8 http://localhost:8000/ubuntu jammy/restricted amd64 Packages [129 kB] Get:9 http://localhost:8000/ubuntu jammy/restricted Translation-en [18.6 kB] Ign:10 http://localhost:8000/ubuntu jammy/restricted amd64 c-n-f Metadata Get:11 http://localhost:8000/ubuntu jammy/universe amd64 Packages [14.1 MB] Get:12 http://localhost:8000/ubuntu jammy/universe Translation-en [5652 kB] Get:13 http://localhost:8000/ubuntu jammy/universe amd64 DEP-11 Metadata [3559 kB] Get:14 http://localhost:8000/ubuntu jammy/universe DEP-11 48x48 Icons [3447 kB] Get:15 http://localhost:8000/ubuntu jammy/universe DEP-11 64x64 Icons [7609 kB] Ign:16 http://localhost:8000/ubuntu jammy/universe amd64 c-n-f Metadata Get:17 http://localhost:8000/ubuntu jammy/multiverse amd64 Packages [217 kB] Get:18 http://localhost:8000/ubuntu jammy/multiverse Translation-en [112 kB] Get:19 http://localhost:8000/ubuntu jammy/multiverse amd64 DEP-11 Metadata [42.1 kB] Get:20 http://localhost:8000/ubuntu jammy/multiverse DEP-11 48x48 Icons [42.7 kB] Get:21 http://localhost:8000/ubuntu jammy/multiverse DEP-11 64x64 Icons [193 kB] Ign:22 http://localhost:8000/ubuntu jammy/multiverse amd64 c-n-f Metadata Get:7 http://localhost:8000/ubuntu jammy/main amd64 c-n-f Metadata [30.3 kB] Get:10 http://localhost:8000/ubuntu jammy/restricted amd64 c-n-f Metadata [488 B] Get:16 http://localhost:8000/ubuntu jammy/universe amd64 c-n-f Metadata [286 kB] Get:22 http://localhost:8000/ubuntu jammy/multiverse amd64 c-n-f Metadata [8372 B] Fetched 595 kB in 3s (199 kB/s) Reading package lists... Done 참고 자료  https://askubuntu.com/questions/1252828/apt-mirror-for-amd64-did-not-include-focal-main-dep11-and-focal-main-cnf-command  ","permalink":"https://chhanz88.github.io/post/2022-05-09-ubuntu-apt-mirror-issue/","summary":"apt-mirror 를 이용하여 sync 후, apt-get error 발생 apt-mirror 를 이용하여 Repository 를 sync 하고 아래와 같은 에러가 발생했습니다.\n... 생략 Ign:22 http://localhost:8000/ubuntu jammy/multiverse amd64 c-n-f Metadata Err:7 http://localhost:8000/ubuntu jammy/main amd64 c-n-f Metadata  404 File not found [IP: 127.0.0.1 8000] \u0026lt;\u0026lt;\u0026lt; Ign:10 http://localhost:8000/ubuntu jammy/restricted amd64 c-n-f Metadata Ign:16 http://localhost:8000/ubuntu jammy/universe amd64 c-n-f Metadata Ign:22 http://localhost:8000/ubuntu jammy/multiverse amd64 c-n-f Metadata Fetched 38.0 MB in 0s (128 MB/s) Reading package lists... Done E: Failed to fetch http://localhost:8000/ubuntu/dists/jammy/main/cnf/Commands-amd64 404 File not found [IP: 127.","title":"[Ubuntu] apt-mirror 를 이용하여 sync 후, apt-get error 발생"},{"content":"목차  overview DVD 사용  Mount DVD sources.list 추가 Repository 추가   Sync Repository  apt-mirror 설치 mirror.list 수정 Sync Repository Check Sync mirror data location   Mirror site 운영  Mirror site 추가 APT update    overview 이번 포스팅은 DVD 를 활용하여 인터넷이 안되는 환경에서 Ubuntu package 를 다루는 방법과\n외부 환경이 아닌 내부에 Repository mirror site 를 구성하는 방법에 대해 알아보도록 하겠습니다.\nDVD 사용 주로 인터넷이 안되는 망분리 된 곳에서 많이 활용될 방식입니다.\nMount DVD 아래 명령어를 통해 DVD 를 Mount 합니다.\n$ sudo mount /dev/sr0 /mnt mount: /mnt: WARNING: source write-protected, mounted read-only. sources.list 추가 아래와 같이 /etc/apt/sources.list.d/ 경로에 list 파일을 추가합니다.\n$ cat /etc/apt/sources.list.d/dvd.list deb file:/mnt/ubuntu jammy main 외부와 통신이 안되는 상황이라면 외부로 설정 되어 있는 Repository 설정을 주석으로 막거나 제거합니다.\n$ vi /etc/apt/sources.list # deb ...... ....... \u0026lt;\u0026lt; 주석 처리 ... Repository 추가 아래와 같이 Repository 목록을 update 합니다.\nroot@u-node-1:/etc/apt/sources.list.d# apt-get update Get:1 file:/mnt/ubuntu jammy InRelease Ign:1 file:/mnt/ubuntu jammy InRelease Get:2 file:/mnt/ubuntu jammy Release [1486 B] Get:2 file:/mnt/ubuntu jammy Release [1486 B] Get:3 file:/mnt/ubuntu jammy Release.gpg [833 B] Get:3 file:/mnt/ubuntu jammy Release.gpg [833 B] Get:4 file:/mnt/ubuntu jammy/main amd64 Packages [32.3 kB] Ign:4 file:/mnt/ubuntu jammy/main amd64 Packages Get:4 file:/mnt/ubuntu jammy/main amd64 Packages [32.3 kB] Hit:5 https://ftp.lanet.kr/ubuntu jammy InRelease Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease Reading package lists... Done  root@u-node-1:/etc/apt/sources.list.d# apt-add-repository -L deb http://archive.ubuntu.com/ubuntu jammy multiverse main universe restricted deb http://archive.ubuntu.com/ubuntu jammy-updates multiverse main universe restricted deb http://archive.ubuntu.com/ubuntu jammy-backports multiverse main universe restricted deb http://security.ubuntu.com/ubuntu jammy-security multiverse main universe restricted deb https://ftp.lanet.kr/ubuntu/ jammy multiverse main universe deb file:/mnt/ubuntu jammy main \u0026lt;\u0026lt;\u0026lt; \u0026#34;추가된 local DVD repository\u0026#34; Sync Repository  내부 망 분리된 환경에 한대의 시스템에 Repository 를 Sync 하고 Sync 된 시스템에서 HTTP 서비스를 하여 Repository mirror site 를 구성하고 이용하는 방법에 대해 알아보도록 하겠습니다.\napt-mirror 설치 아래와 같이 apt-mirror 를 설치합니다.\n주의 : 공식 apt-mirror 는 최신 버전의 Ubuntu Release 를 정상적으로 Sync 하지 못하고 있습니다.\n(사유: 해당 명령어가 유지보수가 안되고 있음)\n관련하여 issue 및 workaround 는 아래 문서를 참고합니다.\nhttps://chhanz.github.io/linux/2022/05/09/ubuntu-apt-mirror-issue/\nmirror.list 수정 /etc/apt/mirror.list 를 수정하여 mirror 할 내용을 점검합니다.\n$ cat /etc/apt/mirror.list ############# config ################## # # set base_path /var/spool/apt-mirror # # set mirror_path $base_path/mirror # set skel_path $base_path/skel # set var_path $base_path/var # set cleanscript $var_path/clean.sh # set defaultarch \u0026lt;running host architecture\u0026gt; # set postmirror_script $var_path/postmirror.sh # set run_postmirror 0 set nthreads 5 set _tilde 0 # ############# end config ##############  deb http://archive.ubuntu.com/ubuntu jammy main restricted universe multiverse deb-amd64 http://archive.ubuntu.com/ubuntu jammy main restricted universe multiverse deb-i386 http://archive.ubuntu.com/ubuntu jammy main restricted universe multiverse #deb http://archive.ubuntu.com/ubuntu jammy-security main restricted universe multiverse #deb http://archive.ubuntu.com/ubuntu jammy-updates main restricted universe multiverse #deb http://archive.ubuntu.com/ubuntu artful-proposed main restricted universe multiverse #deb http://archive.ubuntu.com/ubuntu artful-backports main restricted universe multiverse  #deb-src http://archive.ubuntu.com/ubuntu artful main restricted universe multiverse #deb-src http://archive.ubuntu.com/ubuntu artful-security main restricted universe multiverse #deb-src http://archive.ubuntu.com/ubuntu artful-updates main restricted universe multiverse #deb-src http://archive.ubuntu.com/ubuntu artful-proposed main restricted universe multiverse #deb-src http://archive.ubuntu.com/ubuntu artful-backports main restricted universe multiverse  clean http://archive.ubuntu.com/ubuntu 간단하게 주요 옵션들에 대해 정리하면,\n base_path : mirror site data 저장 경로\nnthreads : sync 간 사용될 wget thread 수 deb , deb-amd64 , deb-i386 : 현재 사용하는 OS 의 Bit, 64 Bit, 32 Bit\n Sync Repository 아래와 같이 Repository 를 Sync 합니다.\n$ sudo apt-mirror Downloading 114 index files using 5 threads... Begin time: Mon May 9 10:54:15 2022 [5]... [4]... [3]... [2]... [1]... [0]... End time: Mon May 9 10:54:21 2022  Processing translation indexes: [TTT]  Downloading 558 translation files using 5 threads... Begin time: Mon May 9 10:54:21 2022 [5]... [4]... [3]... [2]... [1]... [0]... End time: Mon May 9 10:54:51 2022  Processing DEP-11 indexes: [DDD]  Downloading 56 dep11 files using 5 threads... Begin time: Mon May 9 10:54:51 2022 [5]... [4]... [3]... [2]... [1]... [0]... End time: Mon May 9 10:54:55 2022  Processing indexes: [PPP]  3.5 GiB will be downloaded into archive. Downloading 422 archive files using 5 threads... Begin time: Mon May 9 10:54:58 2022 [5]... [4]... [3]... [2]... [1]... [0]... End time: Mon May 9 10:57:47 2022  16.0 KiB in 1 files and 0 directories can be freed. Run /var/spool/apt-mirror/var/clean.sh for this purpose.  Running the Post Mirror script ... (/var/spool/apt-mirror/var/postmirror.sh)   Post Mirror script has completed. See above output for any possible errors. Check Sync mirror data location 기본적으로 base_path 를 설정 안했다면 아래와 같은 경로로 mirror site sync 가 완료 되어있습니다.\nroot@u-node-1:/var/spool/apt-mirror/mirror/archive.ubuntu.com# ls ubuntu root@u-node-1:/var/spool/apt-mirror/mirror/archive.ubuntu.com# pwd /var/spool/apt-mirror/mirror/archive.ubuntu.com Mirror site 운영 apache 혹은 nginx 를 사용하는 것이 제일 좋은 방법이고 아래와 같이 간단하게 명령어 한줄로도 Mirror site 를 구동 할 수 있습니다.\nPython Simple HTTP Server 모듈을 이용합니다.\nroot@u-node-1:/var/spool/apt-mirror/mirror/archive.ubuntu.com# python3 -m http.server Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ... Mirror site 추가 아래와 같이 내부 Mirror site 정보를 추가합니다.\nroot@u-node-1:/etc/apt# cat sources.list deb http://localhost:8000/ubuntu jammy main restricted universe multiverse APT update 아래와 같이 정상적으로 내부 Repository 에서 정보를 가져오는 것을 확인 할 수 있습니다.\nroot@u-node-1:~# apt-get update Get:1 http://localhost:8000/ubuntu jammy InRelease [270 kB] Get:2 http://localhost:8000/ubuntu jammy/main amd64 Packages [1395 kB] Get:3 http://localhost:8000/ubuntu jammy/main Translation-en [510 kB] Get:4 http://localhost:8000/ubuntu jammy/main amd64 DEP-11 Metadata [423 kB] Get:5 http://localhost:8000/ubuntu jammy/main DEP-11 48x48 Icons [100.0 kB] Get:6 http://localhost:8000/ubuntu jammy/main DEP-11 64x64 Icons [148 kB] Ign:7 http://localhost:8000/ubuntu jammy/main amd64 c-n-f Metadata Get:8 http://localhost:8000/ubuntu jammy/restricted amd64 Packages [129 kB] Get:9 http://localhost:8000/ubuntu jammy/restricted Translation-en [18.6 kB] Ign:10 http://localhost:8000/ubuntu jammy/restricted amd64 c-n-f Metadata Get:11 http://localhost:8000/ubuntu jammy/universe amd64 Packages [14.1 MB] Get:12 http://localhost:8000/ubuntu jammy/universe Translation-en [5652 kB] Get:13 http://localhost:8000/ubuntu jammy/universe amd64 DEP-11 Metadata [3559 kB] Get:14 http://localhost:8000/ubuntu jammy/universe DEP-11 48x48 Icons [3447 kB] Get:15 http://localhost:8000/ubuntu jammy/universe DEP-11 64x64 Icons [7609 kB] Ign:16 http://localhost:8000/ubuntu jammy/universe amd64 c-n-f Metadata Get:17 http://localhost:8000/ubuntu jammy/multiverse amd64 Packages [217 kB] Get:18 http://localhost:8000/ubuntu jammy/multiverse Translation-en [112 kB] Get:19 http://localhost:8000/ubuntu jammy/multiverse amd64 DEP-11 Metadata [42.1 kB] Get:20 http://localhost:8000/ubuntu jammy/multiverse DEP-11 48x48 Icons [42.7 kB] Get:21 http://localhost:8000/ubuntu jammy/multiverse DEP-11 64x64 Icons [193 kB] Ign:22 http://localhost:8000/ubuntu jammy/multiverse amd64 c-n-f Metadata Get:7 http://localhost:8000/ubuntu jammy/main amd64 c-n-f Metadata [30.3 kB] Get:10 http://localhost:8000/ubuntu jammy/restricted amd64 c-n-f Metadata [488 B] Get:16 http://localhost:8000/ubuntu jammy/universe amd64 c-n-f Metadata [286 kB] Get:22 http://localhost:8000/ubuntu jammy/multiverse amd64 c-n-f Metadata [8372 B] Fetched 595 kB in 3s (199 kB/s) Reading package lists... Done ","permalink":"https://chhanz88.github.io/post/2022-05-09-ubuntu-config-apt-mirror-site/","summary":"목차  overview DVD 사용  Mount DVD sources.list 추가 Repository 추가   Sync Repository  apt-mirror 설치 mirror.list 수정 Sync Repository Check Sync mirror data location   Mirror site 운영  Mirror site 추가 APT update    overview 이번 포스팅은 DVD 를 활용하여 인터넷이 안되는 환경에서 Ubuntu package 를 다루는 방법과\n외부 환경이 아닌 내부에 Repository mirror site 를 구성하는 방법에 대해 알아보도록 하겠습니다.\nDVD 사용 주로 인터넷이 안되는 망분리 된 곳에서 많이 활용될 방식입니다.","title":"[Ubuntu] Local mirror site 구성(DVD/Repository sync)"},{"content":"Ubuntu 20.04 NTP Service Ubuntu Server Version 을 설치하게 되면 기본적으로 NTP 서비스를 활성화가 되어있습니다.\nroot@u-node-0:/etc/systemd# timedatectl -a  Local time: Thu 2022-04-28 11:14:26 KST  Universal time: Thu 2022-04-28 02:14:26 UTC  RTC time: Thu 2022-04-28 02:14:27  Time zone: Asia/Seoul (KST, +0900) System clock synchronized: yes \u0026lt;\u0026lt;  NTP service: active  RTC in local TZ: no RHEL 계열의 ntpd 혹은 chronyd 가 자동으로 설치가 되어서 그런건가?\nUbuntu 는 설치가 되면 systemd-timesyncd 라는 daemon 이 기본적으로 NTP 서비스를 수행합니다.\nroot@u-node-0:/etc/systemd# systemctl status systemd-timesyncd ● systemd-timesyncd.service - Network Time Synchronization  Loaded: loaded (/lib/systemd/system/systemd-timesyncd.service; enabled; vendor preset: enabled)  Active: active (running) since Sat 2022-01-15 06:36:26 KST; 3 months 11 days ago  Docs: man:systemd-timesyncd.service(8)  Main PID: 164170 (systemd-timesyn)  Status: \u0026#34;Initial synchronization to time server 91.189.91.157:123 (ntp.ubuntu.com).\u0026#34;  Tasks: 2 (limit: 2343)  Memory: 1.4M  CGroup: /system.slice/systemd-timesyncd.service  └─164170 /lib/systemd/systemd-timesyncd  Apr 06 20:29:10 u-node-0 systemd-timesyncd[164170]: Initial synchronization to time server 91.189.91.157:123 (ntp.ubuntu.com). systemd-timesyncd의 설정을 변경하기 위해서는 /etc/systemd/timesyncd.conf 을 수정해야 되고,\nroot@u-node-0:/etc/systemd# cat /etc/systemd/timesyncd.conf ... 생략  [Time] #NTP= #FallbackNTP=ntp.ubuntu.com #RootDistanceMaxSec=5 #PollIntervalMinSec=32 #PollIntervalMaxSec=2048 자세한 설정 수정 방법은 $ man timesyncd.conf를 참고합니다.\nChronyd 위와 같이 기본 timesyncd 를 사용하거나 내부 NTP Client 및 Server 를 구축하기 위해서는 권장되는 솔루션은 chronyd 입니다.\n설치 아래 명령을 통해 설치가 가능힙니다.\n$ sudo apt search chronyd Sorting... Done Full Text Search... Done chrony/focal-updates,focal-security,now 3.5-6ubuntu6.2 amd64 [installed]  Versatile implementation of the Network Time Protocol  $ sudo apt install chrony -y 재미있는건 chrony 가 설치가 되면 자동으로 enable 되어 있던 systemd-timesyncd는 disable 이 되고 chrony 가 enable 됩니다.\n$ cat /var/log/apt/history.log ... Commandline: apt install chrony Install: chrony:amd64 (3.5-6ubuntu6.2) Upgrade: libsystemd0:amd64 (245.4-4ubuntu3.15, 245.4-4ubuntu3.16), systemd-sysv:amd64 (245.4-4ubuntu3.15, 245.4-4ubuntu3.16), libpam-systemd:amd64 (245.4-4ubuntu3.15, 245.4-4ubuntu3.16), systemd:amd64 (245.4-4ubuntu3.15, 245.4-4ubuntu3.16), libnss-systemd:amd64 (245.4-4ubuntu3.15, 245.4-4ubuntu3.16) Remove: systemd-timesyncd:amd64 (245.4-4ubuntu3.15) \u0026lt;\u0026lt;\u0026lt; $ systemctl status systemd-timesyncd ● systemd-timesyncd.service  Loaded: masked (Reason: Unit systemd-timesyncd.service is masked.) \u0026lt;\u0026lt;\u0026lt;  Active: inactive (dead) since Thu 2022-04-28 11:15:22 KST; 7min ago  Main PID: 164170 (code=exited, status=0/SUCCESS)  Status: \u0026#34;Shutting down...\u0026#34; 설치가 완료되면 아래와 같이 서비스가 기동중입니다.\n$ systemctl status chronyd ● chrony.service - chrony, an NTP client/server  Loaded: loaded (/lib/systemd/system/chrony.service; enabled; vendor preset: enabled)  Active: active (running) since Thu 2022-04-28 11:15:25 KST; 7min ago  Docs: man:chronyd(8)  man:chronyc(1)  man:chrony.conf(5)  Main PID: 414899 (chronyd)  Tasks: 2 (limit: 2343)  Memory: 1.2M  CGroup: /system.slice/chrony.service  ├─414899 /usr/sbin/chronyd -F -1  └─414900 /usr/sbin/chronyd -F -1 설정 chronyd 의 설정은 RHEL 에서 사용하는 방식과 동일하지만 설정 파일의 경로에 차이가 있습니다.\n  RHEL 계열 : /etc/chrony.conf Debian 계열 : /etc/chrony/chrony.conf   $ cat /etc/chrony/chrony.conf ... 생략  pool ntp.ubuntu.com iburst maxsources 4 pool 0.ubuntu.pool.ntp.org iburst maxsources 1 pool 1.ubuntu.pool.ntp.org iburst maxsources 1 pool 2.ubuntu.pool.ntp.org iburst maxsources 2  ... 생략 chrony 설정 파일은 위와 같으며 내부 NTP 서비스를 하기 위해서는\nallow 1.2.3.4 위와 같이 allow line 을 추가하면 됩니다.\n그 외 자세한 설정은 문서 를 참고 합니다.\nCheck 아래 명령을 통해 NTP 서비스 동기화 상태를 확인 할 수 있습니다.\n$ chronyc sources -v 210 Number of sources = 8   .-- Source mode \u0026#39;^\u0026#39; = server, \u0026#39;=\u0026#39; = peer, \u0026#39;#\u0026#39; = local clock.  / .- Source state \u0026#39;*\u0026#39; = current synced, \u0026#39;+\u0026#39; = combined , \u0026#39;-\u0026#39; = not combined, | / \u0026#39;?\u0026#39; = unreachable, \u0026#39;x\u0026#39; = time may be in error, \u0026#39;~\u0026#39; = time too variable. || .- xxxx [ yyyy ] +/- zzzz || Reachability register (octal) -. | xxxx = adjusted offset, || Log2(Polling interval) --. | | yyyy = measured offset, || \\  | | zzzz = estimated error. || | | \\ MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^- 91.189.91.157 2 6 377 36 +8219us[+8219us] +/- 151ms ^- 91.189.94.4 2 6 377 35 +4838us[+4838us] +/- 146ms ^- 91.189.89.198 2 6 337 35 +4675us[+4675us] +/- 137ms ^- 91.189.89.199 2 6 377 35 +5279us[+5279us] +/- 158ms ^- 194.0.5.123 2 6 377 38 -2538us[-2538us] +/- 41ms ^- 193.123.243.2 2 6 377 40 -778us[ -846us] +/- 36ms ^- 3.36.253.109 2 6 377 39 -273us[ -273us] +/- 44ms ^* 163.152.23.171 2 6 377 39 -250us[ -317us] +/- 9385us 참고 문서  Ubuntu - NTP : https://ubuntu.com/server/docs/network-ntp RHEL - chrony :\nhttps://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/ch-configuring_ntp_using_the_chrony_suite chrony.conf : https://chrony.tuxfamily.org/doc/3.1/chrony.conf.html  ","permalink":"https://chhanz88.github.io/post/2022-05-02-ubuntu-ntp-chrony/","summary":"Ubuntu 20.04 NTP Service Ubuntu Server Version 을 설치하게 되면 기본적으로 NTP 서비스를 활성화가 되어있습니다.\nroot@u-node-0:/etc/systemd# timedatectl -a  Local time: Thu 2022-04-28 11:14:26 KST  Universal time: Thu 2022-04-28 02:14:26 UTC  RTC time: Thu 2022-04-28 02:14:27  Time zone: Asia/Seoul (KST, +0900) System clock synchronized: yes \u0026lt;\u0026lt;  NTP service: active  RTC in local TZ: no RHEL 계열의 ntpd 혹은 chronyd 가 자동으로 설치가 되어서 그런건가?\nUbuntu 는 설치가 되면 systemd-timesyncd 라는 daemon 이 기본적으로 NTP 서비스를 수행합니다.","title":"[Ubuntu] NTP Service"},{"content":"VirtualBMC 를 이용한 IPMI node 생성 IPMI Node 설정 SSH Key 배포 SSH 를 이용하여 KVM 에 접근 및 통제를 하기 위해 SSH KEY Password 배포를 진행합니다.\n[root@ipmi-node ~]# ssh-copy-id root@kvm Python-pip 설치 아래와 같은 방법으로 pip 을 설치합니다.\n[root@ipmi-node ~]# yum -y install python3 python3-pip  [root@ipmi-node ~]# pip3 install -U pip VirtualBMC 설치 아래와 같은 방법으로 vBMC 를 설치합니다.\n[root@ipmi-node ~]# pip3 install virtualbmc 설치 과정 중, 아래와 같은 Error 가 발생되면 추가 Package 를 설치합니다.\n[root@ipmi-node ~]# yum install -y libvirt-devel gcc python3-devel ipmitool Error Case \u0026#34;case 1\u0026#34; ...  Complete output (16 lines):  Package libvirt was not found in the pkg-config search path.  Perhaps you should add the directory containing `libvirt.pc\u0026#39;  to the PKG_CONFIG_PATH environment variable  No package \u0026#39;libvirt\u0026#39; found  Package libvirt was not found in the pkg-config search path.  Perhaps you should add the directory containing `libvirt.pc\u0026#39;  to the PKG_CONFIG_PATH environment variable  No package \u0026#39;libvirt\u0026#39; found  running install  running build  /usr/bin/pkg-config --print-errors --atleast-version=0.9.11 libvirt  Package libvirt was not found in the pkg-config search path.  Perhaps you should add the directory containing `libvirt.pc\u0026#39;  to the PKG_CONFIG_PATH environment variable  No package \u0026#39;libvirt\u0026#39; found  error: command \u0026#39;/usr/bin/pkg-config\u0026#39; failed with exit status ... \u0026#34;case 2\u0026#34; ...  creating build/temp.linux-x86_64-3.6  creating build/temp.linux-x86_64-3.6/build  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I. -I/usr/include/python3.6m -c libvirt-override.c -o build/temp.linux-x86_64-3.6/libvirt-override.o  unable to execute \u0026#39;gcc\u0026#39;: No such file or directory  error: command \u0026#39;gcc\u0026#39; failed with exit status 1 ... \u0026#34;case 3\u0026#34; ...  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I. -I/usr/include/python3.6m -c libvirt-override.c -o build/temp.linux-x86_64-3.6/libvirt-override.o  libvirt-override.c:21:20: fatal error: Python.h: No such file or directory  #include \u0026lt;Python.h\u0026gt; ... vBMC 서비스 시작 아래 명령을 통해 서비스를 시작합니다.\n[root@ipmi-node ~]# vbmcd vBMC 포트 추가 아래 명령과 같이 vBMC IPMI Port 를 생성합니다.\n$ vbmc --username \u0026lt;vBMC IPMI 계정 이름\u0026gt; --password \u0026lt;vBMC IPMI 계정의 패스워드\u0026gt; --port \u0026lt;vBMC IPMI PORT\u0026gt; --libvirt-uri qemu+ssh://root@\u0026lt;KVM HOSTNAME\u0026gt;/system \u0026lt;VM NAME\u0026gt; 실제 추가 명령 예제는 아래와 같습니다.\n[root@ipmi-node ~]# vbmc add --username admin --password testtest --port 6250 --libvirt-uri qemu+ssh://root@kvm/system u20 vBMC 포트 확인 아래 명령을 통해 생성된 포트를 확인합니다.\n[root@ipmi-node ~]# vbmc list +-------------+--------+---------+------+ | Domain name | Status | Address | Port | +-------------+--------+---------+------+ | u20 | down | :: | 6250 | +-------------+--------+---------+------+ vBMC 포트 서비스 시작 아래 명령을 통해 vBMC 포트 서비스를 시작합니다.\n[root@ipmi-node ~]# vbmc start u20 ... [root@ipmi-node ~]# vbmc list +-------------+---------+---------+------+ | Domain name | Status | Address | Port | +-------------+---------+---------+------+ | ipmi-node | down | :: | 6252 | | maas-node | down | :: | 6251 | | u20 | running | :: | 6250 | \u0026lt;\u0026lt; +-------------+---------+---------+------+ vBMC 포트 상세 정보 확인 아래 명령을 통해 vBMC 포트 상세 정보를 확인 할 수 있습니다.\n[root@ipmi-node ~]# vbmc show u20 +-----------------------+----------------------------+ | Property | Value | +-----------------------+----------------------------+ | active | True | | address | :: | | domain_name | u20 | | libvirt_sasl_password | *** | | libvirt_sasl_username | None | | libvirt_uri | qemu+ssh://root@kvm/system | | password | *** | | port | 6250 | | status | running | | username | admin | +-----------------------+----------------------------+ vBMC 포트 서비스 확인 (netstat) netstat 를 통해 IPMI Node 에 각각의 포트로 vBMC port 가 활성화 됩니다.\n[root@ipmi-node ~]# netstat -anup Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name udp 0 0 0.0.0.0:68 0.0.0.0:* 831/dhclient udp 0 0 0.0.0.0:111 0.0.0.0:* 550/rpcbind udp 0 0 127.0.0.1:323 0.0.0.0:* 554/chronyd udp 0 0 0.0.0.0:722 0.0.0.0:* 550/rpcbind udp6 0 0 :::6250 :::* 1934/python3 \u0026lt;\u0026lt;\u0026lt; udp6 0 0 :::6251 :::* 1983/python3 \u0026lt;\u0026lt;\u0026lt; udp6 0 0 :::6252 :::* 2000/python3 \u0026lt;\u0026lt;\u0026lt; udp6 0 0 :::111 :::* 550/rpcbind udp6 0 0 ::1:323 :::* 554/chronyd udp6 0 0 :::722 :::* 550/rpcbind IPMITOOL 을 통해 전원 제어 아래와 같은 방법을 통해서 VM 의 전원을 통제 할 수 있습니다.\n[root@c-node-0 ~]# ipmitool -I lanplus -p 6250 -Uadmin -Ptesttest -H ipmi-node chassis power status Chassis Power is off  [root@c-node-0 ~]# ipmitool -I lanplus -p 6251 -Uadmin -Ptesttest -H ipmi-node chassis power status Chassis Power is on 전원 on/off 예제는 아래와 같습니다.\n$ ipmitool -I lanplus -p 6251 -Uadmin -Ptesttest -H ipmi-node chassis power \u0026lt;on | off\u0026gt; 참고 자료  IPMITOOL 사용 가이드 : https://chhanz.github.io/linux/2022/03/01/ipmitool/  ","permalink":"https://chhanz88.github.io/post/2022-04-14-vbmcd-ipminode/","summary":"VirtualBMC 를 이용한 IPMI node 생성 IPMI Node 설정 SSH Key 배포 SSH 를 이용하여 KVM 에 접근 및 통제를 하기 위해 SSH KEY Password 배포를 진행합니다.\n[root@ipmi-node ~]# ssh-copy-id root@kvm Python-pip 설치 아래와 같은 방법으로 pip 을 설치합니다.\n[root@ipmi-node ~]# yum -y install python3 python3-pip  [root@ipmi-node ~]# pip3 install -U pip VirtualBMC 설치 아래와 같은 방법으로 vBMC 를 설치합니다.\n[root@ipmi-node ~]# pip3 install virtualbmc 설치 과정 중, 아래와 같은 Error 가 발생되면 추가 Package 를 설치합니다.","title":"[Linux] VirtualBMC 를 이용한 IPMI node 생성"},{"content":" 매번 잊어버려서 따로 정리함.\n ipmitool 설치 아래 명령을 통해 ipmitool 명령을 설치 할 수 있다.\n  RHEL/CentOS/Rocky 계열\n$ sudo yum install ipmitool or $ sudo dnf install ipmitool   Ubuntu\n$ sudo apt install ipmitool   IPMI 정보 확인 아래 명령을 통해 IPMI 정보를 확인 할 수 있다.\n$ ipmitool lan print [\u0026lt;channel num\u0026gt;] Set in Progress : Set Complete Auth Type Support : NONE MD5 PASSWORD Auth Type Enable : Callback : MD5 PASSWORD  : User : MD5 PASSWORD  : Operator : MD5 PASSWORD  : Admin : MD5 PASSWORD  : OEM : IP Address Source : Static Address IP Address : 222.222.222.222 Subnet Mask : 255.255.255.255 MAC Address : xx:xx:xx:xx:xx:xx SNMP Community String : BMC ARP Control : ARP Responses Enabled, Gratuitous ARP Disabled Default Gateway IP : 222.222.222.222 802.1q VLAN ID : Disabled 802.1q VLAN Priority : 0 RMCP+ Cipher Suites : 0,1,2,3 Cipher Suite Priv Max : XuuaXXXXXXXXXXX  : X=Cipher Suite Unused  : c=CALLBACK  : u=USER  : o=OPERATOR  : a=ADMIN  : O=OEM Bad Password Threshold : Not Available IPMI IP 설정 아래와 같이 ipmitool 명령을 이용하여 IPMI 에 Remote 접근 IP 를 설정 할 수 있다.\n$ ipmitool lan set \u0026lt;channel num\u0026gt; ipsrc static $ ipmitool lan set \u0026lt;channel num\u0026gt; ipaddr \u0026lt;IPMI Remote IP\u0026gt; $ ipmitool lan set \u0026lt;channel num\u0026gt; netmask \u0026lt;IPMI Subnet\u0026gt; $ ipmitool lan set \u0026lt;channel num\u0026gt; defgw ipaddr \u0026lt;IPMI Gateway IP\u0026gt; $ ipmitool lan set \u0026lt;channel num\u0026gt; access on 위와 같이 IP 설정 이후, IPMI 를 재시작 해야 정상적으로 작동한다.\n$ ipmitool bmc reset cold IPMI User 확인 아래 명령을 통해 IPMI 의 User 를 확인 할 수 있다.\n$ ipmitool user list \u0026lt;channel num\u0026gt; ID Name Callin Link Auth IPMI Msg Channel Priv Limit 1 true false false NO ACCESS 2 Administrator true false true ADMINISTRATOR 3 (Empty User) true false true NO ACCESS 4 (Empty User) true false true NO ACCESS 5 (Empty User) true false true NO ACCESS 6 (Empty User) true false true NO ACCESS 7 (Empty User) true false true NO ACCESS 8 (Empty User) true false true NO ACCESS 9 (Empty User) true false true NO ACCESS 10 (Empty User) true false true NO ACCESS 11 (Empty User) true false true NO ACCESS 12 (Empty User) true false true NO ACCESS 13 (Empty User) true false true NO ACCESS IPMI User Control 아래 명령을 통해 User 생성이 가능하다.\n$ ipmitool user set name \u0026lt;user id\u0026gt; \u0026lt;user name\u0026gt; [\u0026lt;channel num\u0026gt;] 아래 명령을 통해 생성한 User 의 Password 를 설정 할 수 있다.\n$ ipmitool user set password \u0026lt;user id\u0026gt; [\u0026lt;channel num\u0026gt;] 아래 명령을 통해 User 에 권한을 부여 할 수 있다.\n$ ipmitool user priv \u0026lt;user id\u0026gt; \u0026lt;privilege level\u0026gt; [\u0026lt;channel num\u0026gt;] privilege level 목록은 아래와 같다.\nPossible privilege levels are:  1 Callback level  2 User level  3 Operator level  4 Administrator level  5 OEM Proprietary level  15 No access Administrator 권한을 부여하기 위해서는 4 를 사용하면 된다.\nIPMI Power Control 아래 명령을 통해 system power status 를 확인 할 수 있다.\n$ ipmitool chassis status System Power : on Power Overload : false Power Interlock : inactive Main Power Fault : false Power Control Fault : false Power Restore Policy : previous Last Power Event : Chassis Intrusion : inactive Front-Panel Lockout : inactive Drive Fault : false Cooling/Fan Fault : false Front Panel Control : none $ ipmitool chassis power status Chassis Power is on 아래 명령을 통해 system power on / off 를 진행 할 수 있다.\n$ ipmitool chassis power \u0026lt;on | off\u0026gt; IPMI System Event Log 아래 명령을 통해 system event log 를 확인 할 수 있다.\n$ ipmitool sel list  1 | Pre-Init |0000000156| System ACPI Power State #0xaa | S0/G0: working | Asserted  2 | Pre-Init |0000000190| System ACPI Power State #0xaa | S4/S5: soft-off | Asserted ... 생략 IPMI Sensor Data Repository 아래 명령을 통해 Sensor Data Repository 를 확인 할 수 있다.\n$ ipmitool sdr list UID | 0x00 | ok SysHealth_Stat | 0x00 | ok 01-Inlet Ambient | 24 degrees C | ok 02-CPU 1 | 40 degrees C | ok 03-P1 DIMM 1-8 | 35 degrees C | ok 04-P1 DIMM 9-16 | 36 degrees C | ok 05-HD Max | disabled | ns 06-Exp Bay Drive | disabled | ns 07-VR P1 | 48 degrees C | ok 08-VR P1 Mem 1 | 46 degrees C | ok 09-VR P1 Mem 2 | 44 degrees C | ok 10-Stor Batt | disabled | ns 11-BMC | 61 degrees C | ok 12-BMC Zone | 37 degrees C | ok 13-Battery Zone | 36 degrees C | ok 14-System Board | 33 degrees C | ok ... 생략 IPMI FRU List 아래 명령을 통해 system part 의 fru 를 확인 할 수 있다.\n$ ipmitool fru list FRU Device Description : Builtin FRU Device (ID 0)  Chassis Type : Rack Mount Chassis  Chassis Serial : xxxxxxxxxx  Board Mfg Date : Wed xxx xx xx:xx:xx xxxx  Board Mfg : HPE  Board Product : ProLiant DLxxx Genxx  Board Serial : xxxxxxxxxx  Board Part Number : P01234-B12  Product Manufacturer : HPE  Product Name : ProLiant DLxxx Genxx  Product Part Number : P01234-B12  Product Serial : xxxxxxxxxx  FRU Device Description : BMC CONTROLLER (ID 238)  Product Manufacturer : HPE  Product Name : BMC CONTROLLER  Product Part Number : iLO 5 ... 생략 Remote 접근 아래 명령을 통해 Remote 에서 IPMI Control 이 가능하다.\n$ ipmitool -I lanplus -U \u0026lt;user id\u0026gt; -P \u0026lt;user passwd\u0026gt; -H \u0026lt;IPMI IP\u0026gt; [-p \u0026lt;IPMI Port\u0026gt;] chassis power status Chassis Power is on 참고 문서  https://www.thomas-krenn.com/en/wiki/Configuring_IPMI_under_Linux_using_ipmitool  ","permalink":"https://chhanz88.github.io/post/2022-03-01-ipmitool/","summary":"매번 잊어버려서 따로 정리함.\n ipmitool 설치 아래 명령을 통해 ipmitool 명령을 설치 할 수 있다.\n  RHEL/CentOS/Rocky 계열\n$ sudo yum install ipmitool or $ sudo dnf install ipmitool   Ubuntu\n$ sudo apt install ipmitool   IPMI 정보 확인 아래 명령을 통해 IPMI 정보를 확인 할 수 있다.\n$ ipmitool lan print [\u0026lt;channel num\u0026gt;] Set in Progress : Set Complete Auth Type Support : NONE MD5 PASSWORD Auth Type Enable : Callback : MD5 PASSWORD  : User : MD5 PASSWORD  : Operator : MD5 PASSWORD  : Admin : MD5 PASSWORD  : OEM : IP Address Source : Static Address IP Address : 222.","title":"[Linux] ipmitool 사용법"},{"content":"목차  Njmon 란? Architecture Telegraf config Njmon config  Prometheus config Grafana Dashboard 참고 자료  Njmon 이란? Architecture Telegraf config 이번 글에서 사용한 테스트 시스템은 Rocky Linux 8 이며 아래 문서를 참고 하여 Telegraf 를 구성하였습니다.\n(https://docs.influxdata.com/telegraf/v1.21/introduction/installation/?t=RedHat+%26amp%3B+CentOS)\n아래와 같이 Repository 를 추가합니다.\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo [influxdb] name = InfluxDB Repository - RHEL \\$releasever baseurl = https://repos.influxdata.com/rhel/\\$releasever/\\$basearch/stable enabled = 1 gpgcheck = 1 gpgkey = https://repos.influxdata.com/influxdb.key EOF dnf 를 통해 Package 를 설치합니다.\n$sudo dnf install telegraf -y /etc/telegraf/telegraf.conf 를 아래와 같이 수정합니다.\n[[outputs.prometheus_client]]  listen = \u0026#34;:8088\u0026#34;  metric_version = 2  path = \u0026#34;/metrics\u0026#34;  expiration_interval = \u0026#34;120s\u0026#34;  string_as_label = false [[inputs.socket_listener]]  service_address = \u0026#34;tcp://:8888\u0026#34;  data_format = \u0026#34;influx\u0026#34;  read_buffer_size = \u0026#34;256KiB\u0026#34;  read_timeout = \u0026#34;2s\u0026#34; 기본적으로 들어간 [[outputs.XXXXXX]] config 값은 제외해야지 정상적으로 서비스가 기동됩니다.\n$ sudo systemctl enable --now telegraf 서비스가 정상적으로 기동되면 http://localhost:8088/metrics subpath 로 metric 이 expose 됩니다.\nNjmon config (nimon) http://nmon.sourceforge.net/pmwiki.php?n=Site.Njmon 에서 Latest Release 를 확인 할 수 있고,\nhttps://sourceforge.net/projects/nmon/files/ 제공 중인 All Release 를 확인 할 수 있습니다.\nOS Version 과 CPU Type 를 확인하여 njmon 을 다운 받습니다.\n[root@chhan-influxdb ~]# unzip njmon_linux_binaries_v78.zip [root@chhan-influxdb ~]# cp njmon_RHEL8_x86_64_v78 /usr/local/bin/njmon [root@chhan-influxdb ~]# chmod 775 /usr/local/bin/njmon [root@chhan-influxdb ~]# chown root:root /usr/local/bin/njmon [root@chhan-influxdb ~]# ln -f /usr/local/bin/njmon /usr/local/bin/nimon 위와 같이 njmon 를 설치하고 아래 명령을 통해 nimon 을 기동하고 telegraf 로 데이터를 전송하도록 설정합니다.\n$ sudo nimon -s30 -c1440 -w -D -P -i localhost -p 8888 nimon 이 정상적으로 telegraf 로 데이터가 전송이 된다면, http://localhost:8088/metrics 에서 아래와 같이 metric 이 확인 할 수 있습니다.\n$ curl -s localhost:8088/metrics ... # HELP cpu_total_guest Telegraf collected metric # TYPE cpu_total_guest untyped cpu_total_guest{architecture=\u0026#34;x86-64\u0026#34;,host=\u0026#34;chhan-influxdb\u0026#34;,mtm=\u0026#34;OpenStack-Compute\u0026#34;,os=\u0026#34;Rocky\u0026#34;,serial_no=\u0026#34;a9221572-6655-4dab-b7ab-b5ec8831ab35\u0026#34;} 0 # HELP cpu_total_guestnice Telegraf collected metric # TYPE cpu_total_guestnice untyped cpu_total_guestnice{architecture=\u0026#34;x86-64\u0026#34;,host=\u0026#34;chhan-influxdb\u0026#34;,mtm=\u0026#34;OpenStack-Compute\u0026#34;,os=\u0026#34;Rocky\u0026#34;,serial_no=\u0026#34;a9221572-6655-4dab-b7ab-b5ec8831ab35\u0026#34;} 0 # HELP cpu_total_hardirq Telegraf collected metric # TYPE cpu_total_hardirq untyped cpu_total_hardirq{architecture=\u0026#34;x86-64\u0026#34;,host=\u0026#34;chhan-influxdb\u0026#34;,mtm=\u0026#34;OpenStack-Compute\u0026#34;,os=\u0026#34;Rocky\u0026#34;,serial_no=\u0026#34;a9221572-6655-4dab-b7ab-b5ec8831ab35\u0026#34;} 0 # HELP cpu_total_idle Telegraf collected metric # TYPE cpu_total_idle untyped cpu_total_idle{architecture=\u0026#34;x86-64\u0026#34;,host=\u0026#34;chhan-influxdb\u0026#34;,mtm=\u0026#34;OpenStack-Compute\u0026#34;,os=\u0026#34;Rocky\u0026#34;,serial_no=\u0026#34;a9221572-6655-4dab-b7ab-b5ec8831ab35\u0026#34;} 99.634 ... Prometheus config Prometheus Scrape Config Target 을 아래와 같은 형식으로 설정합니다.\n$ cat prometheus.yml global:  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. scrape_configs:  - job_name: \u0026#39;prometheus\u0026#39;  static_configs:  - targets: [\u0026#39;localhost:9090\u0026#39;]  - job_name: \u0026#39;node_exporter\u0026#39;  static_configs:  - targets: [\u0026#39;localhost:9100\u0026#39;]  - job_name: \u0026#39;njmon\u0026#39; \u0026lt;\u0026lt;\u0026lt;  static_configs: \u0026lt;\u0026lt;\u0026lt;  - targets: [\u0026#39;192.168.1.18:8088\u0026#39;] \u0026lt;\u0026lt;\u0026lt; Prometheus 에서 정상적으로 scrape 이 되면 Prometheus 에서 Metric 이 확인됩니다.\nGrafana Dashboard 기본적으로 njmon 은 Grafana Dashboard 를 따로 제공 하지 않습니다.\n그리하여 아래와 같이 njmon 을 통해 Dashboard 를 사용 할 수 있도록 만들었습니다.\nLinux System 에서 사용 할 수 있도록 만들었고 https://grafana.com/grafana/dashboards/15776 에서\nCopy ID 를 이용하여 Import 가 가능합니다.\n source : https://github.com/chhanz/njmon-grafana-dashboard-json  참고 자료  http://nmon.sourceforge.net/pmwiki.php?n=Site.Njmon https://www.ibm.com/support/pages/nimon-working-prometheus https://grafana.com/grafana/dashboards/15776 https://docs.influxdata.com/telegraf/v1.21/introduction/installation/ https://github.com/chhanz/njmon-grafana-dashboard-json  ","permalink":"https://chhanz88.github.io/post/2022-02-18-njmon-telegraf-grafana/","summary":"목차  Njmon 란? Architecture Telegraf config Njmon config  Prometheus config Grafana Dashboard 참고 자료  Njmon 이란? Architecture Telegraf config 이번 글에서 사용한 테스트 시스템은 Rocky Linux 8 이며 아래 문서를 참고 하여 Telegraf 를 구성하였습니다.\n(https://docs.influxdata.com/telegraf/v1.21/introduction/installation/?t=RedHat+%26amp%3B+CentOS)\n아래와 같이 Repository 를 추가합니다.\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo [influxdb] name = InfluxDB Repository - RHEL \\$releasever baseurl = https://repos.influxdata.com/rhel/\\$releasever/\\$basearch/stable enabled = 1 gpgcheck = 1 gpgkey = https://repos.influxdata.com/influxdb.key EOF dnf 를 통해 Package 를 설치합니다.","title":"[Linux] njmon with Prometheus"},{"content":"EOS CentOS 8 CentOS 8 은 December 31, 2021 부로 EOS 되었습니다.\n지속적인 사용을 위해서는 CentOS 8 Stream 으로 배포판 변경을 해야지 지속적인 지원을 받을 수 있습니다.\nError dnf Error: Failed to download metadata for repo 'appstream': Cannot prepare internal mirrorlist: No URLs in mirrorlist\n메시지가 나오면서 dnf 가 수행이 안되는 것을 확인 할 수 있습니다.\n[root@chhan-c8 ~]# dnf repolist CentOS Linux 8 - AppStream 87 B/s | 38 B 00:00 Error: Failed to download metadata for repo \u0026#39;appstream\u0026#39;: Cannot prepare internal mirrorlist: No URLs in mirrorlist 해결 방안 이는 CentOS 8 EOS 로 인해 CentOS 8 Mirror site 가 vault 로 전환되어 Mirror site 를 못 찾아 발생되는 문제입니다.\n(https://www.centos.org/centos-linux-eol/)\n아래 명령어를 통해 기존에 Mirror site 를 Vault 로 전환하여 dnf 사용을 할 수 있습니다.\n[root@chhan-c8 ~]# sed -i \u0026#39;s/mirrorlist/#mirrorlist/g\u0026#39; /etc/yum.repos.d/CentOS-Linux-* [root@chhan-c8 ~]# sed -i \u0026#39;s|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g\u0026#39; /etc/yum.repos.d/CentOS-Linux-* 아래와 같이 정상적으로 repolist 를 받아오는 것을 확인 할 수 있습니다.\n[root@chhan-c8 ~]# dnf repolist repo id repo name appstream CentOS Linux 8 - AppStream baseos CentOS Linux 8 - BaseOS extras CentOS Linux 8 - Extras [root@chhan-c8 ~]# cat /etc/yum.repos.d/CentOS-Linux-BaseOS.repo  ... [baseos] name=CentOS Linux $releasever - BaseOS #mirrorlist=http://#mirrorlist.centos.org/?release=$releasever\u0026amp;arch=$basearch\u0026amp;repo=BaseOS\u0026amp;infra=$infra baseurl=http://vault.centos.org/$contentdir/$releasever/BaseOS/$basearch/os/ \u0026lt;\u0026lt; vault site 로 변경 위와 같이 당장은 해결이 가능하지만 해당 Repo 는 더이상 Package 의 유지보수가 없으므로 보안에 취약합니다.\n근본적인 해결을 위해서는 다른 배포판(ex. rocky linux 등) 이나 CentOS 8 Stream, RHEL 8 로 전환이 필요합니다.\nCentOS 8 Stream 전환 해당 문서와 혼선을 방지하기 위해 외부 사이트를 링크합니다.\n https://linuxhandbook.com/update-to-centos-stream/  참고 자료  https://endoflife.software/operating-systems/linux/centos https://www.centos.org/centos-linux-eol/ https://forums.centos.org/viewtopic.php?f=54\u0026amp;t=78708 https://stackoverflow.com/questions/70926799/centos-through-vm-no-urls-in-mirrorlist  ","permalink":"https://chhanz88.github.io/post/2022-02-04-dnf-error-centos-8/","summary":"EOS CentOS 8 CentOS 8 은 December 31, 2021 부로 EOS 되었습니다.\n지속적인 사용을 위해서는 CentOS 8 Stream 으로 배포판 변경을 해야지 지속적인 지원을 받을 수 있습니다.\nError dnf Error: Failed to download metadata for repo 'appstream': Cannot prepare internal mirrorlist: No URLs in mirrorlist\n메시지가 나오면서 dnf 가 수행이 안되는 것을 확인 할 수 있습니다.\n[root@chhan-c8 ~]# dnf repolist CentOS Linux 8 - AppStream 87 B/s | 38 B 00:00 Error: Failed to download metadata for repo \u0026#39;appstream\u0026#39;: Cannot prepare internal mirrorlist: No URLs in mirrorlist 해결 방안 이는 CentOS 8 EOS 로 인해 CentOS 8 Mirror site 가 vault 로 전환되어 Mirror site 를 못 찾아 발생되는 문제입니다.","title":"[Linux] CentOS 8: Failed to download metadata for repo 'appstream'"},{"content":"Intro 시스템 장애로 인해 OS 를 재설치 해야되는 경우가 생겼다.\n시스템팀에서 OS 재설치를 해야되는데 DVD 도 없고 USB 이미지도 없었다.\n급하게 설치를 해야되서 내가 가지고 있는 IODD 를 이용하여 OS 설치를 하게 되었다.\nVentoy 무료로 IODD 와 같이 사용 할 수 있는 것이 있나 찾다가 발견된 것은 Ventoy 이다.\n따로 Boot device 로 생성안하고 바로 ISO 이미지를 넣어서 OS Install Device 로 사용이 가능하다.\nVentoy 설치 (for Linux) Ubuntu 시스템에서 Ventoy device 설치를 해본다.\nroot@u-node-1:~/ventoy# wget https://github.com/ventoy/Ventoy/releases/download/v1.0.64/ventoy-1.0.64-linux.tar.gz Release 에서 Linux 용으로 Download 받습니다.\nroot@u-node-1:~/ventoy# gzip -d ventoy-1.0.64-linux.tar.gz root@u-node-1:~/ventoy# tar xvf ventoy-1.0.64-linux.tar root@u-node-1:~/ventoy# cd ventoy-1.0.64/ 이후 위와 같이 uncompress 를 한다.\nVentoy device 생성 아래와 같이 Ventoy2Disk.sh script 를 이용하여 생성이 가능하다.\nroot@u-node-1:~/ventoy/ventoy-1.0.64# ./Ventoy2Disk.sh --help  **********************************************  Ventoy: 1.0.64 x86_64  longpanda admin@ventoy.net  https://www.ventoy.net **********************************************  Usage: Ventoy2Disk.sh CMD [ OPTION ] /dev/sdX  CMD:  -i install Ventoy to sdX (fails if disk already installed with Ventoy)  -I force install Ventoy to sdX (no matter installed or not)  -u update Ventoy in sdX  -l list Ventoy information in sdX   OPTION: (optional)  -r SIZE_MB preserve some space at the bottom of the disk (only for install)  -s/-S enable/disable secure boot support (default is disabled)  -g use GPT partition style, default is MBR (only for install)  -L Label of the 1st exfat partition (default is Ventoy)  -n try non-destructive installation (only for install) 인식된 USB 에 아래와 같이 명령을 실행한다.\nroot@u-node-1:~/ventoy/ventoy-1.0.64# ./Ventoy2Disk.sh -i /dev/vdc  **********************************************  Ventoy: 1.0.64 x86_64  longpanda admin@ventoy.net  https://www.ventoy.net **********************************************  Disk : /dev/vdc Model: Virtio Block Device (virtblk) Size : 4 GB Style: MBR   Attention: You will install Ventoy to /dev/vdc. All the data on the disk /dev/vdc will be lost!!!  Continue? (y/n) y  All the data on the disk /dev/vdc will be lost!!! Double-check. Continue? (y/n) y  Create partitions on /dev/vdc by parted in MBR style ... Done Wait for partitions ... partition exist OK create efi fat fs /dev/vdc2 ... mkfs.fat 4.1 (2017-01-24) success Wait for partitions ... /dev/vdc1 exist OK /dev/vdc2 exist OK partition exist OK Format partition 1 /dev/vdc1 ... mkexfatfs 1.3.0 Creating... done. Flushing... done. File system created successfully. mkexfatfs success writing data to disk ... sync data ... esp partition processing ... Open ventoy efi file 0x610ac0 ventoy x64 efi file size 1757184 ... Open bootx64 efi file 0x610ac0 Open ventoy ia32 efi file 0x610f10 ventoy efi file size 1183744 ... Open bootia32 efi file 0x610ac0  Install Ventoy to /dev/vdc successfully finished. 위와 같이 설치가 완료되면 exfat filesystem type 로 format 된 partition 이 있다.\nroot@u-node-1:~/ventoy/ventoy-1.0.64# lsblk ... vdc 252:32 0 4G 0 disk ├─vdc1 252:33 0 4G 0 part └─vdc2 252:34 0 32M 0 part 추가로 exfat filesystem 을 mount 하기 위해서는 아래와 같이 package 설치가 필요하다.\nroot@u-node-1:~/ventoy/ventoy-1.0.64# apt install exfat-utils ISO 추가 Ventoy 에 ISO 를 추가하기 위해 filesystem 을 mount 한다.\nroot@u-node-1:~# mkdir /imsi root@u-node-1:~# mount /dev/vdc1 /imsi root@u-node-1:~# cd /imsi mount 된 filesystem 에 ISO 이미지를 넣는다.\nroot@u-node-1:/imsi# wget https://mirror.kakao.com/centos/7.9.2009/isos/x86_64/CentOS-7-x86_64-Minimal-2009.iso 이미지 추가 후, 해당 filesystem 은 umount 한다.\nTEST 추가한 이미지로 설치 진행이 잘되는지 테스트 진행해본다.\nVentoy 에서 추가한 ISO 목록이 보이고 테스트를 위해 CentOS ISO 를 선택한다.설치 진행 화면설치 완료 화면[참고] Windows 에서 생성 과정 Windows 에서는 아래와 같이 Ventoy device 를 생성하고 사용하면 된다.\nend 생각보다 IODD 만큼 사용 방법도 간편하며 무료로 사용이 가능하다는 것이 매우 매력적인 오픈소스 솔루션이 였다. ^^\n참고 자료  https://www.ventoy.net/en/doc_start.html https://github.com/ventoy/Ventoy/  ","permalink":"https://chhanz88.github.io/post/2022-02-03-ventoy/","summary":"Intro 시스템 장애로 인해 OS 를 재설치 해야되는 경우가 생겼다.\n시스템팀에서 OS 재설치를 해야되는데 DVD 도 없고 USB 이미지도 없었다.\n급하게 설치를 해야되서 내가 가지고 있는 IODD 를 이용하여 OS 설치를 하게 되었다.\nVentoy 무료로 IODD 와 같이 사용 할 수 있는 것이 있나 찾다가 발견된 것은 Ventoy 이다.\n따로 Boot device 로 생성안하고 바로 ISO 이미지를 넣어서 OS Install Device 로 사용이 가능하다.\nVentoy 설치 (for Linux) Ubuntu 시스템에서 Ventoy device 설치를 해본다.","title":"[Ventoy] ISO 를 바로 사용하는 OS Install Boot Device 생성"},{"content":"흥미 최근 OpenStack 커뮤니티 그룹에서 올라온 글을 보고 기존에 Horizon 의 약한 부분에 대해 매우 아쉬움을 가지고 있던 저는 매우 흥미를 느끼며 이 포스팅을 작성하게 되었습니다.\nPreview 이 흥미로운 Project 는 Skyline 이라는 이름의 새로운 OpenStack Dashboard Project 입니다.\n(https://wiki.openstack.org/wiki/Skyline#Description)\nInstall 테스트에 활용된 OpenStack 환경은 아래와 같습니다.\n* OpenStack wallaby (all-in-one version) [deploy tool `kolla-ansible`] * ubuntu 20.04 Skyline 에서 사용될 User 를 생성합니다.\n(osp) root@u-node-1:/etc/kolla# openstack user create --domain default --password-prompt skyline User Password: Repeat User Password: +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | 1bd73a672fa343569fa4d771eb3f0d23 | | name | skyline | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ Service 프로젝트에 skyline 계정 Admin role 을 추가합니다.\n(osp) root@u-node-1:/etc/kolla# openstack role add --project service --user skyline admin Database 생성 테스트 환경이므로 OpenStack 내부의 mariadb 를 활용합니다.\n(운영 환경에 적용할 경우, 별도의 DB 를 사용하는 것이 좋을 것으로 보입니다.)\nkolla-ansible 배포에서 사용된 database password 를 확인합니다.\n(osp) root@u-node-1:/etc/kolla# cat passwords.yml | grep data ... database_password: R25MrFQko4WhQ8NeTbBZVU36h12v2SREVOrR0fAp ... database 생성 및 권한 부여를 합니다.\n(osp) root@u-node-1:/etc/kolla# mysql -u root -h 10.10.10.11 -p Enter password: {{ database_password }} Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 1975 Server version: 10.3.31-MariaDB-1:10.3.31+maria~focal-log mariadb.org binary distribution  Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.  Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement.  MariaDB [(none)]\u0026gt; CREATE DATABASE IF NOT EXISTS skyline DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; Query OK, 1 row affected (0.013 sec)  MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON skyline.* TO \u0026#39;skyline\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;MySkylineDBPassword\u0026#39;; Query OK, 0 rows affected (0.010 sec)  MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON skyline.* TO \u0026#39;skyline\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;MySkylineDBPassword\u0026#39;; Query OK, 0 rows affected (0.014 sec)  MariaDB [(none)]\u0026gt; Skyline 배포 Source 를 Clone 합니다.\n(osp) root@u-node-1:/etc/kolla# cd /root (osp) root@u-node-1:~# git clone https://opendev.org/skyline/skyline-apiserver Cloning into \u0026#39;skyline-apiserver\u0026#39;... remote: Enumerating objects: 277, done. remote: Counting objects: 100% (277/277), done. remote: Compressing objects: 100% (137/137), done. remote: Total 877 (delta 221), reused 140 (delta 140), pack-reused 600 Receiving objects: 100% (877/877), 595.88 KiB | 1.02 MiB/s, done. Resolving deltas: 100% (497/497), done. Default config file 을 /etc/skyline/skyline.yaml 로 추가하고 설정을 수정합니다.\n(osp) root@u-node-1:~# mkdir /etc/skyline (osp) root@u-node-1:~# cp skyline-apiserver/etc/skyline.yaml.sample /etc/skyline/skyline.yaml default:  access_token_expire: 3600  access_token_renew: 1800  cors_allow_origins: []  database_url: mysql://skyline:MySkylineDBPassword@10.10.10.11:3306/skyline \u0026lt;\u0026lt; 접근이 가능한 database_url 수정 필요 (기존 localhost)  debug: false  log_dir: ./log  secret_key: aCtmgbcUqYUy_HNVg5BDXCaeJgJQzHJXwqbXr0Nmb2o  session_name: session developer:  show_raw_sql: false openstack:  base_domains:  - heat_user_domain  base_roles:  - keystone_system_admin  - keystone_system_reader  - keystone_project_admin  - keystone_project_member  - keystone_project_reader  - nova_system_admin  - nova_system_reader  - nova_project_admin  - nova_project_member  - nova_project_reader  - cinder_system_admin  - cinder_system_reader  - cinder_project_admin  - cinder_project_member  - cinder_project_reader  - glance_system_admin  - glance_system_reader  - glance_project_admin  - glance_project_member  - glance_project_reader  - neutron_system_admin  - neutron_system_reader  - neutron_project_admin  - neutron_project_member  - neutron_project_reader  - heat_system_admin  - heat_system_reader  - heat_project_admin  - heat_project_member  - heat_project_reader  - placement_system_admin  - placement_system_reader  - panko_system_admin  - panko_system_reader  - panko_project_admin  - panko_project_member  - panko_project_reader  - ironic_system_admin  - ironic_system_reader  - octavia_system_admin  - octavia_system_reader  - octavia_project_admin  - octavia_project_member  - octavia_project_reader  default_region: RegionOne  extension_mapping:  fwaas_v2: neutron_firewall  vpnaas: neutron_vpn  interface_type: public  keystone_url: http://10.10.10.250:5000/v3/ \u0026lt;\u0026lt; openstack keystone_url 로 변경  nginx_prefix: /api/openstack  reclaim_instance_interval: 604800  service_mapping:  compute: nova  identity: keystone  image: glance  network: neutron  orchestration: heat  placement: placement  volumev3: cinder  system_admin_roles:  - admin  - system_admin  system_project: service  system_project_domain: Default  system_reader_roles:  - system_reader  system_user_domain: Default  system_user_name: skyline  system_user_password: {{ password }} \u0026lt;\u0026lt; skyline user 생성할때 입력한 password setting:  base_settings:  - flavor_families  - gpu_models  - usb_models  flavor_families:  - architecture: x86_architecture  categories:  - name: general_purpose  properties: []  - name: compute_optimized  properties: []  - name: memory_optimized  properties: []  - name: high_clock_speed  properties: []  - architecture: heterogeneous_computing  categories:  - name: compute_optimized_type_with_gpu  properties: []  - name: visualization_compute_optimized_type_with_gpu  properties: []  gpu_models:  - nvidia_t4  usb_models:  - usb_c Bootstrap 을 진행합니다.\n(osp) root@u-node-1:~/skyline-apiserver/container# docker run -d --name skyline_bootstrap -e KOLLA_BOOTSTRAP=\u0026#34;\u0026#34; -v /etc/skyline/skyline.yaml:/etc/skyline/skyline.yaml --net=host 99cloud/skyline:latest 614a74d2204441cfb42266cefe17eb06eb96a4234fd03b1a7e2ddaf64140ab98  (osp) root@u-node-1:~/skyline-apiserver/container# docker logs skyline_bootstrap + echo \u0026#39;/usr/local/bin/gunicorn -c /etc/skyline/gunicorn.py skyline_apiserver.main:app\u0026#39; + mapfile -t CMD ++ xargs -n 1 ++ tail /run_command + [[ -n 0 ]] + cd /skyline/libs/skyline-apiserver/ + make db_sync poetry run alembic upgrade head Skipping virtualenv creation, as specified in config file. + exit 0 위 과정은 생성한 Database 를 초기화 하는 과정으로 보입니다.\nbootstrap 컨테이너를 제거하고 skyline main application 을 배포합니다.\n(osp) root@u-node-1:~/skyline-apiserver/container# docker rm -f skyline_bootstrap skyline_bootstrap  (osp) root@u-node-1:~/skyline-apiserver/container# docker run -d --name skyline --restart=always -v /etc/skyline/skyline.yaml:/etc/skyline/skyline.yaml --net=host 99cloud/skyline:latest 8cde23649596ef629558889a0c7b6700b5528da63160154292bcfab17bb24f9b  (osp) root@u-node-1:~/skyline-apiserver/container# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8cde23649596 99cloud/skyline:latest  (osp) root@u-node-1:~/skyline-apiserver/container# docker logs -f skyline ... + echo \u0026#39;Running command: /usr/local/bin/gunicorn -c /etc/skyline/gunicorn.py skyline_apiserver.main:app\u0026#39; + exec /usr/local/bin/gunicorn -c /etc/skyline/gunicorn.py skyline_apiserver.main:app + echo \u0026#39;/usr/local/bin/gunicorn -c /etc/skyline/gunicorn.py skyline_apiserver.main:app\u0026#39; + mapfile -t CMD ++ tail /run_command ++ xargs -n 1 + [[ -n \u0026#39;\u0026#39; ]] + nginx-generator -o /etc/nginx/nginx.conf + nginx + echo \u0026#39;Running command: /usr/local/bin/gunicorn -c /etc/skyline/gunicorn.py skyline_apiserver.main:app\u0026#39; + exec /usr/local/bin/gunicorn -c /etc/skyline/gunicorn.py skyline_apiserver.main:app Running command: /usr/local/bin/gunicorn -c /etc/skyline/gunicorn.py skyline_apiserver.main:app 정상적으로 Application 이 작동하면 https://XXXXXXXXXXXXXXXXXXXX:8080 으로 접근이 가능합니다.\nPreview   Login page : region 별로 domain 을 지정 할 수 있도록 되어있습니다. multi region 을 관리하는데 편리함을 위해 만들어진 것으로 보입니다.\n(초기 설치 이후, 중국어로 콘솔이 나와서 매우 당황했으나 언어를 English 로 변경하면 사용하는데 문제가 없었다.)\n  Overview page : 프로젝트 Quota 현황을 볼 수 있습니다.\n  Instance page : Instance 상세 현황을 볼 수 있습니다.\n(Instance 현황은 error 지만 Dashboard 의 화면 구성을 보는데 목적으로 추가하였습니다.)\n  개인적 의견  장점  Horizon 보다 빠른 속도 깔끔하고 정리된 느낌   단점  아직 정보가 부족함    총평 : 기존의 Horizon 보다 사용하기 편리했고 빠른 성능으로 인해 불편함을 느끼지 못함. 매우 기대되는 Project 이다!!\n참고 문서  https://opendev.org/skyline/skyline-apiserver https://satishdotpatel.github.io/openstack-skyline-dashborad/  ","permalink":"https://chhanz88.github.io/post/2022-01-17-openstack-dashboard-skyline/","summary":"흥미 최근 OpenStack 커뮤니티 그룹에서 올라온 글을 보고 기존에 Horizon 의 약한 부분에 대해 매우 아쉬움을 가지고 있던 저는 매우 흥미를 느끼며 이 포스팅을 작성하게 되었습니다.\nPreview 이 흥미로운 Project 는 Skyline 이라는 이름의 새로운 OpenStack Dashboard Project 입니다.\n(https://wiki.openstack.org/wiki/Skyline#Description)\nInstall 테스트에 활용된 OpenStack 환경은 아래와 같습니다.\n* OpenStack wallaby (all-in-one version) [deploy tool `kolla-ansible`] * ubuntu 20.04 Skyline 에서 사용될 User 를 생성합니다.\n(osp) root@u-node-1:/etc/kolla# openstack user create --domain default --password-prompt skyline User Password: Repeat User Password: +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | 1bd73a672fa343569fa4d771eb3f0d23 | | name | skyline | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ Service 프로젝트에 skyline 계정 Admin role 을 추가합니다.","title":"[OpenStack] [Preview] OpenStack Dashboard - Skyline"},{"content":"목차  Kustomize 란? Kustomize 설치 Completion 설정 Kustomize 기본  Kustomize build Run kustomize resources 추가 patch prefix/label secretGenerator 이용 overlay 적용   참고 자료  Kustomize 란? Kustomize는 쿠버네티스 구성을 사용자 정의화하는 도구이다. 이는 애플리케이션 구성 파일을 관리하기 위해 다음 기능들을 가진다.\n 다른 소스에서 리소스 생성 리소스에 대한 교차 편집 필드 설정 리소스 집합을 구성하고 사용자 정의\n(https://kubernetes.io/ko/docs/tasks/manage-kubernetes-objects/kustomization/#kustomize-%EA%B0%9C%EC%9A%94 발췌)  개인적으론 Helm 보다 Kustomize 가 사용하기 좋았다.\nKustomize 설치 아래 명령어를 통해 OS 에 맞게 자동으로 설치 진행된다.\nroot@node1:~# curl -s \u0026#34;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026#34; | bash  {Version:kustomize/v4.4.0 GitCommit:63ec6bdb3d737a7c66901828c5743656c49b60e1 BuildDate:2021-09-27T16:24:12Z GoOs:linux GoArch:amd64} kustomize installed to //root/kustomize 사용의 편의를 위해 Path 를 이동하였다.\nroot@node1:~# mv kustomize /usr/local/bin/  root@node1:~# kustomize --help  Manages declarative configuration of Kubernetes. See https://sigs.k8s.io/kustomize  Usage:  kustomize [command]  Available Commands:  build Build a kustomization target from a directory or URL.  cfg Commands for reading and writing configuration.  completion Generate shell completion script  create Create a new kustomization in the current directory  edit Edits a kustomization file  fn Commands for running functions against configuration.  help Help about any command  version Prints the kustomize version  Flags:  -h, --help help for kustomize  --stack-trace print a stack-trace on error  Additional help topics:  kustomize docs-fn [Alpha] Documentation for developing and invoking Configuration Functions.  kustomize docs-fn-spec [Alpha] Documentation for Configuration Functions Specification.  kustomize docs-io-annotations [Alpha] Documentation for annotations used by io.  kustomize docs-merge [Alpha] Documentation for merging Resources (2-way merge).  kustomize docs-merge3 [Alpha] Documentation for merging Resources (3-way merge).  kustomize tutorials-command-basics [Alpha] Tutorials for using basic config commands.  kustomize tutorials-function-basics [Alpha] Tutorials for using functions.  Use \u0026#34;kustomize [command] --help\u0026#34; for more information about a command. Completion 설정 아래 명령어를 통해 completion 을 설정한다.\nroot@node1:~/test# kustomize completion bash \u0026gt; /etc/bash_completion.d/kustomize Kustomize 기본 Kustomize 의 테스트를 위해 flask 예제 어플리케이션을 준비한다.\nroot@node1:~/test# kubectl create deployment --image=quay.io/chhanz/flask-example-app flash-app -o yaml --dry-run \u0026gt; deployment.yml W1110 13:11:30.171887 79409 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client. 아래와 같이 kustomization.yaml 을 작성한다.\nroot@node1:~/test# cat kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization  resources: - base/deployment.yml \u0026lt;\u0026lt;\u0026lt; 위 kustomization.yaml 은 아래와 같은 파일 구조를 가진다.\n├── base │ └── deployment.yml └── kustomization.yaml kustomize 의 기본은 resources 부분에 어플리케이션을 선언하면 된다.\nKustomize build 아래 명령어를 통해 작성한 kustomization.yaml 를 kubernetes 에 맞는 yaml 형식으로 변환 할 수 있다.\nroot@node1:~/test# kustomize build apiVersion: apps/v1 kind: Deployment metadata:  creationTimestamp: null  labels:  app: flash-app  name: flash-app spec:  replicas: 1  selector:  matchLabels:  app: flash-app  strategy: {}  template:  metadata:  creationTimestamp: null  labels:  app: flash-app  spec:  containers:  - image: quay.io/chhanz/flask-example-app  name: flask-example-app  resources: {} status: {} Run kustomize 생성한 kustomization.yaml 을 이용하여 아래와 같이 어플리케이션을 배포 할 수 있다.\nroot@node1:~/test# kustomize build | kubectl create -f - \u0026lt;\u0026lt;\u0026lt; deployment.apps/flash-app created root@node1:~/test#  root@node1:~/test# kubectl get all NAME READY STATUS RESTARTS AGE pod/flash-app-69f5f676d6-z7wf4 1/1 Running 0 14s  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.233.0.1 \u0026lt;none\u0026gt; 443/TCP 131m  NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/flash-app 1/1 1 1 14s  NAME DESIRED CURRENT READY AGE replicaset.apps/flash-app-69f5f676d6 1 1 1 14s resources 추가 만들어둔 kustomize 에 신규 리소스 service 를 추가해본다.\nroot@node1:~/test# cat kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization  resources: - base/deployment.yml - base/service.yml \u0026lt;\u0026lt;\u0026lt; 추가한 kustomize 를 kubernetes 에 배포한다.\nroot@node1:~/test# kustomize build . | kubectl create -f - service/flask-app created root@node1:~/test# kubectl get all NAME READY STATUS RESTARTS AGE pod/flask-app-669644b479-vh969 1/1 Running 0 31s  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/flask-app ClusterIP 10.233.27.235 \u0026lt;none\u0026gt; 80/TCP 31s \u0026lt;\u0026lt;\u0026lt; service/kubernetes ClusterIP 10.233.0.1 \u0026lt;none\u0026gt; 443/TCP 3h3m  NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/flask-app 1/1 1 1 31s  NAME DESIRED CURRENT READY AGE replicaset.apps/flask-app-669644b479 1 1 1 31s 위와 같이 서비스가 생성 된 것을 확인 할 수 있다.\npatch kustomize 는 resource 의 spec 별로 patch 를 이용하여 Base yaml 을 수정안하고 전체 어플리케이션을 수정 할 수 있다.\n예를 들면 service 의 속성만 기존과 다르게 변경도 할 수 있다.\nroot@node1:~/test# cat patch/svc-nodeport.yml apiVersion: v1 kind: Service metadata:  creationTimestamp: null  labels:  app: flask-app  name: flask-app spec:  type: NodePort 위는 기존에 ClusterIP 로 선언된 type 을 NodePort 로 patch 하는 yaml 이다.\n해당 yaml 을 작성하고 아래와 같이 kustomization.yaml에 추가한다.\nroot@node1:~/test# cat kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization  resources: - base/deployment.yml - base/service.yml  patches: - patch/svc-nodeport.yml \u0026lt;\u0026lt;\u0026lt; 위와 같이 작성한 kustomization.yaml 을 build 해서 확인해보면,\nroot@node1:~/test# kustomize build . apiVersion: v1 kind: Service metadata:  labels:  app: flask-app  name: flask-app spec:  ports:  - port: 80  protocol: TCP  targetPort: 80  selector:  app: flask-app  type: NodePort \u0026lt;\u0026lt;\u0026lt; status:  loadBalancer: {} --- apiVersion: apps/v1 kind: Deployment ... 생략 ... ClusterIP 에서 NodePort 로 변경 된 것을 볼 수 있다.\n변경된 kustomize 를 배포한다.\nroot@node1:~/test# kustomize build . | kubectl apply -f - Warning: resource services/flask-app is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. service/flask-app configured Warning: resource deployments/flask-app is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. deployment.apps/flask-app configured 서비스 확인.\nroot@node1:~/test# kubectl get all NAME READY STATUS RESTARTS AGE pod/flask-app-669644b479-4gjwz 1/1 Terminating 0 2m58s pod/flask-app-669644b479-4lscv 1/1 Running 0 5s  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/flask-app NodePort 10.233.49.196 \u0026lt;none\u0026gt; 80:32312/TCP 5s \u0026lt;\u0026lt;\u0026lt; service/kubernetes ClusterIP 10.233.0.1 \u0026lt;none\u0026gt; 443/TCP 3h17m  NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/flask-app 1/1 1 1 5s  NAME DESIRED CURRENT READY AGE replicaset.apps/flask-app-669644b479 1 1 1 5s  root@node1:~/test# curl node6:32312  Container LAB | POD Working : flask-app-669644b479-4lscv | v=1 root@node1:~/test# curl node5:32312  Container LAB | POD Working : flask-app-669644b479-4lscv | v=1 root@node1:~/test# curl node4:32312  Container LAB | POD Working : flask-app-669644b479-4lscv | v=1 NodePort 로 Patch가 되었다.\nprefix/label 유용한 옵션으로 prefix 와 label 설정이 있다.\n해당 옵션은 배포되는 어플리케이션에 리소스에 prefix 이름을 넣고 label 을 설정하는 옵션이다.\nroot@node1:~/test# cat kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization  resources: - base/deployment.yml - base/service.yml  namePrefix: prefix- \u0026lt;\u0026lt;\u0026lt; commonLabels:  t_label: testtest \u0026lt;\u0026lt;\u0026lt;  patches: - patch/svc-nodeport.yml root@node1:~/test# kubectl get all -l t_label -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/prefix-flask-app-7df7849944-wr9bs 1/1 Running 0 17s 10.233.70.1 node5 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/prefix-flask-app NodePort 10.233.5.176 \u0026lt;none\u0026gt; 80:32705/TCP 18s app=flask-app,t_label=testtest  NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/prefix-flask-app 1/1 1 1 18s flask-example-app quay.io/chhanz/flask-example-app app=flask-app,t_label=testtest  NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/prefix-flask-app-7df7849944 1 1 1 18s flask-example-app quay.io/chhanz/flask-example-app app=flask-app,pod-template-hash=7df7849944,t_label=testtest 위와 같이 prefix 로 설정한 prefix- 가 각 리소스에 name 으로 설정되었고, label 도 t_label=testtest 가 추가된 것을 볼 수 있다.\nsecretGenerator 이용 kustomize 를 통해 secret 도 관리 할 수 있다.\nroot@node1:~/test/base# cat test.conf [test] name=testtest config=test.conf description=testtest config 위 내용은 secret 으로 mount 할 test config 이다.\nbase deployment 수정\nroot@node1:~/test/base# cat deployment.yml apiVersion: apps/v1 kind: Deployment ... 생략 ...  spec:  containers:  - image: quay.io/chhanz/flask-example-app  name: flask-example-app  volumeMounts:  - name: mount-secret \u0026lt;\u0026lt;\u0026lt;  mountPath: \u0026#34;/usr/src/app/data\u0026#34;  resources: {}  volumes:  - name: mount-secret  secret:  secretName: mount-test-secret \u0026lt;\u0026lt;\u0026lt; 위와 같이 해당 어플리케이션은 mount-test-secret 이름의 secret 을 pod 에 mount 하도록 되어 있다.\nkustomization.yaml 작성\nroot@node1:~/test/base# cat kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization  resources: - deployment.yml - service.yml  namePrefix: prefix- commonLabels:  app: flask-app  t_label: testtest  secretGenerator: \u0026lt;\u0026lt;\u0026lt; - name: mount-test-secret  files:  - test.conf  type: Opaque secretGenerator 를 이용하여 mount-test-secret 이름의 secret 을 생성한다.\nbuild 를 하면 아래와 같은 yaml 을 볼 수 있다.\nroot@node1:~/test/base# kustomize build . apiVersion: v1 data:  test.conf: | \u0026lt;\u0026lt;\u0026lt;  W3Rlc3RdCm5hbWU9dGVzdHRlc3QKY29uZmlnPXRlc3QuY29uZgpkZXNjcmlwdGlvbj10ZX \u0026lt;\u0026lt;\u0026lt;  N0dGVzdCBjb25maWcK \u0026lt;\u0026lt;\u0026lt; kind: Secret metadata:  labels:  app: flask-app  t_label: testtest  name: prefix-moun-test-secret-6c8d796cc7  ... 생략 --- apiVersion: apps/v1 kind: Deployment ... 생략   volumes:  - name: mount-secret  secret:  secretName: prefix-moun-test-secret-6c8d796cc7 \u0026lt;\u0026lt;\u0026lt; secret 들에 hash 들이 붙으면서 자동으로 base yaml 을 수정, 적용한다.\n해당 어플리케이션을 배포하고 secret 이 mount 되었는지 확인한다.\n\u0026lt;\u0026lt;K9s-Shell\u0026gt;\u0026gt; Pod: default/prefix-flask-app-787d6569db-k6f98 | Container: flask-example-app /usr/src/app # ls Dockerfile data main.py requirements.txt /usr/src/app # cat data/test.conf [test] name=testtest config=test.conf description=testtest config /usr/src/app # overlay 적용 base yaml 을 다른 kubernetes cluster 혹은 namespace 에 배포를 쉽게 하기 위해 overlay 를 적용 할 수 있다.\nroot@node1:~/test# kubectl create ns prod namespace/prod created  root@node1:~/test# tree . ├── base │ ├── deployment.yml │ ├── kustomization.yaml │ └── service.yml ├── overlays │ └── prod │ └── kustomization.yaml └── patch  └── svc-nodeport.yml  4 directories, 5 file 위와 같이 테스트를 위해 prod 라는 namespace 를 생성하고 overlays/prod/kustomizetion.yaml 을 생성한다.\nroot@node1:~/test# cat overlays/prod/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization  bases: - ../../base  namespace: prod 위와 같이 bases 는 기존에 default namespace 에 배포 되는 어플리케이션을 prod namespace 로 변경하는 kustomization.yaml 이다.\n간단히 설명하면 bases 에 있는 어플리케이션을 다른 overlay 인 prod namespace 로 배포하게 된다.\nbuild 를 해보면,\nroot@node1:~/test# kustomize build overlays/prod/ apiVersion: v1 kind: Service metadata:  creationTimestamp: null  labels:  app: flask-app  t_label: testtest  name: prefix-flask-app  namespace: prod spec:  ports:  - port: 80  protocol: TCP  targetPort: 80  selector:  app: flask-app  t_label: testtest status:  loadBalancer: {} --- apiVersion: apps/v1 kind: Deployment metadata:  creationTimestamp: null  labels:  app: flask-app  t_label: testtest  name: prefix-flask-app  namespace: prod spec:  replicas: 1  selector:  matchLabels:  app: flask-app  t_label: testtest  strategy: {}  template:  metadata:  creationTimestamp: null  labels:  app: flask-app  t_label: testtest  spec:  containers:  - image: quay.io/chhanz/flask-example-app  name: flask-example-app  resources: {} status: {} 위와 같은 yaml 을 생성되고 어플리케이션을 배포하면,\nroot@node1:~/test# kustomize build overlays/prod/ | kubectl create -f - service/prefix-flask-app created deployment.apps/prefix-flask-app created  root@node1:~/test# kubectl get all -n prod NAME READY STATUS RESTARTS AGE pod/prefix-flask-app-7df7849944-chbsb 1/1 Running 0 6s  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/prefix-flask-app ClusterIP 10.233.17.231 \u0026lt;none\u0026gt; 80/TCP 7s  NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/prefix-flask-app 1/1 1 1 6s  NAME DESIRED CURRENT READY AGE replicaset.apps/prefix-flask-app-7df7849944 1 1 1 6s 위와 같이 prod namespace 에 어플리케이션이 배포 된 것을 볼 수 있다.\n참고 자료  https://kubernetes.io/ko/docs/tasks/manage-kubernetes-objects/kustomization/ https://www.jacobbaek.com/1172 https://github.com/kubernetes-sigs/kustomize https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/  ","permalink":"https://chhanz88.github.io/post/2021-12-01-kustomize/","summary":"목차  Kustomize 란? Kustomize 설치 Completion 설정 Kustomize 기본  Kustomize build Run kustomize resources 추가 patch prefix/label secretGenerator 이용 overlay 적용   참고 자료  Kustomize 란? Kustomize는 쿠버네티스 구성을 사용자 정의화하는 도구이다. 이는 애플리케이션 구성 파일을 관리하기 위해 다음 기능들을 가진다.\n 다른 소스에서 리소스 생성 리소스에 대한 교차 편집 필드 설정 리소스 집합을 구성하고 사용자 정의\n(https://kubernetes.io/ko/docs/tasks/manage-kubernetes-objects/kustomization/#kustomize-%EA%B0%9C%EC%9A%94 발췌)  개인적으론 Helm 보다 Kustomize 가 사용하기 좋았다.","title":"[Kubernetes] Kustomize "},{"content":"HTTPS ingress 적용 이전에 배포한 kube-prometheus 의 ingress 를 TLS 적용하여 HTTPS 로 서비스 해보도록 하겠습니다.\nTLS 방식 Kubernetes 에서 사용이 가능한 TLS 방식은 아래와 같습니다.\n아래 내용은 OpenShift 기준으로 작성된 내용이지만 이해하기 좋아서 첨부합니다.\n  Clear 방식 (clear to clear) : 일반적인 HTTP 서비스 Edge 방식 (TLS to clear) : client 와 ingress 구간을 TLS 암호화하는 방식 Re-encrypt 방식 (TLS1 to TLS2) : ingress 기점으로 TLS 암호화를 다르게 하는 방식(두번의 TLS 암호화) Pass-Through 방식 (TLS1 to TLS1) : Application 단에서 TLS 암호화 처리하는 방식   TLS 적용 위의 설명된 방식중 Edge 방식을 적용 해보도록 하겠습니다.\n ┌──┐ ┌──────────────────┐  │ │ │ │  │ │ │ ┌─────┐ │ ┌───────────┐ HTTPS │ │ HTTP │ │APP1 │ │ │ Client │ ───────────────►│ │ ────────────────────► │ └─────┘ │ └───────────┘ │ │ │ ┌──────┐ │  │ │ │ │ APP2 │ │  │ │ │ └──────┘ │  │ │ │ │  └──┘ └──────────────────┘  Ingress Pod Diagram 을 그려본다면 위와 같은 방식으로 구현된다고 생각하면 좋습니다.\nTLS Secret 생성 아래와 같이 TLS Secret 을 생성합니다.\n#!/bin/bash  BASECRT=`base64 -w0 STAR_chhan_testdom.crt` BASEKEY=`base64 -w0 STAR_chhan_testdom.key`  cat \u0026lt;\u0026lt; EOF \u0026gt; STAR_chhan_testdom-tls-secret.yml apiVersion: v1 data:  tls.crt: ${BASECRT}  tls.key: ${BASEKEY} kind: Secret metadata:  name: prometheus-tls-secret  namespace: monitoring type: kubernetes.io/tls EOF  echo \u0026#34;done generate file \u0026#39;STAR_chhan_testdom-tls-secret.yml\u0026#39;\u0026#34; crt 와 key 파일을 base64 로 인코딩을 하여 yaml 을 편하게 생성하게 위해 스크립트를 작성했습니다.\n(TLS 는 wildcard TLS 입니다.)\n$ cat STAR_chhan_testdom-tls-secret.yml apiVersion: v1 data:  tls.crt: Cxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  tls.key: Kxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx kind: Secret metadata:  name: prometheus-tls-secret  namespace: monitoring type: kubernetes.io/tls 생성된 secret.yml 을 이용하여 TLS Secret 을 생성합니다.\n$ kubectl create -f STAR_chhan_testdom-tls-secret.yml secret/prometheus-tls-secret created Ingress 생성 아래와 같이 Ingress 에 TLS 를 적용하여 Edge 방식을 사용하는 Ingress 를 생성합니다.\n$ cat \u0026lt;\u0026lt; EOF \u0026gt; prometheus-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata:  annotations:  kubernetes.io/ingress.class: nginx  name: prometheus-ingress  namespace: monitoring spec:  tls:  - hosts:  - \u0026#34;*.chhanz.testdom\u0026#34;  - secretName: prometheus-tls-secret  rules:  - host: monitor-prometheus.chhanz.testdom  http:  paths:  - pathType: Prefix  path: \u0026#34;/\u0026#34;  backend:  service:  name: prometheus-k8s  port:  number: 9090 $ kubectl create -f prometheus-ingress.yaml Ingress 생성\nTLS 인증 확인 아래와 같이 Client 에서 openssl 명령어 혹은 Web Browser 를 통해 확인이 가능합니다.\nchhan@chhanPC:/mnt/c/Users/chhanz$ openssl s_client -servername monitor-prometheus.chhanz.testdom --connect monitor-prometheus.chhanz.testdom:443  ...TL;DR...  --- Certificate chain  0 s:CN = *.chhanz.testdom \u0026lt;\u0026lt;\u0026lt;  i:C = SXXXXXX, ST = SXXXXXX, L = SXXXXXX, O = SXXXXXX, CN = SXXXXXX ---  ...TL;DR... NGINX Ingress 의 주요 annotations    annotations 속성     nginx.ingress.kubernetes.io/backend-protocol: HTTPS Ingress 이후, Backend 구간을 HTTPS 통신을 하도록 하는 옵션   nginx.ingress.kubernetes.io/ssl-redirect 기본적으로 NGINX Ingress 는 TLS 가 활성화 되면 HTTPS 로 Redirect 하는데 해당 옵션을 통해 비활성화를 할 수 있다. (기본값 true)   nginx.ingress.kubernetes.io/force-ssl-redirect 강제로 HTTPS 로 Redirect 하는 옵션, TLS 인증서가 없는 경우에도 강제로 HTTPS 로 Redirect 할 수 있다.    이 외 annotations 는 문서 에서 확인이 가능합니다.\n참고 자료  Route : https://cloud.redhat.com/blog/self-serviced-end-to-end-encryption-approaches-for-applications-deployed-in-openshift https://kubernetes.io/ko/docs/concepts/services-networking/ingress/#tls https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/  ","permalink":"https://chhanz88.github.io/post/2021-11-08-kube-tls-edge/","summary":"HTTPS ingress 적용 이전에 배포한 kube-prometheus 의 ingress 를 TLS 적용하여 HTTPS 로 서비스 해보도록 하겠습니다.\nTLS 방식 Kubernetes 에서 사용이 가능한 TLS 방식은 아래와 같습니다.\n아래 내용은 OpenShift 기준으로 작성된 내용이지만 이해하기 좋아서 첨부합니다.\n  Clear 방식 (clear to clear) : 일반적인 HTTP 서비스 Edge 방식 (TLS to clear) : client 와 ingress 구간을 TLS 암호화하는 방식 Re-encrypt 방식 (TLS1 to TLS2) : ingress 기점으로 TLS 암호화를 다르게 하는 방식(두번의 TLS 암호화) Pass-Through 방식 (TLS1 to TLS1) : Application 단에서 TLS 암호화 처리하는 방식   TLS 적용 위의 설명된 방식중 Edge 방식을 적용 해보도록 하겠습니다.","title":"[Kubernetes] HTTPS ingress 적용 (use prometheus)"},{"content":"kolla-ansible 이란? Kolla-Ansible 은 Docker 컨테이너에 OpenStack 서비스 및 인프라 구성 요소를 배포하는 Tool 이며, OpenStack 클라우드 운영을 위한 프로덕션 준비 컨테이너 및 배포 도구를 제공합니다.\ndeploy openstack venv 환경 설정 python venv 환경을 아래와 같이 생성합니다.\nroot@u-node-0:~# python3 -m venv osp root@u-node-0:~# . osp/bin/activate requirement module 설치 (osp) root@u-node-0:~# pip install -U pip (osp) root@u-node-0:~# pip install \u0026#39;ansible\u0026lt;3.0\u0026#39; kolla-ansible 설치 (osp) root@u-node-0:~# pip install kolla-ansible kolla-ansible config 아래와 같이 config 및 inventory template 를 copy 하고 ansible 설정을 합니다.\n(osp) root@u-node-0:~# sudo mkdir -p /etc/kolla (osp) root@u-node-0:~# cp osp/share/kolla-ansible/etc_examples/kolla/* /etc/kolla/ (osp) root@u-node-0:/etc/kolla# cp ~/osp/share/kolla-ansible/ansible/inventory/* .  (osp) root@u-node-0:/etc/kolla# cat /etc/ansible/ansible.cfg [defaults] host_key_checking=False pipelining=True forks=100 수정 globals.yml 배포할 환경에 맞게 globals.yml 를 수정한다.\n(openstack 이 배포될 환경에 맞게 수정이 필요하다.)\n--- kolla_base_distro: \u0026#34;ubuntu\u0026#34; kolla_install_type: \u0026#34;source\u0026#34; openstack_release: \u0026#34;wallaby\u0026#34; kolla_internal_vip_address: \u0026#34;10.10.10.250\u0026#34; kolla_external_vip_address: \u0026#34;10.10.10.250\u0026#34; docker_registry: 10.10.10.10:4000 network_interface: \u0026#34;ens4\u0026#34; kolla_external_vip_interface: \u0026#34;ens4\u0026#34; api_interface: \u0026#34;ens4\u0026#34; neutron_external_interface: \u0026#34;ens5\u0026#34; enable_chrony: \u0026#34;yes\u0026#34; enable_cinder: \u0026#34;yes\u0026#34; inventory 수정 아래와 같이 배포 환경에 맞게 multinode 를 수정한다.\n(osp) root@u-node-0:/etc/kolla# cat multinode [control] control01 ansible_host=\u0026#34;u-ctl-4\u0026#34; control02 ansible_host=\u0026#34;u-ctl-5\u0026#34; control03 ansible_host=\u0026#34;u-ctl-6\u0026#34;  [network] network01 ansible_host=\u0026#34;u-network-7\u0026#34;  [compute] compute01 ansible_host=\u0026#34;u-compute-8\u0026#34; compute02 ansible_host=\u0026#34;u-compute-9\u0026#34; compute03 ansible_host=\u0026#34;u-compute-10\u0026#34;  [monitoring] network01 ansible_host=\u0026#34;u-network-7\u0026#34;  [storage] control01 ansible_host=\u0026#34;u-ctl-4\u0026#34; control02 ansible_host=\u0026#34;u-ctl-5\u0026#34; control03 ansible_host=\u0026#34;u-ctl-6\u0026#34;  ... 생략 ... kolla-password 생성 openstack service 에서 사용 될 password 를 생성한다.\n(osp) root@u-node-0:/etc/kolla# kolla-genpwd bootstrap 배포전 bootstrap 을 수행한다.\n(osp) root@u-node-0:/etc/kolla# kolla-ansible -i ./multinode bootstrap-servers prechecks precheck 를 수행한다.\n(osp) root@u-node-0:/etc/kolla# kolla-ansible -i ./multinode prechecks pull image 각 node에 image 를 pull 받는다.\n(osp) root@u-node-0:/etc/kolla# kolla-ansible -i ./multinode pull deploy Kolla-ansible을 이용하여 openstack 을 배포한다.\n(osp) root@u-node-0:/etc/kolla# kolla-ansible -i ./multinode deploy post openstack openstack cli 를 설치한다.\npip install python3-openstackclient 아래와 같이 post-deploy 옵션으로 admin-openrc.sh 을 생성한다.\nkolla-ansible post-deploy . /etc/kolla/admin-openrc.sh Check 위와 같이 horizon dashboard 로 접근이 가능하다.\nadmin password 는 전 과정에서 생성한 passwords.yml 을 참고한다.\n하기 문서를 참고하여 서비스가 정상적으로 배포가 되었는지 확인한다.\n(https://docs.openstack.org/ocata/ko_KR/install-guide-rdo/nova-verify.html)\n참고 자료  https://docs.openstack.org/kolla-ansible/latest/user/quickstart.html https://docs.openstack.org/ocata/ko_KR/install-guide-rdo/nova-verify.html  ","permalink":"https://chhanz88.github.io/post/2021-10-22-deploy-osp-wallaby-kolla-ansible/","summary":"kolla-ansible 이란? Kolla-Ansible 은 Docker 컨테이너에 OpenStack 서비스 및 인프라 구성 요소를 배포하는 Tool 이며, OpenStack 클라우드 운영을 위한 프로덕션 준비 컨테이너 및 배포 도구를 제공합니다.\ndeploy openstack venv 환경 설정 python venv 환경을 아래와 같이 생성합니다.\nroot@u-node-0:~# python3 -m venv osp root@u-node-0:~# . osp/bin/activate requirement module 설치 (osp) root@u-node-0:~# pip install -U pip (osp) root@u-node-0:~# pip install \u0026#39;ansible\u0026lt;3.0\u0026#39; kolla-ansible 설치 (osp) root@u-node-0:~# pip install kolla-ansible kolla-ansible config 아래와 같이 config 및 inventory template 를 copy 하고 ansible 설정을 합니다.","title":"[OpenStack] kolla-ansible 을 이용하여 OpenStack multinode 배포 (wallaby)"},{"content":"Registry kolla-build 를 통해 생성한 이미지를 저장할 사설 Registry 를 만든다.\ndeploy registry 아래와 같이 배포 진행한다.\n(osp) root@u-node-0:/etc/kolla# docker run -d -p 4000:5000 --name registry -v /etc/kolla/build:/var/lib/registry registry:2 아래와 같이 insecure-registries 를 설정한다.\n추가로 kolla-image 를 pull 받을 node 들도 동일하게 설정을 한다.\n(osp) root@u-node-0:/etc/docker# cat /etc/docker/daemon.json {  \u0026#34;insecure-registries\u0026#34; : [\u0026#34;10.10.10.10:4000\u0026#34;] } (osp) root@u-node-0:/etc/docker# Install kolla-build kolla-build 를 사용하기 위해 아래와 같이 pip module 을 설치합니다.\n(osp) root@u-node-0:/etc/kolla# pip install kollaCollecting kolla  Downloading kolla-12.0.1-py3-none-any.whl (381 kB)  |████████████████████████████████| 381 kB 1.8 MB/s Requirement already satisfied: oslo.config\u0026gt;=5.1.0 in /root/osp/lib/python3.8/site-packages (from kolla) (8.7.1) Collecting docker\u0026gt;=2.4.2  Downloading docker-5.0.0-py2.py3-none-any.whl (146 kB)  |████████████████████████████████| 146 kB 20.8 MB/s Requirement already satisfied: pbr!=2.1.0,\u0026gt;=2.0.0 in /root/osp/lib/python3.8/site-packages (from kolla) (5.6.0) Requirement already satisfied: Jinja2\u0026gt;=2.8 in /root/osp/lib/python3.8/site-packages (from kolla) (3.0.1) Collecting GitPython\u0026gt;=1.0.1  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)  |████████████████████████████████| 170 kB 22.2 MB/s Collecting websocket-client\u0026gt;=0.32.0  Downloading websocket_client-1.1.0-py2.py3-none-any.whl (68 kB)  |████████████████████████████████| 68 kB 9.0 MB/s Requirement already satisfied: requests!=2.18.0,\u0026gt;=2.14.2 in /root/osp/lib/python3.8/site-packages (from docker\u0026gt;=2.4.2-\u0026gt;kolla) (2.26.0) Collecting gitdb\u0026lt;5,\u0026gt;=4.0.1  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)  |████████████████████████████████| 63 kB 2.5 MB/s Collecting smmap\u0026lt;5,\u0026gt;=3.0.1  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB) Requirement already satisfied: MarkupSafe\u0026gt;=2.0 in /root/osp/lib/python3.8/site-packages (from Jinja2\u0026gt;=2.8-\u0026gt;kolla) (2.0.1) Requirement already satisfied: oslo.i18n\u0026gt;=3.15.3 in /root/osp/lib/python3.8/site-packages (from oslo.config\u0026gt;=5.1.0-\u0026gt;kolla) (5.0.1) Requirement already satisfied: debtcollector\u0026gt;=1.2.0 in /root/osp/lib/python3.8/site-packages (from oslo.config\u0026gt;=5.1.0-\u0026gt;kolla) (2.2.0) Requirement already satisfied: stevedore\u0026gt;=1.20.0 in /root/osp/lib/python3.8/site-packages (from oslo.config\u0026gt;=5.1.0-\u0026gt;kolla) (3.3.0) Requirement already satisfied: PyYAML\u0026gt;=5.1 in /root/osp/lib/python3.8/site-packages (from oslo.config\u0026gt;=5.1.0-\u0026gt;kolla) (5.4.1) Requirement already satisfied: netaddr\u0026gt;=0.7.18 in /root/osp/lib/python3.8/site-packages (from oslo.config\u0026gt;=5.1.0-\u0026gt;kolla) (0.8.0) Requirement already satisfied: rfc3986\u0026gt;=1.2.0 in /root/osp/lib/python3.8/site-packages (from oslo.config\u0026gt;=5.1.0-\u0026gt;kolla) (1.5.0) Requirement already satisfied: six\u0026gt;=1.10.0 in /root/osp/lib/python3.8/site-packages (from debtcollector\u0026gt;=1.2.0-\u0026gt;oslo.config\u0026gt;=5.1.0-\u0026gt;kolla) (1.16.0) Requirement already satisfied: wrapt\u0026gt;=1.7.0 in /root/osp/lib/python3.8/site-packages (from debtcollector\u0026gt;=1.2.0-\u0026gt;oslo.config\u0026gt;=5.1.0-\u0026gt;kolla) (1.12.1) Requirement already satisfied: charset-normalizer~=2.0.0 in /root/osp/lib/python3.8/site-packages (from requests!=2.18.0,\u0026gt;=2.14.2-\u0026gt;docker\u0026gt;=2.4.2-\u0026gt;kolla) (2.0.4) Requirement already satisfied: urllib3\u0026lt;1.27,\u0026gt;=1.21.1 in /root/osp/lib/python3.8/site-packages (from requests!=2.18.0,\u0026gt;=2.14.2-\u0026gt;docker\u0026gt;=2.4.2-\u0026gt;kolla) (1.26.6) Requirement already satisfied: idna\u0026lt;4,\u0026gt;=2.5 in /root/osp/lib/python3.8/site-packages (from requests!=2.18.0,\u0026gt;=2.14.2-\u0026gt;docker\u0026gt;=2.4.2-\u0026gt;kolla) (3.2) Requirement already satisfied: certifi\u0026gt;=2017.4.17 in /root/osp/lib/python3.8/site-packages (from requests!=2.18.0,\u0026gt;=2.14.2-\u0026gt;docker\u0026gt;=2.4.2-\u0026gt;kolla) (2021.5.30) Installing collected packages: smmap, websocket-client, gitdb, GitPython, docker, kolla Successfully installed GitPython-3.1.18 docker-5.0.0 gitdb-4.0.7 kolla-12.0.1 smmap-4.0.0 websocket-client-1.1.0 build image 아래 명령을 통해 kolla-image 를 build 할 수 있다.\n(osp) root@u-node-0:/etc/kolla# kolla-build --registry 10.10.10.10:4000 -b ubuntu -t source --openstack-release wallaby --push --tag wallaby base image 는 ubuntu 를 사용하고 openstack version 은 wallaby 로 지정했다.\nbuild 가 완료 되면 아래와 같이 지정한 tag 로 image 가 build 된 것을 볼 수 있다.\nosp) root@u-node-0:/etc/kolla# docker images REPOSITORY TAG IMAGE ID CREATED SIZE 10.10.10.10:4000/kolla/ubuntu-source-bifrost-deploy wallaby 1b1326827dbf 45 minutes ago 1.87GB 10.10.10.10:4000/kolla/ubuntu-source-bifrost-base wallaby cd0eb65edda7 45 minutes ago 1.85GB 10.10.10.10:4000/kolla/ubuntu-source-ironic-conductor wallaby 7d06430242ef 47 minutes ago 1.32GB 10.10.10.10:4000/kolla/ubuntu-source-cinder-volume wallaby a6ee5035a3f1 49 minutes ago 1.25GB 10.10.10.10:4000/kolla/ubuntu-source-monasca-api wallaby 0503276e0e16 49 minutes ago 942MB 10.10.10.10:4000/kolla/ubuntu-source-cinder-backup wallaby fcec7cfbfc4c 49 minutes ago 1.24GB 10.10.10.10:4000/kolla/ubuntu-source-cinder-api wallaby 64557aaef6e5 49 minutes ago 1.22GB 10.10.10.10:4000/kolla/ubuntu-source-neutron-infoblox-ipam-agent wallaby aa23a9c3ad21 49 minutes ago 1.01GB 10.10.10.10:4000/kolla/ubuntu-source-manila-share wallaby 8cdd929f6a20 49 minutes ago 1.07GB 10.10.10.10:4000/kolla/ubuntu-source-monasca-agent wallaby 63c0f5d7b3f7 49 minutes ago 897MB 10.10.10.10:4000/kolla/ubuntu-source-cinder-scheduler wallaby 65ac9546103e 49 minutes ago 1.22GB 10.10.10.10:4000/kolla/ubuntu-source-watcher-applier wallaby b1b492b6db10 49 minutes ago 886MB 10.10.10.10:4000/kolla/ubuntu-source-monasca-persister wallaby dbd1c1090cbd 49 minutes ago 895MB 10.10.10.10:4000/kolla/ubuntu-source-monasca-notification wallaby 4e1d2960c5cd 49 minutes ago 885MB 10.10.10.10:4000/kolla/ubuntu-source-watcher-api wallaby d526a3cd9e21 49 minutes ago 886MB 10.10.10.10:4000/kolla/ubuntu-source-watcher-engine wallaby 82faadd15d21 49 minutes ago 886MB 10.10.10.10:4000/kolla/ubuntu-source-cyborg-agent wallaby 9b2d20d94f53 49 minutes ago 875MB 10.10.10.10:4000/kolla/ubuntu-source-magnum-api wallaby 7275d24d098a 50 minutes ago 916MB  ... 참고 자료  https://docs.openstack.org/kolla/latest/admin/image-building.html  ","permalink":"https://chhanz88.github.io/post/2021-10-21-kolla-build/","summary":"Registry kolla-build 를 통해 생성한 이미지를 저장할 사설 Registry 를 만든다.\ndeploy registry 아래와 같이 배포 진행한다.\n(osp) root@u-node-0:/etc/kolla# docker run -d -p 4000:5000 --name registry -v /etc/kolla/build:/var/lib/registry registry:2 아래와 같이 insecure-registries 를 설정한다.\n추가로 kolla-image 를 pull 받을 node 들도 동일하게 설정을 한다.\n(osp) root@u-node-0:/etc/docker# cat /etc/docker/daemon.json {  \u0026#34;insecure-registries\u0026#34; : [\u0026#34;10.10.10.10:4000\u0026#34;] } (osp) root@u-node-0:/etc/docker# Install kolla-build kolla-build 를 사용하기 위해 아래와 같이 pip module 을 설치합니다.\n(osp) root@u-node-0:/etc/kolla# pip install kollaCollecting kolla  Downloading kolla-12.","title":"[OpenStack] kolla-build 를 이용한 kolla image build (wallaby)"},{"content":"kube-prometheus 배포 Kubernetes cluster 를 모니터링 하기 위해 Prometheus 를 구성하고 사용하려고 한다.\nPrometheus 에는 다양한 배포 방법이 있으나 이번 글에선 kube-prometheus 를 이용하여 Kubernetes Cluster 에 배포 해보도록 한다.\nKubernetes Cluster 정보 사용된 Kubernetes Cluster 환경은 아래와 같습니다.\nroot@node1:~# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready control-plane,master 41d v1.20.7 192.168.200.26 \u0026lt;none\u0026gt; Ubuntu 20.04.2 LTS 5.4.0-80-generic cri-o://1.20.3 node2 Ready control-plane,master 41d v1.20.7 192.168.200.81 \u0026lt;none\u0026gt; Ubuntu 20.04.2 LTS 5.4.0-80-generic cri-o://1.20.3 node3 Ready control-plane,master 41d v1.20.7 192.168.200.145 \u0026lt;none\u0026gt; Ubuntu 20.04.2 LTS 5.4.0-80-generic cri-o://1.20.3 node4 Ready \u0026lt;none\u0026gt; 41d v1.20.7 192.168.200.108 \u0026lt;none\u0026gt; Ubuntu 20.04.2 LTS 5.4.0-80-generic cri-o://1.20.3 node5 Ready \u0026lt;none\u0026gt; 41d v1.20.7 192.168.200.61 \u0026lt;none\u0026gt; Ubuntu 20.04.2 LTS 5.4.0-80-generic cri-o://1.20.3 node6 Ready \u0026lt;none\u0026gt; 13d v1.20.7 192.168.200.116 \u0026lt;none\u0026gt; Ubuntu 20.04.2 LTS 5.4.0-80-generic cri-o://1.20.5 Persistent Volume 는 StorageClass 를 이용하고 ceph-csi 를 이용하여 rbd 를 제공 받습니다.\nroot@node1:~# kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE csi-rbd-sc rbd.csi.ceph.com Delete Immediate true 13d clone kuber-prometheus source 를 clone 합니다.\nroot@node1:~/imsi# git clone https://github.com/prometheus-operator/kube-prometheus.git Cloning into \u0026#39;kube-prometheus\u0026#39;... remote: Enumerating objects: 13531, done. remote: Counting objects: 100% (2030/2030), done. remote: Compressing objects: 100% (864/864), done. remote: Total 13531 (delta 1270), reused 1603 (delta 993), pack-reused 11501 Receiving objects: 100% (13531/13531), 6.68 MiB | 12.56 MiB/s, done. Resolving deltas: 100% (8399/8399), done. Kubernetes Version 에 맞는 release version 을 선택합니다.\nKubernetes compatibility matrix 는 해당 문서를 참고 합니다.\nroot@node1:~/imsi# cd kube-prometheus/ root@node1:~/imsi/kube-prometheus# git checkout release-0.8 Branch \u0026#39;release-0.8\u0026#39; set up to track remote branch \u0026#39;release-0.8\u0026#39; from \u0026#39;origin\u0026#39;. Switched to a new branch \u0026#39;release-0.8\u0026#39; source 수정 Prometheus-operator 는 Kubernetes CRD 를 사용하여 관리, 배포 됩니다. Persistent Volume 사용을 위해 prometheus 와 alertmanager 는 아래와 같이 CRD spec 에 맞게 수정합니다.\nprometheus-prometheus.yaml 수정 root@node1:~/imsi/kube-prometheus/manifests# git diff prometheus-prometheus.yaml diff --git a/manifests/prometheus-prometheus.yaml b/manifests/prometheus-prometheus.yaml index e45a86f..69cb53a 100644 --- a/manifests/prometheus-prometheus.yaml +++ b/manifests/prometheus-prometheus.yaml @@ -46,3 +46,10 @@ spec:  serviceMonitorNamespaceSelector: {}  serviceMonitorSelector: {}  version: 2.26.0 + storage: + volumeClaimTemplate: + spec: + storageClassName: csi-rbd-sc + resources: + requests: + storage: 50Gi alertmanager-alertmanager.yaml 수정 root@node1:~/imsi/kube-prometheus/manifests# git diff alertmanager-alertmanager.yaml diff --git a/manifests/alertmanager-alertmanager.yaml b/manifests/alertmanager-alertmanager.yaml index f4c02a7..008e0df 100644 --- a/manifests/alertmanager-alertmanager.yaml +++ b/manifests/alertmanager-alertmanager.yaml @@ -33,3 +33,10 @@ spec:  runAsUser: 1000  serviceAccountName: alertmanager-main  version: 0.21.0 + storage: + volumeClaimTemplate: + spec: + storageClassName: csi-rbd-sc + resources: + requests: + storage: 10Gi 위와 같이 StorageClass 는 csi-rbd-sc 를 사용하고 CRD spec 에 맞게 volumeClaimTemplate 작성합니다.\ngrafana-pvc.yaml 작성 grafana 의 경우, CRD 로 관리가 안되므로 grafana 의 pvc 생성을 합니다.\nroot@node1:~/imsi/kube-prometheus/manifests# cat grafana-pvc.yaml  apiVersion: v1 kind: PersistentVolumeClaim metadata:  name: grafana-storage-pvc  namespace: monitoring spec:  storageClassName: csi-rbd-sc  accessModes:  - ReadWriteOnce  resources:  requests:  storage: 50Gi grafana-deployment.yaml 수정 grafana 에서 pvc 를 사용하도록 아래와 같이 수정합니다.\nroot@node1:~/imsi/kube-prometheus/manifests# git diff grafana-deployment.yaml diff --git a/manifests/grafana-deployment.yaml b/manifests/grafana-deployment.yaml index c69b637..446a486 100644 --- a/manifests/grafana-deployment.yaml +++ b/manifests/grafana-deployment.yaml @@ -130,8 +130,9 @@ spec:  runAsUser: 65534  serviceAccountName: grafana  volumes: - - emptyDir: {} - name: grafana-storage + - name: grafana-storage + persistentVolumeClaim: + claimName: grafana-storage-pvc deploy 아래와 같이 kube-prometheus 를 배포합니다.\nroot@node1:~/imsi/kube-prometheus# kubectl create -f ./manifests/setup/ namespace/monitoring created customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created clusterrole.rbac.authorization.k8s.io/prometheus-operator created clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created deployment.apps/prometheus-operator created service/prometheus-operator created serviceaccount/prometheus-operator created root@node1:~/imsi/kube-prometheus# kubectl get all -n monitoring NAME READY STATUS RESTARTS AGE pod/prometheus-operator-7775c66ccf-4ckkh 2/2 Running 0 18s ... prometheus-operator resource 가 시작되면 다음 resource 를 배포합니다.\nroot@node1:~/imsi/kube-prometheus# kubectl create -f ./manifests/ ... check 아래와 같이 CRD 를 통해 상태를 확인합니다.\nroot@node1:~/imsi/kube-prometheus# kubectl get prometheus -n monitoring NAME VERSION REPLICAS AGE k8s 2.26.0 2 23s  root@node1:~/imsi/kube-prometheus# kubectl get alertmanager -n monitoring NAME VERSION REPLICAS AGE main 0.21.0 3 41s root@node1:~/imsi/kube-prometheus/manifests# kubectl get all -n monitoring NAME READY STATUS RESTARTS AGE pod/alertmanager-main-0 2/2 Running 0 3m21s pod/alertmanager-main-1 2/2 Running 0 2m41s pod/alertmanager-main-2 2/2 Running 0 9m32s pod/blackbox-exporter-55c457d5fb-2ddng 3/3 Running 0 9m32s pod/grafana-795d45f967-89jmb 1/1 Running 0 9m29s pod/kube-state-metrics-76f6cb7996-69x45 3/3 Running 0 9m29s pod/node-exporter-clwhk 2/2 Running 0 9m27s pod/node-exporter-kslcr 2/2 Running 0 9m28s pod/node-exporter-mmrm6 2/2 Running 0 9m27s pod/node-exporter-rxn79 2/2 Running 0 9m27s pod/node-exporter-shxrk 2/2 Running 0 9m27s pod/node-exporter-x5vjm 2/2 Running 0 9m27s pod/prometheus-adapter-59df95d9f5-7tlcr 1/1 Running 0 9m26s pod/prometheus-adapter-59df95d9f5-nghzv 1/1 Running 0 9m26s pod/prometheus-k8s-0 2/2 Running 1 9m22s pod/prometheus-k8s-1 2/2 Running 1 9m22s pod/prometheus-operator-7775c66ccf-4ckkh 2/2 Running 0 10m ... pod 가 run 되면 kube-prometheus 가 배포 완료 되었습니다.\nexpose grafana 접근을 위해 grafana service 를 수정합니다.\nroot@node1:~/imsi/kube-prometheus/manifests# kubectl get service -n monitoring grafana NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana ClusterIP 10.233.1.30 \u0026lt;none\u0026gt; 3000/TCP 9m31s  root@node1:~/imsi/kube-prometheus/manifests# kubectl edit service/grafana -n monitoring service/grafana edited  root@node1:~/imsi/kube-prometheus/manifests# kubectl get service -n monitoring grafana NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana NodePort 10.233.1.30 \u0026lt;none\u0026gt; 3000:30020/TCP 11m WEB GUI 또한 기존 grafana 와 같이 dashboard 를 생성하고 Import 도 가능합니다.\n참고 문서  https://github.com/prometheus-operator/kube-prometheus#kubernetes-compatibility-matrix https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/  ","permalink":"https://chhanz88.github.io/post/2021-09-30-deploy-kube-prometheus/","summary":"kube-prometheus 배포 Kubernetes cluster 를 모니터링 하기 위해 Prometheus 를 구성하고 사용하려고 한다.\nPrometheus 에는 다양한 배포 방법이 있으나 이번 글에선 kube-prometheus 를 이용하여 Kubernetes Cluster 에 배포 해보도록 한다.\nKubernetes Cluster 정보 사용된 Kubernetes Cluster 환경은 아래와 같습니다.\nroot@node1:~# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready control-plane,master 41d v1.20.7 192.168.200.26 \u0026lt;none\u0026gt; Ubuntu 20.04.2 LTS 5.4.0-80-generic cri-o://1.20.3 node2 Ready control-plane,master 41d v1.","title":"[Kubernetes] kube-prometheus 배포 (use storageclass)"},{"content":"Ceph 란? Ceph는 오픈소스 소프트웨어(Software Defined Storage) 스토리지 플랫폼으로 단일 분산 컴퓨터 클러스터에 object 스토리지를 구현하고 object, block 및 file Level 의 스토리지 기능을 제공한다.\n또한 single point of failure 이 없는 완전히 분산된 운영을 주로 목표로 하며 엑사바이트 수준으로 scale-out이 가능하다.\n소프트웨어 정의 스토리지(Software Defined Storage)를 선택해야 하는 이유 소프트웨어 정의 스토리지의 주요 이점은 다음과 같다.\n 비용 대비 성능 절충 : 애플리케이션이 동일한 소프트웨어 스택을 사용하여 서로 다른 하드웨어 및 내구성 구성의 성능 및 비용 절충을 선택할 수 있다. 유연한 인터페이스 : 산업 표준 API를 선택하거나, 애플리케이션에 클라이언트 라이브러리를 내장하거나, 필요한 경우 독점 API를 사용할 수 있다. 다양한 스토리지 구현 : object, block 및 file 추상화 전반에 걸쳐 동일한 스토리지 소프트웨어 스택을 활용하여 R\u0026amp;D 및 운영 비용을 절감한다.  Ceph architecture  Ceph OSD daemon : Ceph OSD는 Ceph 클라이언트를 대신하여 데이터를 저장한다. 또한 Ceph OSD는 Ceph 노드의 CPU, 메모리 및 네트워킹을 활용하여 data replication, erasure coding, rebalancing, recovery, monitoring 및 report 기능을 수행한다. Ceph Monitor : Ceph Monitor는 Ceph 스토리지 클러스터의 현재 상태에 대한 Ceph 스토리지 클러스터 맵의 마스터 복사본을 유지한다.\n모니터에는 높은 일관성이 필요하며, Ceph 스토리지 클러스터의 상태에 대한 합의를 보장하기 위해 Paxos 알고리즘을 사용한다. Ceph Manager : Ceph Manager는 Ceph Monitor 대신 placement groups(PG), 프로세스 메타데이터 및 호스트 메타데이터에 대한 자세한 정보를 유지하여 규모에 맞게 성능을 크게 향상시킨다.\nCeph Manager는 PG 통계와 같은 많은 읽기 전용 Ceph CLI 쿼리의 실행을 처리한다. Ceph Manager는 RESTful 모니터링 API도 제공한다. MDS(Metadata Servers) : 클라이언트에 의한 효율적인 POSIX 명령 실행을 위해 CephFS 에서 사용하는 메타데이터를 저장한다.  Ceph Clients Ceph Object Gateway (RADOS Gateway) Ceph Object Gateway(RADOS Gateway, RADOSGW 또는 RGW)는 라이브러리(librados)를 사용하여 구축된 object 스토리지 인터페이스이다.\n라이브러리를 사용하여 Ceph 클러스터와 통신하고 OSD 프로세스에 직접 데이터를 쓴다. 응용 프로그램에 RESTful API가 포함된 게이트웨이를 제공하고 다음 두 인터페이스를 지원한다. (Amazon S3 및 OpenStack Swift)\nCeph Object Gateway는 배포할 수 있는 게이트웨이 수를 제한하지 않고 표준 HTTP 로드 밸런서를 지원하여 확장성 지원을 제공한다.\nRGW 의 몇 가지 사용 사례는 다음과 같다.\n Image storage (for example, SmugMug, Tumblr) Backup services File storage and sharing (for example, Dropbox)  RADOS Block Device Ceph Block Device(RADOS block device, RBD)는 RBD 이미지를 통해 Ceph 클러스터 내에 블록 스토리지를 제공한다.\nRBD 이미지는 클러스터의 서로 다른 OSD에 흩어져 있는 개별 object로부터 구성됩니다. 클러스터 내의 object 간에 데이터를 스트라이핑 할 수 있다.\nRBD를 구성하는 object는 클러스터 주변의 서로 다른 OSD에 분산되기 때문에 블록 디바이스에 대한 액세스는 자동으로 병렬화된다.\nRBD는 다음과 같은 기능을 제공한다.\n Storage for virtual disks in the Ceph cluster Mount support in the Linux kernel Boot support in QEMU, KVM, and OpenStack Cinder  Ceph File System (CephFS) Ceph 파일 시스템(CephFS)은 확장 가능한 단일 계층 공유 디스크를 제공하는 병렬 파일 시스템이다.\nCephFS에 저장된 파일과 관련된 메타데이터는 Ceph Metadata Server(MDS)에서 관리한다.\nPlacement Groups 클러스터에 수백만 개의 object 를 저장하고 개별적으로 관리하는 것은 리소스를 많이 사용한다.\n따라서 Ceph는 placement group (PG)을 사용하여 수많은 object 를 보다 효율적으로 관리한다.\nPG는 object 모음을 포함하는 역할을 하는 Pool 의 하위 집합이다.\nCeph는 Pool 을 일련의 PG로 분할하고 CRUSH 알고리즘은 클러스터 맵과 클러스터 상태를 고려하여 PG를 클러스터의 OSD에 무작위로 고르게 배포한다.\nPool PG 의 집합(Object 의 집합)이며 Pool 단위로 PG 를 관리하고 데이터를 저장한다.\nPool Type 기본적으로 Ceph 는 Replicated pool 방식과 Erasure coded pool 방식을 통해 데이터 복원을 지원한다.\n주로 일반 스토리지의 RAID 1 (mirror) 와 RAID 6 or RAID 5 (parity) 와 비교한다면 이해가 편할 것이다.\n각 방식에는 아래와 같은 장단점이 있다.\n Replicated pool  높은 내구성 3 replica 로 인한 200% overhead 빠른 복구   Erasure coded pool  비용 효율적인 내구성 50% overhead expensive recovery    참고 자료 원문 및 캡처 자료는 아래 자료들을 참고 했습니다.\n https://docs.ceph.com/en/latest/start/intro/ https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/architecture_guide/the-ceph-architecture_arch https://www.slideshare.net/sageweil1/20150222-scale-sdc-tiering-and-ec https://docs.ceph.com/en/mimic/rados/operations/pools/ https://www.theregister.com/2018/11/22/ceph_ref_architecture/ https://whatis.techtarget.com/definition/CRUSH-Controlled-Replication-Under-Scalable-Hashing https://en.wikipedia.org/wiki/Erasure_code  ","permalink":"https://chhanz88.github.io/post/2021-08-18-ceph-architecture/","summary":"Ceph 란? Ceph는 오픈소스 소프트웨어(Software Defined Storage) 스토리지 플랫폼으로 단일 분산 컴퓨터 클러스터에 object 스토리지를 구현하고 object, block 및 file Level 의 스토리지 기능을 제공한다.\n또한 single point of failure 이 없는 완전히 분산된 운영을 주로 목표로 하며 엑사바이트 수준으로 scale-out이 가능하다.\n소프트웨어 정의 스토리지(Software Defined Storage)를 선택해야 하는 이유 소프트웨어 정의 스토리지의 주요 이점은 다음과 같다.\n 비용 대비 성능 절충 : 애플리케이션이 동일한 소프트웨어 스택을 사용하여 서로 다른 하드웨어 및 내구성 구성의 성능 및 비용 절충을 선택할 수 있다.","title":"[CEPH] Ceph Architecture"},{"content":"RHEL v8 이후 변경점 RHEL v8 계열(CentOS/Oracle Linux/Rocky Linux) 부터는 이전과 같이 PAM 을 수정하여 설정하는 것을 권장하지 않습니다.\n[root@chhanz-c8-vm ~]# cat /etc/pam.d/system-auth ... # Generated by authselect on Fri Jul 16 13:13:24 2021 # Do not modify this file manually. \u0026lt;\u0026lt;\u0026lt; ... 추가로 RHEL v8 계열부터는 기존에 사용하던 보안 설정으로 많이 사용 되던 pam_tally2 module 이 deprecated 되었습니다. anthconfig 로 PAM 설정이 가능하였으나, RHEL v8 계열부터 authselect 을 사용하도록 변경 되었습니다.\nauthselect 이란? authselect 는 특정 프로파일을 선택하여 시스템의 ID 및 인증 소스를 설정 할 수 있는 도구입니다.\nauthselect 사용법 Check current the profile [root@chhan-rocky authselect]# authselect current  Profile ID: sssd Enabled features: None Enable feature faillock module 활성화.\n[root@chhan-rocky authselect]# authselect enable-feature with-faillock  Make sure that SSSD service is configured and enabled. See SSSD documentation for more information.  [root@chhan-rocky authselect]# authselect current  Profile ID: sssd Enabled features: - with-faillock Create the custom profile sssd Profile 를 base 로 한 Profile 생성.\n[root@chhan-rocky authselect]# authselect create-profile -b sssd security-profile New profile was created at /etc/authselect/custom/security-profile Change profile [root@chhan-rocky security-profile]# authselect select custom/security-profile Profile \u0026#34;custom/security-profile\u0026#34; was selected. The following nsswitch maps are overwritten by the profile: - passwd - group - netgroup - automount - services  Make sure that SSSD service is configured and enabled. See SSSD documentation for more information. faillock module 을 이용한 보안 설정 기존에 pam_tally2등으로 주로 이용되던 계정 인증 실패에 대한 계정 잠금 정설이 faillock 으로 변경되면서 기능 활성화 및 정책 설정은 아래와 같이 해야됩니다.\nfaillock module 활성화 아래 명령을 통해 faillock module 을 활성화 합니다.\n[root@chhanz-c8-vm ~]# authselect enable-feature with-faillock  Make sure that SSSD service is configured and enabled. See SSSD documentation for more information.  [root@chhanz-c8-vm ~]# authselect current Profile ID: sssd Enabled features: - with-fingerprint - with-silent-lastlog - with-faillock faillock 정책 설정 아래와 같이 /etc/securrity/faillock.conf 를 수정합니다.\n(Note: /etc/security/faillock.conf is available from pam-1.3.1-8.el8.)\n[root@chhanz-c8-vm ~]# cat /etc/security/faillock.conf | grep -v \u0026#34;#\u0026#34; dir = /var/run/faillock audit silent deny = 4 faillock.conf 가 수정이 되면 변경된 profile을 적용합니다.\n[root@chhanz-c8-vm ~]# authselect apply-changes  Changes were successfully applied. faillock 보안 설정 테스트 현재 적용한 정책은 기본적으로 4회 계정 인증 실패시 계정이 LOCK 되고 600s(Default value) 후에 UNLOCK 됩니다.\n[root@chhanz-c8-vm ~]# ssh test@localhost test@localhost\u0026#39;s password: Permission denied, please try again. 위와 같이 계정 인증 실패시 faillock 명령을 통해 인증 실패 현황을 파악 할 수 있습니다.\n[root@chhanz-c8-vm ~]# faillock  root: When Type Source Valid test: When Type Source Valid 2021-07-16 13:14:36 RHOST ::1 V 2021-07-16 13:14:39 RHOST ::1 V 2021-07-16 13:14:42 RHOST ::1 V 2021-07-16 13:14:46 RHOST ::1 V /var/log/secure 에서 위와 같이 계정 LOCK 에 대한 로그를 확인 할 수 있습니다.\n... Jul 16 13:14:46 localhost unix_chkpwd[7186]: password check failed for user (test) Jul 16 13:14:46 localhost sshd[7184]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=::1 user=test Jul 16 13:14:46 localhost sshd[7184]: pam_faillock(sshd:auth): Consecutive login failures for user test account temporarily locked \u0026lt;\u0026lt;\u0026lt; ... 실제로 4회 계정 인증 실패 후에는 계정 접근이 안되었고, 10분후에 다시 인증 시도를 할 수 있었습니다.\n계정 잠금 해제 LOCK 된 계정을 수동으로 unlock 하는 방법은 아래와 같습니다.\n[root@chhanz-c8-vm ~]# faillock --reset --user test [root@chhanz-c8-vm ~]# faillock  root: When Type Source Valid test: When Type Source Valid [참고] authselect 에서 지원하는 feature (profile sssd)    Feature Name Description     with-faillock Lock the account after too many authentication failures.   with-mkhomedir Create home directory on user\u0026rsquo;s first log in.   with-ecryptfs Enable automatic per-user ecryptfs.   with-smartcard Authenticate smart cards through SSSD.   with-smartcard-lock-on-removal Lock the screen when the smart card is removed. Requires that with-smartcard is also enabled.   with-smartcard-required Only smart card authentication is operative; others, including password, are disabled. Requires that with-smartcard is also enabled.   with-fingerprint Authenticate through fingerprint reader.   with-silent-lastlog Disable generation of pam_lostlog messages during login   with-sudo Enable sudo to use SSSD for rules besides /etc/sudoers.   with-pamaccess Refer to /etc/access.conf for account authorization.   without-nullock Do not add the nullock parameter to pam_unix    참고 자료  https://access.redhat.com/solutions/62949 https://docs.oracle.com/en/operating-systems/oracle-linux/8/userauth/auth.html#feature-support  ","permalink":"https://chhanz88.github.io/post/2021-07-16-authselect-use-faillock/","summary":"RHEL v8 이후 변경점 RHEL v8 계열(CentOS/Oracle Linux/Rocky Linux) 부터는 이전과 같이 PAM 을 수정하여 설정하는 것을 권장하지 않습니다.\n[root@chhanz-c8-vm ~]# cat /etc/pam.d/system-auth ... # Generated by authselect on Fri Jul 16 13:13:24 2021 # Do not modify this file manually. \u0026lt;\u0026lt;\u0026lt; ... 추가로 RHEL v8 계열부터는 기존에 사용하던 보안 설정으로 많이 사용 되던 pam_tally2 module 이 deprecated 되었습니다. anthconfig 로 PAM 설정이 가능하였으나, RHEL v8 계열부터 authselect 을 사용하도록 변경 되었습니다.","title":"[Linux] authselect 을 이용하여 faillock module 적용"},{"content":"du 현재 경로의 사용률을 확인 하는 명령입니다.\n[opc@instance-20201011-1438 TEST]$ tree . ├── big1 │ ├── 1g_bigfiles.img │ ├── 1g_bigfiles1.img │ └── 1g_bigfiles2.img ├── big2 │ └── 1g_bigfiles6.img ├── big3 ├── big4 ├── dummyfile1 ├── dummyfile10 ├── dummyfile2 ├── dummyfile3 ├── dummyfile4 ├── dummyfile5 ├── dummyfile6 ├── dummyfile7 ├── dummyfile8 └── dummyfile9 4 directories, 14 files 위와 같은 구조를 가지고 있는 경로에서 du 명령을 사용할 경우,\n[opc@instance-20201011-1438 TEST]$ du -h 3.0G ./big1 1.0G ./big2 0 ./big3 0 ./big4 4.1G . 위와 같이 각 디렉토리 및 현재 경로의 사용량을 확인 할 수 있습니다.\n유용한 du 옵션 / 하위의 특정 경로가 사용량이 높은지 확인 하려면 아래 옵션을 추가하면 됩니다.\n-d, --max-depth=N print the total for a directory (or file, with --all)\nNo option 일 경우 [opc@instance-20201011-1438 /]$ du -h / 16K /boot/efi 0 /boot/grub2 241M /boot 0 /dev/oracleoci 0 /dev/dri 0 /dev/snd 0 /dev/mqueue 0 /dev/hugepages 0 /dev/disk/by-partlabel 0 /dev/disk/by-uuid 0 /dev/disk/by-partuuid 0 /dev/disk/by-path 0 /dev/disk/by-id 0 /dev/disk 0 /dev/block 0 /dev/bsg 0 /dev/char 0 /dev/mapper 0 /dev/net 0 /dev/pts 168K /dev/shm ... 위와 같이 / 하위 모든 depth 에 대해 출력합니다.\n--max-depth 사용할 경우 [opc@instance-20201011-1438 /]$ du -h / --max-depth=1 2\u0026gt;/dev/null 241M /boot 168K /dev 0 /proc 56M /run 0 /sys 23M /etc 2.4G /var 4.0K /root 1.1M /tmp 2.4G /usr 위와 같이 depth 1 에 해당되는 디렉토리 경로 사용량을 출력합니다.\nncdu tool 위와 같이 du --max-depth 명령을 사용하면 원하는 정보 수집이 가능하지만 depth 1 에 대해 확인 후 depth 2 level 확인이 필요한 경우,\n특정 경로로 이동 후 다시 명령을 수행해야되는 불편함이 있습니다.\n그리하여 ncdu 와 같은 Tool 을 이용하면 좀 더 편리하게 depth 별로 사용량 확인이 가능합니다.\nncdu 설치 해당 Package 를 설치하기 위해서는 epel repository 가 활성화 되어 있어야 됩니다.\n[opc@instance-20201011-1438 /]$ sudo yum install ncdu Loaded plugins: langpacks, ulninfo Resolving Dependencies --\u0026gt; Running transaction check ---\u0026gt; Package ncdu.x86_64 0:1.15.1-1.el7 will be installed --\u0026gt; Finished Dependency Resolution Dependencies Resolved ========================================================================================================================= Package Arch Version Repository Size ========================================================================================================================= Installing:  ncdu x86_64 1.15.1-1.el7 ol7_developer_EPEL 52 k ... ncdu 사용 사용량 확인이 필요한 경로에서 ncdu 명령을 수행하면 아래와 같이 출력이 되며, 방향키와 enter 를 이용하여 사용량 확인이 가능합니다.\nncdu 1.15.1 ~ Use the arrow keys to navigate, press ? for help --- /home/opc/TEST ------------------------------------------------------------------------------------------------------ /.. 3.0 GiB [##########] /big1 1.0 GiB [### ] /big2 e 0.0 B [ ] /big4 e 0.0 B [ ] /big3 0.0 B [ ] dummyfile9 0.0 B [ ] dummyfile8 0.0 B [ ] dummyfile7 0.0 B [ ] dummyfile6 0.0 B [ ] dummyfile5 0.0 B [ ] dummyfile4 0.0 B [ ] dummyfile3 0.0 B [ ] dummyfile2 0.0 B [ ] dummyfile10 0.0 B [ ] dummyfile1 Total disk usage: 4.0 GiB Apparent size: 4.0 GiB Items: 18 ncdu 1.15.1 ~ Use the arrow keys to navigate, press ? for help --- /home/opc/TEST/big1 ------------------------------------------------------------------------------------------------- /.. 1.0 GiB [##########] 1g_bigfiles2.img 1.0 GiB [##########] 1g_bigfiles1.img 1.0 GiB [##########] 1g_bigfiles.img Total disk usage: 3.0 GiB Apparent size: 3.0 GiB Items: 3 [참고] ncdu 기본 사용법 ? 를 입력하면 기본 사용법을 확인 할 수 있습니다.\n┌───ncdu help─────────────────1:Keys───2:Format───3:About──┐ │ │ │ up, k Move cursor up │ │ down, j Move cursor down │ │ right/enter Open selected directory │ │ left, \u0026lt;, h Open parent directory │ │ n Sort by name (ascending/descending) │ │ s Sort by size (ascending/descending) │ │ C Sort by items (ascending/descending) │ │ M Sort by mtime (-e flag) │ │ d Delete selected file or directory │ │ t Toggle dirs before files when sorting │ │ -- more -- │ │ Press q to close │ └──────────────────────────────────────────────────────────┘ ","permalink":"https://chhanz88.github.io/post/2021-05-25-use-du/","summary":"du 현재 경로의 사용률을 확인 하는 명령입니다.\n[opc@instance-20201011-1438 TEST]$ tree . ├── big1 │ ├── 1g_bigfiles.img │ ├── 1g_bigfiles1.img │ └── 1g_bigfiles2.img ├── big2 │ └── 1g_bigfiles6.img ├── big3 ├── big4 ├── dummyfile1 ├── dummyfile10 ├── dummyfile2 ├── dummyfile3 ├── dummyfile4 ├── dummyfile5 ├── dummyfile6 ├── dummyfile7 ├── dummyfile8 └── dummyfile9 4 directories, 14 files 위와 같은 구조를 가지고 있는 경로에서 du 명령을 사용할 경우,\n[opc@instance-20201011-1438 TEST]$ du -h 3.0G .","title":"[Linux] du / ncdu 활용"},{"content":"yum group 관련 명령어 Group Package 를 설치 할 때, 주로 이용하는 명령어 입니다.\nyum groupinstall , yum groupinfo , yum grouplist 등이 있습니다.\n기본 사용법 주로 사용 하는 명령어 몇가지에 대해 알아보자.\nyum grouplist 해당 명령은 Group Package 목록 및 설치 가능, 설치됨 여부를 확인 할 수 있습니다.\n[root@fastvm-centos-7-6-30 ~]# yum group list hidden ids ... Available Environment Groups:  최소 설치 (minimal)  계산 노드 (compute-node-environment)  인프라 서버 (infrastructure-server-environment)  파일 및 프린트 서버 (file-print-server-environment)  기본 웹 서버 (web-server-environment)  가상화 호스트 (virtualization-host-environment)  서버 - GUI 사용 (graphical-server-environment)  GNOME 데스크탑 (gnome-desktop-environment)  KDE Plasma Workspaces (kde-desktop-environment)  개발 및 창조를 위한 워크스테이션 (developer-workstation-environment) Available Groups:  Anaconda 도구 (anaconda-tools)  CentOS Linux Client product core (client-product)  CentOS Linux ComputeNode product core (computenode-product)  CentOS Linux Server product core (server-product)  CentOS Linux Workstation product core (workstation-product)  Common NetworkManager submodules (networkmanager-submodules)  DNS 네임 서버 (dns-server)  FTP 서버 (ftp-server)  GNOME (gnome-desktop)  GNOME 응용 프로그램 (gnome-apps)  Hyper-v platform specific packages (platform-microsoft)  ID 관리 서버 (identity-management-server)  Infiniband 지원 (infiniband) ... Done yum groupinfo Package Group의 정보 및 Package sub-group 목록을 확인 할 수 있습니다.\n[opc@instance-20201011-1438 ~]$ sudo yum groupinfo \u0026#34;Server with GUI\u0026#34; Loaded plugins: langpacks, ulninfo There is no installed groups file. Maybe run: yum groups mark convert (see man yum) Environment Group: Server with GUI  Environment-Id: graphical-server-environment  Description: Server for operating network infrastructure services, with a GUI.  Mandatory Groups:  +base  +core  +desktop-debugging  +dial-up  +fonts  +gnome-desktop  +guest-agents  +guest-desktop-agents  +hardware-monitoring  +input-methods  +internet-browser  +multimedia  +print-client  +x11  Optional Groups:  +backup-server  +directory-server  +dns-server  +file-server  +ftp-server  +ha  +hardware-monitoring  +identity-management-server  +infiniband  +java-platform  +kde-desktop  +large-systems  +load-balancer  +mail-server  +mainframe-access  +mariadb  +network-file-system-client  +performance  +postgresql  +print-server  +remote-desktop-clients  +remote-system-management  +resilient-storage  +virtualization-client  +virtualization-hypervisor  +virtualization-tools yum groupinstall 해당 명령은 Group Package 를 설치 할 때 사용합니다.\n(명령어에 @ 를 Package name 에 추가하여 group package 로 명시 합니다.)\n[opc@instance-20201011-1438 ~]$ sudo yum groupinstall @gnome-desktop ... ================================================================================================================================ Package Arch Version Repository Size ================================================================================================================================ Installing for group install \u0026#34;GNOME\u0026#34;:  NetworkManager-libreswan-gnome x86_64 1.2.4-2.el7 ol7_latest 35 k  PackageKit-command-not-found x86_64 1.1.10-2.0.1.el7 ol7_latest 20 k  PackageKit-gtk3-module x86_64 1.1.10-2.0.1.el7 ol7_latest 12 k ... yum grouplist hidden ids 해당 명령은 yum grouplist 의 추가 옵션을 포함하고 있습니다.\n해당 옵션은 Group Package Name 의 ID 값을 같이 보여주도록 하는 명령어입니다.\nID 값을 이용하면 Ansible 및 Shell script 와 같은 자동화 및 Group 내의 sub-group 의 package 를 설치하는 세부적인 조절이 가능합니다.\n[opc@instance-20201011-1438 ~]$ sudo yum grouplist hidden ids Loaded plugins: langpacks, ulninfo There is no installed groups file. Maybe run: yum groups mark convert (see man yum) Available Environment Groups:  Minimal Install (minimal)  Infrastructure Server (infrastructure-server-environment)  File and Print Server (file-print-server-environment)  Cinnamon Desktop (cinnamon-desktop-environment)  MATE Desktop (mate-desktop-environment)  Basic Web Server (web-server-environment)  Virtualization Host (virtualization-host-environment)  Server with GUI (graphical-server-environment) \u0026lt;\u0026lt; ... 위와 같이 기존에는 Server with GUI 로 입력하던 Group Name 을 graphical-server-environment 로 변경하여 사용 할 수 있습니다.\n--- - name: install the \u0026#39;Gnome desktop\u0026#39; environment group (if CentOS)  yum:  name: \u0026#34;@^gnome-desktop-environment\u0026#34; \u0026lt;\u0026lt;  state: present  when: ansible_distribution != \u0026#39;RedHat\u0026#39;  - name: install the \u0026#39;Gnome desktop\u0026#39; environment group (if RedHat)  yum:  name: \u0026#34;@gnome-desktop\u0026#34; \u0026lt;\u0026lt;  state: present  when: ansible_distribution == \u0026#39;RedHat\u0026#39; ... 위 내용은 Ansible Playbook 에서 사용된 예제 입니다. (참고 : install_gnome.yml)\n","permalink":"https://chhanz88.github.io/post/2021-05-25-yum-groupinstall/","summary":"yum group 관련 명령어 Group Package 를 설치 할 때, 주로 이용하는 명령어 입니다.\nyum groupinstall , yum groupinfo , yum grouplist 등이 있습니다.\n기본 사용법 주로 사용 하는 명령어 몇가지에 대해 알아보자.\nyum grouplist 해당 명령은 Group Package 목록 및 설치 가능, 설치됨 여부를 확인 할 수 있습니다.\n[root@fastvm-centos-7-6-30 ~]# yum group list hidden ids ... Available Environment Groups:  최소 설치 (minimal)  계산 노드 (compute-node-environment)  인프라 서버 (infrastructure-server-environment)  파일 및 프린트 서버 (file-print-server-environment)  기본 웹 서버 (web-server-environment)  가상화 호스트 (virtualization-host-environment)  서버 - GUI 사용 (graphical-server-environment)  GNOME 데스크탑 (gnome-desktop-environment)  KDE Plasma Workspaces (kde-desktop-environment)  개발 및 창조를 위한 워크스테이션 (developer-workstation-environment) Available Groups:  Anaconda 도구 (anaconda-tools)  CentOS Linux Client product core (client-product)  CentOS Linux ComputeNode product core (computenode-product)  CentOS Linux Server product core (server-product)  CentOS Linux Workstation product core (workstation-product)  Common NetworkManager submodules (networkmanager-submodules)  DNS 네임 서버 (dns-server)  FTP 서버 (ftp-server)  GNOME (gnome-desktop)  GNOME 응용 프로그램 (gnome-apps)  Hyper-v platform specific packages (platform-microsoft)  ID 관리 서버 (identity-management-server)  Infiniband 지원 (infiniband) .","title":"[Linux] Yum Group 상세 활용"},{"content":"VDO 란? VDO(Virtual Data Optimizer)는 데이터의 중복제거, 압축 등의 기능을 사용해 스토리지의 공간 활용도를 높이는 기술입니다.\nVDO 활용 VDO 장단점  장점 : 중복 제거 및 압축 기능을 이용하여 스토리지 공간 활용도가 높아지며, 주로 VM 및 컨테이너 환경에서 높은 효율을 보인다고 합니다. 단점 : 일반적인 스토리지에 비해 성능 저하가 발생합니다. 참고 자료 : https://www.redhat.com/en/blog/look-vdo-new-linux-compression-layer  VDO 설치  설치 환경 : CentOS 8.1  아래 명령을 통해 설치를 진행합니다.\n[root@fastvm-centos-8-1-30 ~]# dnf install vdo kmod-kvdo ... 설치가 완료된 이후, 서비스를 시작합니다.\n[root@fastvm-centos-8-1-30 ~]# systemctl enable --now vdo [root@fastvm-centos-8-1-30 ~]# systemctl status vdo ● vdo.service - VDO volume services  Loaded: loaded (/usr/lib/systemd/system/vdo.service; enabled; vendor preset: enabled)  Active: active (exited) since Fri 2021-03-19 11:25:01 KST; 3min 21s ago  Process: 875 ExecStart=/usr/bin/vdo start --all --confFile /etc/vdoconf.yml (code=exited, status=0/SUCCESS)  Main PID: 875 (code=exited, status=0/SUCCESS)  Mar 19 11:24:58 fastvm-centos-8-1-30 systemd[1]: Starting VDO volume services... Mar 19 11:25:01 fastvm-centos-8-1-30 systemd[1]: Started VDO volume services. VDO Volume 생성 아래와 같이 VDO Device 를 생성할 수 있습니다.\n[root@fastvm-centos-8-1-30 ~]# vdo create --device /dev/sda3 --vdoLogicalSize 200G --name vdo1 Creating VDO vdo1  The VDO volume can address 90 GB in 45 data slabs, each 2 GB.  It can grow to address at most 16 TB of physical storage in 8192 slabs.  If a larger maximum size might be needed, use bigger slabs. Starting VDO vdo1 Starting compression on VDO vdo1 VDO instance 0 volume is ready at /dev/mapper/vdo1 VDO 파일시스템 생성 생성된 VDO Device 를 LVM 을 이용하여 LV 로 만들고 filesystem 을 생성하도록 하겠습니다.\n# pvcreate /dev/mapper/vdo1 # vgcreate testvg /dev/mapper/vdo1 # lvcreate -l 100%FREE -n testlv testvg # mkfs.xfs /dev/testvg/testlv /etc/fstab 등록 향후 시스템 재부팅 간 자동으로 Mount 되도록 /etc/fstab 에 등록합니다.\n/dev/testvg/testlv /mnt/testlv xfs defaults,_netdev,x-systemd.device-timeout=0,x-systemd.requires=vdo.service 0 0 중복 제거 및 압축 설정 확인 아래 명령을 통해 확인이 가능합니다.\n[root@fastvm-centos-8-1-30 testlv]# vdo status | egrep \u0026#39;Deduplication|Compression\u0026#39;  Compression: enabled  Deduplication: enabled 중복 제거 테스트 아래 명령을 통해 5GB 의 테스트 파일을 생성합니다.\n[root@fastvm-centos-8-1-30 testlv]# dd if=/dev/urandom of=dd.file1 count=1024 bs=1024k 1024+0 records in 1024+0 records out 1073741824 bytes (1.1 GB, 1.0 GiB) copied, 13.1108 s, 81.9 MB/s [root@fastvm-centos-8-1-30 testlv]# dd if=/dev/urandom of=dd.file2 count=1024 bs=1024k 1024+0 records in 1024+0 records out 1073741824 bytes (1.1 GB, 1.0 GiB) copied, 14.5352 s, 73.9 MB/s [root@fastvm-centos-8-1-30 testlv]# dd if=/dev/urandom of=dd.file3 count=1024 bs=1024k 1024+0 records in 1024+0 records out 1073741824 bytes (1.1 GB, 1.0 GiB) copied, 16.0602 s, 66.9 MB/s [root@fastvm-centos-8-1-30 testlv]# dd if=/dev/urandom of=dd.file4 count=1024 bs=1024k 1024+0 records in 1024+0 records out 1073741824 bytes (1.1 GB, 1.0 GiB) copied, 17.0693 s, 62.9 MB/s [root@fastvm-centos-8-1-30 testlv]# dd if=/dev/urandom of=dd.file5 count=1024 bs=1024k 1024+0 records in 1024+0 records out 1073741824 bytes (1.1 GB, 1.0 GiB) copied, 17.0444 s, 63.0 MB/s VDO 사용량은 아래와 같이 변화가 생겼습니다.\n\u0026#34;데이터 생성 이전\u0026#34; [root@fastvm-centos-8-1-30 ~]# vdostats --hu Device Size Used Available Use% Space saving% /dev/mapper/vdo1 94.0G 4.1G 89.9G 4% N/A  \u0026#34;데이터 생성 이후\u0026#34; [root@fastvm-centos-8-1-30 testlv]# vdostats --hu Device Size Used Available Use% Space saving% /dev/mapper/vdo1 94.0G 9.1G 84.9G 9% 1% 생성된 파일을 cp 하여 파일시스템 사용률을 높이도록 하겠습니다.\n[root@fastvm-centos-8-1-30 testlv]# for i in {1..400}; do cp -v dd.file1 cp.file$i; done \u0026#39;dd.file1\u0026#39; -\u0026gt; \u0026#39;cp.file1\u0026#39; \u0026#39;dd.file1\u0026#39; -\u0026gt; \u0026#39;cp.file2\u0026#39; ... \u0026#39;dd.file1\u0026#39; -\u0026gt; \u0026#39;cp.file100\u0026#39; ... 아래와 같이 중복 제거가 되면서 스토리지 공간 Saving 을 하는 것을 확인 할 수 있습니다.\n[root@fastvm-centos-8-1-30 testlv]# df -h /mnt/testlv Filesystem Size Used Avail Use% Mounted on /dev/mapper/testvg-testlv 200G 47G 154G 24% /mnt/testlv \u0026lt;\u0026lt; [root@fastvm-centos-8-1-30 testlv]# vdostats --hu Device Size Used Available Use% Space saving% /dev/mapper/vdo1 94.0G 9.1G 84.9G 9% 88% \u0026lt;\u0026lt; VDO 스토리지 공간 Full 테스트 중복 제거가 불가능한 데이터를 넣으면 VDO 가 어떻게 동작하는지 테스트 해보았습니다.\n[root@fastvm-centos-8-1-30 /]# cat /var/log/messages | grep lvm | grep full Mar 31 15:59:37 fastvm-centos-8-1-30 lvm[1622]: WARNING: VDO pool vdo1 is now 80.33% full. Mar 31 16:01:37 fastvm-centos-8-1-30 lvm[1622]: WARNING: VDO pool vdo1 is now 85.04% full. Mar 31 16:03:37 fastvm-centos-8-1-30 lvm[1622]: WARNING: VDO pool vdo1 is now 90.14% full. Mar 31 16:05:27 fastvm-centos-8-1-30 lvm[1622]: WARNING: VDO pool vdo1 is now 95.03% full. Mar 31 16:07:27 fastvm-centos-8-1-30 lvm[1622]: WARNING: VDO pool vdo1 is now 100.00% full. 먼저 위와 같이 /var/log/messages 에서 error message 가 발생합니다.\n[root@fastvm-centos-8-1-30 /]# vdostats --hu Device Size Used Available Use% Space saving% /dev/mapper/vdo1 94.0G 94.0G 0.0B 100% 0% ... dd: failed to open \u0026#39;dd.file143\u0026#39;: Input/output error dd: failed to open \u0026#39;dd.file144\u0026#39;: Input/output error dd: failed to open \u0026#39;dd.file145\u0026#39;: Input/output error dd: failed to open \u0026#39;dd.file146\u0026#39;: Input/output error dd: failed to open \u0026#39;dd.file147\u0026#39;: Input/output error dd: failed to open \u0026#39;dd.file148\u0026#39;: Input/output error dd: failed to open \u0026#39;dd.file149\u0026#39;: Input/output error dd: failed to open \u0026#39;dd.file150\u0026#39;: Input/output error ... 위와 같이 VDO 의 Used 는 100% 가 되고 해당 filesystem 은 데이터 생성이 불가능합니다.\n참고 자료  https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/vdo-quick-start https://www.redhat.com/en/blog/look-vdo-new-linux-compression-layer  ","permalink":"https://chhanz88.github.io/post/2021-03-31-vdo/","summary":"VDO 란? VDO(Virtual Data Optimizer)는 데이터의 중복제거, 압축 등의 기능을 사용해 스토리지의 공간 활용도를 높이는 기술입니다.\nVDO 활용 VDO 장단점  장점 : 중복 제거 및 압축 기능을 이용하여 스토리지 공간 활용도가 높아지며, 주로 VM 및 컨테이너 환경에서 높은 효율을 보인다고 합니다. 단점 : 일반적인 스토리지에 비해 성능 저하가 발생합니다. 참고 자료 : https://www.redhat.com/en/blog/look-vdo-new-linux-compression-layer  VDO 설치  설치 환경 : CentOS 8.1  아래 명령을 통해 설치를 진행합니다.\n[root@fastvm-centos-8-1-30 ~]# dnf install vdo kmod-kvdo .","title":"[Linux] VDO(Virtual Data Optimizer) 활용"},{"content":"[Ansible] Windows 10 WSL 을 이용하여 Ansible 를 써보자 MacOS 을 사용하거나 Linux laptop 을 사용하는 분들은 Ansible 을 사용하는 것에 있어 문제가 될 것이 없습니다.\nOS 내부에 brew 이나 apt , yum 같은 Package manager 가 있고 Command 를 이용 할 수 있기 때문이죠.\n그럼 Windows 는 사용이 불가능한 것인가?\nWindows 에서는 Windows Subsystem for Linux (WSL) 을 이용하여 Command 환경을 구현해주는 기능을 활용하면 사용이 가능합니다.\n아래 Link를 통해 상세한 정보를 확인 할 수 있습니다.\n 참고 자료 : https://docs.microsoft.com/ko-kr/windows/wsl/faq  WSL 설치 Ansible 설치를 위해 아래 Ubuntu Command 를 사용합니다.\n$ sudo apt-get updaate $ sudo apt install ansible sshpass \u0026lt;\u0026lt; 필요시엔 `sshpass` package 도 같이 설치합니다. 참고 자료  https://docs.microsoft.com/ko-kr/windows/wsl/faq  ","permalink":"https://chhanz88.github.io/post/2021-02-24-ansible-wsl/","summary":"[Ansible] Windows 10 WSL 을 이용하여 Ansible 를 써보자 MacOS 을 사용하거나 Linux laptop 을 사용하는 분들은 Ansible 을 사용하는 것에 있어 문제가 될 것이 없습니다.\nOS 내부에 brew 이나 apt , yum 같은 Package manager 가 있고 Command 를 이용 할 수 있기 때문이죠.\n그럼 Windows 는 사용이 불가능한 것인가?\nWindows 에서는 Windows Subsystem for Linux (WSL) 을 이용하여 Command 환경을 구현해주는 기능을 활용하면 사용이 가능합니다.\n아래 Link를 통해 상세한 정보를 확인 할 수 있습니다.","title":"[Ansible] Windows 10 WSL 을 이용하여 Ansible 를 써보자"},{"content":"[ceph] ceph-ansible 을 이용하여 ceph 배포 ceph-ansible 을 이용하여 ceph 를 배포해보도록 하겠습니다.\n아래 환경은 ceph 테스트를 위해 배포하는 환경이며 운영 환경에 적합한 환경은 아닙니다.\n테스트 환경  deploy/grafana server : CentOS 7.7 mon/osd #1 : CentOS 7.7  osd : /dev/sdb (100g), /dev/sdc (100g)   mon/osd #2 : CentOS 7.7  osd : /dev/sdb (100g), /dev/sdc (100g)   mon/osd #3 : CentOS 7.7  osd : /dev/sdb (100g), /dev/sdc (100g)    배포 준비 clone ceph-ansible [root@ceph-deploy ceph-ansible]# git clone https://github.com/ceph/ceph-ansible.git Cloning into \u0026#39;ceph-ansible\u0026#39;... remote: Enumerating objects: 23, done. remote: Counting objects: 100% (23/23), done. remote: Compressing objects: 100% (22/22), done. remote: Total 56802 (delta 15), reused 1 (delta 1), pack-reused 56779 Receiving objects: 100% (56802/56802), 10.54 MiB | 4.76 MiB/s, done. Resolving deltas: 100% (39491/39491), done. 아래와 같이 checkout 을 하여 ceph version 을 지정합니다.\n[root@ceph-deploy ceph-ansible]# cd ceph-ansible/ [root@ceph-deploy ceph-ansible]# git checkout stable-5.0 Branch stable-5.0 set up to track remote branch stable-5.0 from origin. Switched to a new branch \u0026#39;stable-5.0\u0026#39; [root@ceph-deploy ceph-ansible]# [root@ceph-deploy ceph-ansible]# [root@ceph-deploy ceph-ansible]# git status # On branch stable-5.0 nothing to commit, working directory clean [root@ceph-deploy ceph-ansible]# Inventory 작성 아래와 같이 mon 과 osd 가 같은 서버에 구축이 되도록 inventory 를 작성합니다.\n[mons] mon1 ansible_host=10.50.2.66 mon2 ansible_host=10.50.2.67 mon3 ansible_host=10.50.2.68  [osds] mon1 mon2 mon3  [grafana-server] deploy ansible_host=10.50.2.65  [all:vars] ansible_ssh_user=root ansible_ssh_pass=testtest group_vars 수정 아래와 같이 sample yaml 을 이용하여 group_vars 를 수정합니다.\n[root@ceph-deploy ceph-ansible]# cd group_vars [root@ceph-deploy group_vars]# cp all.yml.sample all.yml  [root@ceph-deploy group_vars]# cp osds.yml.sample osds.yml all.yml 수정 아래와 같이 변수에 대한 값을 입력합니다.\n--- dummy: ntp_service_enabled: true ntp_daemon_type: chronyd monitor_interface: ens160 public_network: 10.50.2.0/24 cluster_network: 40.40.40.0/24 containerized_deployment: true dashboard_enabled: True dashboard_admin_user: admin dashboard_admin_password: P@ssw0rd grafana_admin_user: admin grafana_admin_password: P@ssw0rd 특히 containerized_deployment 는 ceph 를 systemd 가 아닌 container 환경으로 배포하기 위해서는 꼭 설정해야됩니다.\nosds.yml 수정 아래와 같이 변수에 대한 값을 입력합니다.\n--- dummy: devices:  - /dev/sdb  - /dev/sdc site-container.yml 준비 아래와 같이 sample file 을 복사합니다.\n[root@ceph-deploy ceph-ansible]# cp site-container.yml.sample site-container.yml Requirement Package 설치 아래와 같이 Requirement Package 를 설치합니다.\n[root@ceph-deploy ceph-ansible]# yum install python3 python3-pip python3-setuptools  * python3 venv 생성 [root@ceph-deploy ~]# python3 -m venv py3 [root@ceph-deploy ~]# source py3/bin/activate  (py3) [root@ceph-deploy ceph-ansible]# pip -V pip 9.0.3 from /root/py3/lib64/python3.6/site-packages (python 3.6)  (py3) [root@ceph-deploy ceph-ansible]# pip install -U pip (py3) [root@ceph-deploy ceph-ansible]# pip install -r requirements.txt (py3) [root@ceph-deploy ceph-ansible]# pip install six ceph 배포 아래 명령을 통해 ceph 배포를 시작합니다.\n(py3) [root@ceph-deploy ceph-ansible]# ansible-playbook -i inventory site-container.yml ... 배포 완료 ... ... PLAY RECAP ********************************************************************************************************************************************************************************deploy : ok=117 changed=26 unreachable=0 failed=0 skipped=300 rescued=0 ignored=0 mon1 : ok=349 changed=49 unreachable=0 failed=0 skipped=529 rescued=0 ignored=0 mon2 : ok=270 changed=35 unreachable=0 failed=0 skipped=472 rescued=0 ignored=0 mon3 : ok=281 changed=39 unreachable=0 failed=0 skipped=471 rescued=0 ignored=0   INSTALLER STATUS **************************************************************************************************************************************************************************Install Ceph Monitor : Complete (0:01:46) Install Ceph OSD : Complete (0:01:17) Install Ceph Dashboard : Complete (0:01:23) Install Ceph Grafana : Complete (0:01:09) Install Ceph Node Exporter : Complete (0:00:31)  Tuesday 23 February 2021 15:36:17 +0900 (0:00:00.067) 0:12:39.483 ****** =============================================================================== ceph-container-common : pulling docker.io/ceph/daemon:latest-octopus image ------------------------------------------------------------------------------------------------------- 163.68s /root/ceph-ansible/roles/ceph-container-common/tasks/fetch_image.yml:199 ----------------------------------------------------------------------------------------------------------------- ceph-container-engine : install container packages -------------------------------------------------------------------------------------------------------------------------------- 84.46s /root/ceph-ansible/roles/ceph-container-engine/tasks/pre_requisites/prerequisites.yml:26 ------------------------------------------------------------------------------------------------- gather and delegate facts --------------------------------------------------------------------------------------------------------------------------------------------------------- 24.72s /root/ceph-ansible/site-container.yml:38 ------------------------------------------------------------------------------------------------------------------------------------------------- ceph-mon : waiting for the monitor(s) to form the quorum... ----------------------------------------------------------------------------------------------------------------------- 21.31s /root/ceph-ansible/roles/ceph-mon/tasks/ceph_keys.yml:2 ---------------------------------------------------------------------------------------------------------------------------------- ceph-dashboard : create dashboard admin user -------------------------------------------------------------------------------------------------------------------------------------- 20.83s /root/ceph-ansible/roles/ceph-dashboard/tasks/configure_dashboard.yml:141 ---------------------------------------------------------------------------------------------------------------- ceph-grafana : wait for grafana to start ------------------------------------------------------------------------------------------------------------------------------------------ 15.52s /root/ceph-ansible/roles/ceph-grafana/tasks/configure_grafana.yml:112 -------------------------------------------------------------------------------------------------------------------- ceph-mgr : wait for all mgr to be up ---------------------------------------------------------------------------------------------------------------------------------------------- 13.90s /root/ceph-ansible/roles/ceph-mgr/tasks/mgr_modules.yml:7 -------------------------------------------------------------------------------------------------------------------------------- ceph-osd : wait for all osd to be up ---------------------------------------------------------------------------------------------------------------------------------------------- 12.63s /root/ceph-ansible/roles/ceph-osd/tasks/main.yml:87 -------------------------------------------------------------------------------------------------------------------------------------- ceph-grafana : ship systemd service ----------------------------------------------------------------------------------------------------------------------------------------------- 10.97s /root/ceph-ansible/roles/ceph-grafana/tasks/systemd.yml:2 -------------------------------------------------------------------------------------------------------------------------------- ceph-osd : use ceph-volume lvm batch to create bluestore osds --------------------------------------------------------------------------------------------------------------------- 10.77s /root/ceph-ansible/roles/ceph-osd/tasks/scenarios/lvm-batch.yml:3 ------------------------------------------------------------------------------------------------------------------------ ceph-infra : open prometheus port ------------------------------------------------------------------------------------------------------------------------------------------------- 10.76s /root/ceph-ansible/roles/ceph-infra/tasks/dashboard_firewall.yml:40 ---------------------------------------------------------------------------------------------------------------------- ceph-mon : fetch ceph initial keys ------------------------------------------------------------------------------------------------------------------------------------------------- 7.44s /root/ceph-ansible/roles/ceph-mon/tasks/ceph_keys.yml:19 --------------------------------------------------------------------------------------------------------------------------------- ceph-prometheus : write alertmanager config file ----------------------------------------------------------------------------------------------------------------------------------- 5.69s /root/ceph-ansible/roles/ceph-prometheus/tasks/main.yml:48 ------------------------------------------------------------------------------------------------------------------------------- ceph-dashboard : copy self-signed generated certificate on mons -------------------------------------------------------------------------------------------------------------------- 4.88s /root/ceph-ansible/roles/ceph-dashboard/tasks/configure_dashboard.yml:73 ----------------------------------------------------------------------------------------------------------------- ceph-osd : apply operating system tuning ------------------------------------------------------------------------------------------------------------------------------------------- 4.73s /root/ceph-ansible/roles/ceph-osd/tasks/system_tuning.yml:50 ----------------------------------------------------------------------------------------------------------------------------- ceph-config : create ceph initial directories -------------------------------------------------------------------------------------------------------------------------------------- 4.15s /root/ceph-ansible/roles/ceph-config/tasks/create_ceph_initial_dirs.yml:2 ---------------------------------------------------------------------------------------------------------------- add modules to ceph-mgr ------------------------------------------------------------------------------------------------------------------------------------------------------------ 4.10s /root/ceph-ansible/roles/ceph-mgr/tasks/mgr_modules.yml:40 ------------------------------------------------------------------------------------------------------------------------------- ceph-config : create ceph initial directories -------------------------------------------------------------------------------------------------------------------------------------- 3.96s /root/ceph-ansible/roles/ceph-config/tasks/create_ceph_initial_dirs.yml:2 ---------------------------------------------------------------------------------------------------------------- ceph-container-common : get ceph version ------------------------------------------------------------------------------------------------------------------------------------------- 3.87s /root/ceph-ansible/roles/ceph-container-common/tasks/main.yml:13 ------------------------------------------------------------------------------------------------------------------------- ceph-container-engine : start container service ------------------------------------------------------------------------------------------------------------------------------------ 3.52s /root/ceph-ansible/roles/ceph-container-engine/tasks/pre_requisites/prerequisites.yml:81 ------------------------------------------------------------------------------------------------- (py3) [root@ceph-deploy ceph-ansible]# 서비스 점검 Container 로 구성된 ceph 는 아래와 같은 방법으로 mgr 에 ceph 명령을 입력하고 ceph 를 운영 할 수 있습니다.\n[root@ceph-s1 ~]# docker exec ceph-mgr-ceph-s1 ceph -s  cluster:  id: 98b454c8-a44a-41c7-a11d-a20f6d37b672  health: HEALTH_OK   services:  mon: 3 daemons, quorum ceph-s1,ceph-s2,ceph-s3 (age 29m)  mgr: ceph-s1(active, since 24m), standbys: ceph-s2, ceph-s3  osd: 6 osds: 6 up (since 27m), 6 in (since 27m)   data:  pools: 2 pools, 33 pgs  objects: 0 objects, 0 B  usage: 6.0 GiB used, 594 GiB / 600 GiB avail  pgs: 33 active+clean  [root@ceph-s1 ~]#  [root@ceph-s1 ~]# docker exec ceph-mgr-ceph-s1 ceph mon dump  epoch 1 fsid 98b454c8-a44a-41c7-a11d-a20f6d37b672 last_changed 2021-02-23T15:30:24.754250+0900 created 2021-02-23T15:30:24.754250+0900 min_mon_release 15 (octopus) 0: [v2:10.50.2.66:3300/0,v1:10.50.2.66:6789/0] mon.ceph-s1 1: [v2:10.50.2.67:3300/0,v1:10.50.2.67:6789/0] mon.ceph-s2 2: [v2:10.50.2.68:3300/0,v1:10.50.2.68:6789/0] mon.ceph-s3 dumped monmap epoch 1 [root@ceph-s1 ~]# ceph dashboard 아래와 같이 mgr IP 를 통해 ceph dashboard 에 접속 할 수 있습니다. (https://mgr IP:8443)\n참고 문서  https://docs.ceph.com/projects/ceph-ansible/en/latest/index.html  ","permalink":"https://chhanz88.github.io/post/2021-02-23-ceph-ansible/","summary":"[ceph] ceph-ansible 을 이용하여 ceph 배포 ceph-ansible 을 이용하여 ceph 를 배포해보도록 하겠습니다.\n아래 환경은 ceph 테스트를 위해 배포하는 환경이며 운영 환경에 적합한 환경은 아닙니다.\n테스트 환경  deploy/grafana server : CentOS 7.7 mon/osd #1 : CentOS 7.7  osd : /dev/sdb (100g), /dev/sdc (100g)   mon/osd #2 : CentOS 7.7  osd : /dev/sdb (100g), /dev/sdc (100g)   mon/osd #3 : CentOS 7.7  osd : /dev/sdb (100g), /dev/sdc (100g)    배포 준비 clone ceph-ansible [root@ceph-deploy ceph-ansible]# git clone https://github.","title":"[ceph] ceph-ansible 을 이용하여 ceph 배포 (containerized deployment)"},{"content":"[OpenShift] Matchbox 를 이용하여 OpenShift 배포서버 구성 OpenShift / OKD 4 version 이 되면서 일반 Linux (RHEL/CentOS) 가 아닌 CoreOS (RHCOS/Fedora CoreOS) 를 사용하면서 OpenShift 설치 방법이 많이 달라졌습니다.\nOpenShift 설치를 위해 ignition, kernel image, initramfs, rootfs 등의 배포 관리가 필요합니다.\nOpenShift Document 에서는 httpd 및 pxe 를 이용하여 배포를 하도록 하지만 matchbox 를 이용하면 향후 운영에도 편리한 배포가 가능합니다.\nmatchbox 란? matchbox 의 주요 기능은 MAC 주소 기반으로 OpenShift 설치에 필요한 파일을 제공하는 오픈소스 솔루션입니다.\nmatchbox 설치 설치는 아래와 같이 컨테이너를 이용하여 설치를 진행합니다.\n이번 테스트는 DHCP 서버에 matchbox 를 같이 구동하여 DHCP 에서 IP 할당이 되면 배포에 필요한 파일을 matchbox 에서 가져오도록 구현하였습니다.\nPodman 설치 [root@infra ~]# yum -y install podman matchbox 설치 [root@infra ~]# wget https://github.com/poseidon/matchbox/releases/download/v0.8.0/matchbox-v0.8.0-linux-amd64.tar.gz [root@infra ~]# tar xzvf matchbox-v0.8.0-linux-amd64.tar.gz 참고) version 은 차이가 있을수도 있습니다.\n최신 버전 확인 : https://matchbox.psdn.io/\n[root@infra ~]# mkdir /etc/matchbox [root@infra ~]# export SAN=DNS.1:$(hostname),IP.1:10.50.2.52  [root@infra ~]# cd /root/matchbox-v0.8.0-linux-amd64/scripts/tls [root@infra tls]# ./cert-gen ...  X509v3 Subject Alternative Name:  DNS:dhcp.okd.chhanz.com, IP Address:10.50.2.52 ... [root@infra tls]# cp ca.crt server.crt server.key /etc/matchbox 위와 같이 배포 서버 인증서를 생성합니다. cert-gen 라는 인증서 스크립트를 제공하고 있으니 사용하면 됩니다.\nhttps://github.com/poseidon/matchbox/blob/master/docs/deployment.md#installation\nmatchbox 실행 [root@infra ~]# podman run --net=host -d --rm -v /var/lib/matchbox:/var/lib/matchbox \\ -v /etc/matchbox:/etc/matchbox quay.io/poseidon/matchbox:latest \\ -address=0.0.0.0:8080 -rpc-address=0.0.0.0:8081 -log-level=debug  [root@infra ~]# curl 127.0.0.1:8080 matchbox 위와 같이 curl 을 이용하여 matchbox 라고 나오면 정상적으로 실행이 된 것 입니다.\nOpenShift 배포 파일 추가 아래 경로에 ignition, kernel image, initramfs, rootfs 등을 추가하고 profile 을 작성합니다.\n[root@infra matchbox]# pwd /var/lib/matchbox [root@infra matchbox]# tree . |-- assets | |-- fedora-coreos-32.20201018.3.0-live-initramfs.x86_64.img | |-- fedora-coreos-32.20201018.3.0-live-kernel-x86_64 | |-- fedora-coreos-32.20201018.3.0-live-rootfs.x86_64.img | |-- fedora-coreos-32.20201018.3.0-metal.x86_64.raw.xz | |-- fedora-coreos-32.20201018.3.0-metal.x86_64.raw.xz.sig | |-- fedora-coreos-32.20201104.3.0-live-initramfs.x86_64.img | |-- fedora-coreos-32.20201104.3.0-live-kernel-x86_64 | |-- fedora-coreos-32.20201104.3.0-live-rootfs.x86_64.img | |-- fedora-coreos-32.20201104.3.0-metal.x86_64.raw.xz.sig |-- groups | |-- bootstrap.json | |-- master1.json | |-- master2.json | |-- master3.json | |-- worker.json |-- ignition | |-- bootstrap.ign | |-- master.ign | |-- worker.ign |-- profiles  |-- bootstrap.json  |-- master.json  |-- worker.json 위와 같이 assets 경로에는 설치에 필요한 initramfs, kernel, rootfs 를 위치합니다.\n[root@infra matchbox]# cat groups/bootstrap.json  {  \u0026#34;id\u0026#34;: \u0026#34;bootstrap-1\u0026#34;,  \u0026#34;name\u0026#34;: \u0026#34;bootstrap-1\u0026#34;,  \u0026#34;profile\u0026#34;: \u0026#34;bootstrap\u0026#34;,  \u0026#34;selector\u0026#34;: {  \u0026#34;mac\u0026#34;: \u0026#34;00:30:57:81:c9:10\u0026#34;  } } [root@infra matchbox]# cat groups/master1.json  {  \u0026#34;id\u0026#34;: \u0026#34;master-1\u0026#34;,  \u0026#34;name\u0026#34;: \u0026#34;master-1\u0026#34;,  \u0026#34;profile\u0026#34;: \u0026#34;master\u0026#34;,  \u0026#34;selector\u0026#34;: {  \u0026#34;mac\u0026#34;: \u0026#34;00:30:57:81:c9:11\u0026#34;  } } [root@infra matchbox]# 위와 같이 groups 은 각 node 의 MAC 주소를 작성하고 어떤 profile 을 사용할지 지정합니다.\n[root@infra matchbox]# ls -la ignition/ total 300 drwxr-xr-x 2 root root 63 Nov 30 16:34 . drwxr-xr-x 6 root root 66 Jan 26 15:55 .. -rw-r----- 1 root root 295606 Dec 1 14:17 bootstrap.ign -rw-r----- 1 root root 1716 Dec 1 14:17 master.ign -rw-r----- 1 root root 1716 Dec 1 14:17 worker.ign [root@infra matchbox]# ignition 파일은 openshift-install 에서 생성된 ignition 파일을 위치합니다.\n[root@infra matchbox]# cat profiles/master.json  {  \u0026#34;id\u0026#34;: \u0026#34;master\u0026#34;,  \u0026#34;name\u0026#34;: \u0026#34;master\u0026#34;,  \u0026#34;ignition_id\u0026#34;: \u0026#34;master.ign\u0026#34;,  \u0026#34;boot\u0026#34;: {  \u0026#34;kernel\u0026#34;: \u0026#34;/assets/fedora-coreos-32.20201018.3.0-live-kernel-x86_64\u0026#34;,  \u0026#34;initrd\u0026#34;: [  \u0026#34;/assets/fedora-coreos-32.20201018.3.0-live-initramfs.x86_64.img\u0026#34;  ],  \u0026#34;args\u0026#34;: [  \u0026#34;coreos.inst=yes\u0026#34;,  \u0026#34;coreos.inst.install_dev=vda\u0026#34;,  \u0026#34;coreos.live.rootfs_url=http://infra.okd.chhanz.com:8080/assets/fedora-coreos-32.20201018.3.0-live-rootfs.x86_64.img\u0026#34;,  \u0026#34;coreos.inst.ignition_url=http://infra.okd.chhanz.com:8080/ignition?mac=${mac:hexhyp}\u0026#34;  ]  } } 위와 같이 MAC 주소에 맞는 profile을 matchbox 가 자동으로 iPXE 를 이용하여 제공합니다.\nmatchbox 구동 아래와 같이 PXE Boot 를 통해 해당 MAC 을 사용하는 Node 는 profile 에 설정 되어 있는 boot arg 를 이용하여 boot 를 진행하고 자동으로 OpenShift 설치가 진행됩니다.\nmatchbox log ... time=\u0026#34;2020-12-01T05:51:08Z\u0026#34; level=info msg=\u0026#34;HTTP GET /boot.ipxe\u0026#34; time=\u0026#34;2020-12-01T05:51:08Z\u0026#34; level=info msg=\u0026#34;HTTP GET /ipxe?uuid=65136bfd-fd35-4289-b6f1-acec191e20f4\u0026amp;mac=00-30-57-81-c9-14\u0026amp;domain=chhanz.com\u0026amp;hostname=w1.okd.chhanz.com\u0026amp;serial=\u0026#34; time=\u0026#34;2020-12-01T05:51:08Z\u0026#34; level=debug msg=\u0026#34;Matched an iPXE config\u0026#34; labels=\u0026#34;map[domain:chhanz.com hostname:w1.okd.chhanz.com mac:00:30:57:81:c9:14 serial: uuid:65136bfd-fd35-4289-b6f1-acec191e20f4]\u0026#34; profile=worker ... dhcpd 서버 설정 dhcpd 를 이용하여 ipxe boot 제공 방법은 아래와 같이 dhcpd.conf 에 내용을 추가하면 됩니다.\n... next-server 20.0.0.1; filename \u0026#34;http://20.0.0.1:8080/boot.ipxe\u0026#34;; \u0026lt;\u0026lt; matchbox address ... 참고 자료  https://matchbox.psdn.io/ https://rheb.hatenablog.com/entry/rhcos-matchbox  ","permalink":"https://chhanz88.github.io/post/2021-01-26-matchbox/","summary":"[OpenShift] Matchbox 를 이용하여 OpenShift 배포서버 구성 OpenShift / OKD 4 version 이 되면서 일반 Linux (RHEL/CentOS) 가 아닌 CoreOS (RHCOS/Fedora CoreOS) 를 사용하면서 OpenShift 설치 방법이 많이 달라졌습니다.\nOpenShift 설치를 위해 ignition, kernel image, initramfs, rootfs 등의 배포 관리가 필요합니다.\nOpenShift Document 에서는 httpd 및 pxe 를 이용하여 배포를 하도록 하지만 matchbox 를 이용하면 향후 운영에도 편리한 배포가 가능합니다.\nmatchbox 란? matchbox 의 주요 기능은 MAC 주소 기반으로 OpenShift 설치에 필요한 파일을 제공하는 오픈소스 솔루션입니다.","title":"[OpenShift] Matchbox 를 이용하여 OpenShift 배포서버 구성"},{"content":"[Ansible] USER/GROUP 생성 Playbook 잡설 (회고?) 연말과 연초가 너무 바빴습니다. 몸이 바쁘다 보니 글 쓰는게 소홀했습니다. 올해는 바쁘더라도 짧은 내용이라도 더 열심히 글 작성해보겠습니다.\nPlaybook 위에서 말한 것과 같이 작업도 많고 시간은 부족한데 BAU 업무는 여전히 진행 되야 합니다.\n이번에 작성한 Playbook 은 다량의 시스템에 USER과 GROUP 을 생성하는 Playbook 을 작성하였습니다.\nPlaybook 을 이용하여 BAU 업무 투자 시간이 채감상 3분의 1로 줄었습니다.\nPlaybook 사용법 Clone Playbook 아래와 같이 Playbook 을 Clone 진행합니다.\n$ cd /etc/ansible/roles $ sudo git clone https://github.com/chhanz/ansible.role-mgt-usergrp.git Playbook 및 Varfile 작성 아래와 같이 Playbook 및 Varfile 을 작성합니다.\n예제 Playbook 과 Varfile 은 ansible.role-mgt-usergrp/tests/ 경로의 예제 파일을 참고 하시면 됩니다.\nPlaybook 예제 $ cat create_usergrp.yml --- - name: management \u0026#39;user\u0026#39;  hosts: node  vars_files:  - userlist.yml  roles:  - role: ansible.role-mgt-usergrp Varfile 예제 $ cat userlist.yml --- group_list:  - grp_name: demogrp  gid: 777  user_list:  - id: demouser  new_password: 6af286f0509e7c166abf710850f44fc4  uid: 770  groups: demogrp  comment: \u0026#34;KR/ANSIBLE/DEMO/USER/\u0026#34;  - id: demouser1  new_password: 6af286f0509e7c166abf710850f44fc4  uid: 771  groups: demogrp  comment: \u0026#34;KR/ANSIBLE/DEMO/USER/\u0026#34;  - id: demouser2  new_password: 6af286f0509e7c166abf710850f44fc4  uid: 772  groups: demogrp  comment: \u0026#34;KR/ANSIBLE/DEMO/USER/\u0026#34;  - id: demouser3  new_password: 6af286f0509e7c166abf710850f44fc4  uid: 773  groups: demogrp  comment: \u0026#34;KR/ANSIBLE/DEMO/USER/\u0026#34; Inventory 예제 $ cat inventory [node] 192.168.200.201 192.168.200.202  [node:vars] ansible_ssh_user=root ansible_ssh_pass=testtest Playbook 수행 아래와 같이 Playbook 을 수행합니다.\n$ ansible-playbook -i inventory create_usergrp.yml 참고 : 수행 로그 $ ansible-playbook -i inventory create_usergrp.yml  PLAY [management \u0026#39;user\u0026#39;] **********************************************************************************  TASK [Gathering Facts] ************************************************************************************ ok: [192.168.200.202] ok: [192.168.200.201]  TASK [ansible.role-mgt-usergrp : create \u0026#39;groups\u0026#39;] ********************************************************* changed: [192.168.200.202] =\u0026gt; (item={u\u0026#39;grp_name\u0026#39;: u\u0026#39;demogrp\u0026#39;, u\u0026#39;gid\u0026#39;: 777}) changed: [192.168.200.201] =\u0026gt; (item={u\u0026#39;grp_name\u0026#39;: u\u0026#39;demogrp\u0026#39;, u\u0026#39;gid\u0026#39;: 777})  TASK [ansible.role-mgt-usergrp : create \u0026#39;users\u0026#39;] ********************************************************** changed: [192.168.200.202] =\u0026gt; (item={u\u0026#39;comment\u0026#39;: u\u0026#39;KR/ANSIBLE/DEMO/USER/\u0026#39;, u\u0026#39;new_password\u0026#39;: u\u0026#39;6af286f0509e7c166abf710850f44fc4\u0026#39;, u\u0026#39;id\u0026#39;: u\u0026#39;demouser\u0026#39;, u\u0026#39;groups\u0026#39;: u\u0026#39;demogrp\u0026#39;, u\u0026#39;uid\u0026#39;: 770}) changed: [192.168.200.201] =\u0026gt; (item={u\u0026#39;comment\u0026#39;: u\u0026#39;KR/ANSIBLE/DEMO/USER/\u0026#39;, u\u0026#39;new_password\u0026#39;: u\u0026#39;6af286f0509e7c166abf710850f44fc4\u0026#39;, u\u0026#39;id\u0026#39;: u\u0026#39;demouser\u0026#39;, u\u0026#39;groups\u0026#39;: u\u0026#39;demogrp\u0026#39;, u\u0026#39;uid\u0026#39;: 770}) changed: [192.168.200.202] =\u0026gt; (item={u\u0026#39;comment\u0026#39;: u\u0026#39;KR/ANSIBLE/DEMO/USER/\u0026#39;, u\u0026#39;new_password\u0026#39;: u\u0026#39;6af286f0509e7c166abf710850f44fc4\u0026#39;, u\u0026#39;id\u0026#39;: u\u0026#39;demouser1\u0026#39;, u\u0026#39;groups\u0026#39;: u\u0026#39;demogrp\u0026#39;, u\u0026#39;uid\u0026#39;: 771}) changed: [192.168.200.201] =\u0026gt; (item={u\u0026#39;comment\u0026#39;: u\u0026#39;KR/ANSIBLE/DEMO/USER/\u0026#39;, u\u0026#39;new_password\u0026#39;: u\u0026#39;6af286f0509e7c166abf710850f44fc4\u0026#39;, u\u0026#39;id\u0026#39;: u\u0026#39;demouser1\u0026#39;, u\u0026#39;groups\u0026#39;: u\u0026#39;demogrp\u0026#39;, u\u0026#39;uid\u0026#39;: 771}) changed: [192.168.200.201] =\u0026gt; (item={u\u0026#39;comment\u0026#39;: u\u0026#39;KR/ANSIBLE/DEMO/USER/\u0026#39;, u\u0026#39;new_password\u0026#39;: u\u0026#39;6af286f0509e7c166abf710850f44fc4\u0026#39;, u\u0026#39;id\u0026#39;: u\u0026#39;demouser2\u0026#39;, u\u0026#39;groups\u0026#39;: u\u0026#39;demogrp\u0026#39;, u\u0026#39;uid\u0026#39;: 772}) changed: [192.168.200.202] =\u0026gt; (item={u\u0026#39;comment\u0026#39;: u\u0026#39;KR/ANSIBLE/DEMO/USER/\u0026#39;, u\u0026#39;new_password\u0026#39;: u\u0026#39;6af286f0509e7c166abf710850f44fc4\u0026#39;, u\u0026#39;id\u0026#39;: u\u0026#39;demouser2\u0026#39;, u\u0026#39;groups\u0026#39;: u\u0026#39;demogrp\u0026#39;, u\u0026#39;uid\u0026#39;: 772}) changed: [192.168.200.202] =\u0026gt; (item={u\u0026#39;comment\u0026#39;: u\u0026#39;KR/ANSIBLE/DEMO/USER/\u0026#39;, u\u0026#39;new_password\u0026#39;: u\u0026#39;6af286f0509e7c166abf710850f44fc4\u0026#39;, u\u0026#39;id\u0026#39;: u\u0026#39;demouser3\u0026#39;, u\u0026#39;groups\u0026#39;: u\u0026#39;demogrp\u0026#39;, u\u0026#39;uid\u0026#39;: 773}) changed: [192.168.200.201] =\u0026gt; (item={u\u0026#39;comment\u0026#39;: u\u0026#39;KR/ANSIBLE/DEMO/USER/\u0026#39;, u\u0026#39;new_password\u0026#39;: u\u0026#39;6af286f0509e7c166abf710850f44fc4\u0026#39;, u\u0026#39;id\u0026#39;: u\u0026#39;demouser3\u0026#39;, u\u0026#39;groups\u0026#39;: u\u0026#39;demogrp\u0026#39;, u\u0026#39;uid\u0026#39;: 773})  PLAY RECAP ************************************************************************************************ 192.168.200.201 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.200.202 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 점검 아래와 같이 계정 생성이 문제없이 된 것을 확인 할 수 있습니다.\n[root@fastvm-centos-7-7-201 ~]# cat /etc/passwd | tail -n4 demouser:x:770:777:KR/ANSIBLE/DEMO/USER/:/home/demouser:/bin/bash demouser1:x:771:777:KR/ANSIBLE/DEMO/USER/:/home/demouser1:/bin/bash demouser2:x:772:777:KR/ANSIBLE/DEMO/USER/:/home/demouser2:/bin/bash demouser3:x:773:777:KR/ANSIBLE/DEMO/USER/:/home/demouser3:/bin/bash  [root@fastvm-centos-7-7-201 ~]# id demouser uid=770(demouser) gid=777(demogrp) groups=777(demogrp)  [root@fastvm-centos-7-7-201 ~]# id demouser1 uid=771(demouser1) gid=777(demogrp) groups=777(demogrp)  [root@fastvm-centos-7-7-201 ~]# id demouser2 uid=772(demouser2) gid=777(demogrp) groups=777(demogrp)  [root@fastvm-centos-7-7-201 ~]# id demouser3 uid=773(demouser3) gid=777(demogrp) groups=777(demogrp) 참고 자료  https://github.com/chhanz/ansible.role-mgt-usergrp  ","permalink":"https://chhanz88.github.io/post/2021-01-18-ansible-role-user-grp/","summary":"[Ansible] USER/GROUP 생성 Playbook 잡설 (회고?) 연말과 연초가 너무 바빴습니다. 몸이 바쁘다 보니 글 쓰는게 소홀했습니다. 올해는 바쁘더라도 짧은 내용이라도 더 열심히 글 작성해보겠습니다.\nPlaybook 위에서 말한 것과 같이 작업도 많고 시간은 부족한데 BAU 업무는 여전히 진행 되야 합니다.\n이번에 작성한 Playbook 은 다량의 시스템에 USER과 GROUP 을 생성하는 Playbook 을 작성하였습니다.\nPlaybook 을 이용하여 BAU 업무 투자 시간이 채감상 3분의 1로 줄었습니다.\nPlaybook 사용법 Clone Playbook 아래와 같이 Playbook 을 Clone 진행합니다.","title":"[Ansible] USER/GROUP 생성 Playbook"},{"content":"YumRepo Error: All mirror URLs are not using ftp, http[s] or file. 해결 방법 아래와 같이 CentOS 6에서 발생되는 이슈입니다.\n[root@fastvm-centos-6-10-51 ~]# yum repolist Loaded plugins: fastestmirror Determining fastest mirrors YumRepo Error: All mirror URLs are not using ftp, http[s] or file.  Eg. Invalid release/repo/arch combination/ removing mirrorlist with no valid mirrors: /var/cache/yum/x86_64/6/base/mirrorlist.txt YumRepo Error: All mirror URLs are not using ftp, http[s] or file.  Eg. Invalid release/repo/arch combination/ removing mirrorlist with no valid mirrors: /var/cache/yum/x86_64/6/extras/mirrorlist.txt YumRepo Error: All mirror URLs are not using ftp, http[s] or file.  Eg. Invalid release/repo/arch combination/ removing mirrorlist with no valid mirrors: /var/cache/yum/x86_64/6/updates/mirrorlist.txt repo id repo name status base CentOS-6 - Base 0 extras CentOS-6 - Extras 0 updates CentOS-6 - Updates 0 repolist: 0 발생되는 이유는 CentOS 6 의 EOL 이 2020-11-30 로 인해 fastmirror site 에서 CentOS 6 Package가 제거 되어서 그렇습니다.\n해결 방법(임시) 아래와 같이 mirrorlist.txt 에 CentOS Vault Repository 를 추가합니다.\n$ echo \u0026#34;https://vault.centos.org/6.10/os/x86_64/\u0026#34; \u0026gt; /var/cache/yum/x86_64/6/base/mirrorlist.txt $ echo \u0026#34;https://vault.centos.org/6.10/extras/x86_64/\u0026#34; \u0026gt; /var/cache/yum/x86_64/6/extras/mirrorlist.txt $ echo \u0026#34;https://vault.centos.org/6.10/updates/x86_64/\u0026#34; \u0026gt; /var/cache/yum/x86_64/6/updates/mirrorlist.txt 추가 이후에 아래와 같이 yum repolist 가 정상 작동 되는 것을 볼 수 있습니다.\n[root@fastvm-centos-6-10-51 ~]# yum repolist Loaded plugins: fastestmirror Determining fastest mirrors base | 3.7 kB 00:00 base/primary_db | 4.7 MB 00:03 extras | 3.4 kB 00:00 extras/primary_db | 29 kB 00:00 updates | 3.4 kB 00:00 updates/primary_db | 12 MB 00:07 repo id repo name status base CentOS-6 - Base 6,713 extras CentOS-6 - Extras 47 updates CentOS-6 - Updates 1,193 repolist: 7,953 위와 같이 적용하면 yum clean all 명령어가 수행이 되면 다시 동일한 문제가 발생 될 것입니다.\n해결 방법(영구) 아래와 같이 /etc/yum.repos.d/ 하위의 repo 파일을 수정합니다.\n$ sed -i -e \u0026#34;s/^mirrorlist=http:\\/\\/mirrorlist.centos.org/#mirrorlist=http:\\/\\/mirrorlist.centos.org/g\u0026#34; /etc/yum.repos.d/CentOS-Base.repo $ sed -i -e \u0026#34;s/^#baseurl=http:\\/\\/mirror.centos.org/baseurl=https:\\/\\/vault.centos.org/g\u0026#34; /etc/yum.repos.d/CentOS-Base.repo 아래와 같이 repo 가 수정됩니다.\n$ diff CentOS-Base.repo.old CentOS-Base.repo 15,16c15,16 \u0026lt; mirrorlist=http://mirrorlist.centos.org/?release=$releasever\u0026amp;arch=$basearch\u0026amp;repo=os\u0026amp;infra=$infra \u0026lt; #baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/ --- \u0026gt; #mirrorlist=http://mirrorlist.centos.org/?release=$releasever\u0026amp;arch=$basearch\u0026amp;repo=os\u0026amp;infra=$infra \u0026gt; baseurl=https://vault.centos.org/centos/$releasever/os/$basearch/ 23,24c23,24 \u0026lt; mirrorlist=http://mirrorlist.centos.org/?release=$releasever\u0026amp;arch=$basearch\u0026amp;repo=updates\u0026amp;infra=$infra \u0026lt; #baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/ --- \u0026gt; #mirrorlist=http://mirrorlist.centos.org/?release=$releasever\u0026amp;arch=$basearch\u0026amp;repo=updates\u0026amp;infra=$infra \u0026gt; baseurl=https://vault.centos.org/centos/$releasever/updates/$basearch/ 31,32c31,32 \u0026lt; mirrorlist=http://mirrorlist.centos.org/?release=$releasever\u0026amp;arch=$basearch\u0026amp;repo=extras\u0026amp;infra=$infra \u0026lt; #baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/ --- \u0026gt; #mirrorlist=http://mirrorlist.centos.org/?release=$releasever\u0026amp;arch=$basearch\u0026amp;repo=extras\u0026amp;infra=$infra \u0026gt; baseurl=https://vault.centos.org/centos/$releasever/extras/$basearch/ 아래와 같이 yum cache 가 clean 되어도 mirror site 정보가 유지 됩니다.\n[root@fastvm-centos-6-10-51 yum.repos.d]# yum clean all Loaded plugins: fastestmirror Cleaning repos: base extras updates Cleaning up Everything [root@fastvm-centos-6-10-51 yum.repos.d]# yum repolist Loaded plugins: fastestmirror Determining fastest mirrors base | 3.7 kB 00:00 base/primary_db | 4.7 MB 00:03 extras | 3.4 kB 00:00 extras/primary_db | 29 kB 00:00 updates | 3.4 kB 00:00 updates/primary_db | 12 MB 00:07 repo id repo name status base CentOS-6 - Base 6,713 extras CentOS-6 - Extras 47 updates CentOS-6 - Updates 1,193 repolist: 7,953 [root@fastvm-centos-6-10-51 yum.repos.d]# 결론 Vault 가 엄청 느립니다 \u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\n참고 자료  https://www.reddit.com/r/sysadmin/comments/k63mcw/centos_repositories_down/ https://qiita.com/imunew/items/3810a41960f40db85c94 https://forum.directadmin.com/threads/how-do-i-fix-these-yum-errors.54874/  ","permalink":"https://chhanz88.github.io/post/2020-12-10-centos-6-yumrepo-error/","summary":"YumRepo Error: All mirror URLs are not using ftp, http[s] or file. 해결 방법 아래와 같이 CentOS 6에서 발생되는 이슈입니다.\n[root@fastvm-centos-6-10-51 ~]# yum repolist Loaded plugins: fastestmirror Determining fastest mirrors YumRepo Error: All mirror URLs are not using ftp, http[s] or file.  Eg. Invalid release/repo/arch combination/ removing mirrorlist with no valid mirrors: /var/cache/yum/x86_64/6/base/mirrorlist.txt YumRepo Error: All mirror URLs are not using ftp, http[s] or file.  Eg. Invalid release/repo/arch combination/ removing mirrorlist with no valid mirrors: /var/cache/yum/x86_64/6/extras/mirrorlist.","title":"[Linux] `YumRepo Error: All mirror URLs are not using ftp, http[s] or file.`"},{"content":"haproxy 를 이용한 RoundRobin 구성 (on CentOS7)  HAPROXY 를 이용하여 HW L4 를 구현 할 수 있다?????\n 이번 포스팅에서는 haproxy 의 RoundRobin(rr) 알고리즘을 이용하여 RoundRobin 웹서비스를 구현하도록 하겠습니다.\n테스트용 웹 서비스 아래와 같이 각각의 flask app 서비스를 하는 웹 서비스를 테스트에 활용합니다.\n[root@k3s-10-50-1-70 ~]# kubectl get all NAME READY STATUS RESTARTS AGE pod/pod-test-app-1 1/1 Running 0 2m47s pod/pod-test-app-2 1/1 Running 0 83s pod/pod-test-app-3 1/1 Running 0 69s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 54d service/pod-test-app-1 NodePort 10.43.176.145 \u0026lt;none\u0026gt; 80:31636/TCP 43s \u0026lt;\u0026lt; service/pod-test-app-2 NodePort 10.43.172.6 \u0026lt;none\u0026gt; 80:31881/TCP 38s \u0026lt;\u0026lt; service/pod-test-app-3 NodePort 10.43.115.212 \u0026lt;none\u0026gt; 80:30230/TCP 36s \u0026lt;\u0026lt; haproxy 설치 haproxy 를 설치하기전에 SELinux, firewalld 는 disabled 합니다.\nSELinux , firewalld 를 사용하려면 만약 SELinux 와 firewalld 를 사용하려면 아래와 커멘드를 활용합니다.\nSELinux # setsebool -P haproxy_connect_any 1 firewalld # firewalld-cmd --add-service=http # firewalld-cmd --add-service=http --permanent # firewalld-cmd --reload Install 설치는 아래 커멘드로 진행됩니다.\n# yum -y install haproxy Start/Stop Service 서비스의 시작/중지는 아래 커멘드로 진행됩니다.\n+ start console # systemctl start haproxy  + stop console # systemctl stop haproxy \nhaproxy 설정 기본적으로 haproxy 는 /etc/haproxy/haproxy.cfg 설정 파일을 수정하여 설정합니다.\n테스트용 서비스를 RoundRobin 구성을 하기 위해 설정 파일을 수정합니다.\n... frontend testweb-front  bind *:8080  default_backend testweb-backend  mode tcp  option tcplog backend testweb-backend  balance roundrobin  mode tcp  server w1 10.50.1.70:31636 check  server w2 10.50.1.70:31881 check  server w3 10.50.1.70:30230 check ...  참고 : default 로 포함되어 있던 frontend 및 backend 는 삭제하였습니다.  haproxy 알고리즘 아래와 같이 blance 부분이 알고리즘을 적용하는 설정 부분입니다.\n... backend testweb-backend  balance roundrobin \u0026lt;\u0026lt; ... haproxy 에서 사용 가능한 알고리즘은 아래와 같습니다.\n roundrobin : 순차적으로 분배 (최대 연결 가능 서버 4128개) static-rr : 서버에 부여된 가중치(weight)에 따라서 분배 leastconn : 접속 수가 가장 적은 서버로 분배 source : 운영 중인 서버의 가중치를 나눠서 접속자 IP를 해싱(hashing)해서 분배 uri : 접속하는 URI를 해싱해서 운영 중인 서버의 가중치를 나눠서 분배(URI의 길이 또는 depth로 해싱) rdp-cookie : TCP 요청에 대한 RDP 쿠키에 따른 분배  haproxy roundrobin 테스트 실제로 roundrobin 방식으로 웹서비스가 접근이 되는지 확인합니다.\n[root@lb haproxy]# while true; do curl lb.okd.chhanz.com:8080; done  Container LAB | POD Working : pod-test-app-1 | v=1  Container LAB | POD Working : pod-test-app-2 | v=1  Container LAB | POD Working : pod-test-app-3 | v=1  Container LAB | POD Working : pod-test-app-1 | v=1  Container LAB | POD Working : pod-test-app-2 | v=1  Container LAB | POD Working : pod-test-app-3 | v=1  Container LAB | POD Working : pod-test-app-1 | v=1  Container LAB | POD Working : pod-test-app-2 | v=1  Container LAB | POD Working : pod-test-app-3 | v=1  Container LAB | POD Working : pod-test-app-1 | v=1 ... 위와 같이 순서대로 (1 \u0026gt; 2 \u0026gt; 3) 웹서비스에 접근하는 것을 확인 할 수 있었습니다.\n참고 자료  https://cyuu.tistory.com/category/haproxy  ","permalink":"https://chhanz88.github.io/post/2020-11-30-configuration-haproxy-rr/","summary":"haproxy 를 이용한 RoundRobin 구성 (on CentOS7)  HAPROXY 를 이용하여 HW L4 를 구현 할 수 있다?????\n 이번 포스팅에서는 haproxy 의 RoundRobin(rr) 알고리즘을 이용하여 RoundRobin 웹서비스를 구현하도록 하겠습니다.\n테스트용 웹 서비스 아래와 같이 각각의 flask app 서비스를 하는 웹 서비스를 테스트에 활용합니다.\n[root@k3s-10-50-1-70 ~]# kubectl get all NAME READY STATUS RESTARTS AGE pod/pod-test-app-1 1/1 Running 0 2m47s pod/pod-test-app-2 1/1 Running 0 83s pod/pod-test-app-3 1/1 Running 0 69s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.","title":"[Linux] haproxy 를 이용한 RoundRobin 구성 (on CentOS7)"},{"content":"DHCP 란?  동적 호스트 구성 프로토콜(Dynamic Host Configuration Protocol, DHCP)은 호스트 IP 구성 관리를 단순화하는 IP 표준이다.\n동적 호스트 구성 프로토콜 표준에서는 DHCP 서버를 사용하여 IP 주소 및 관련된 기타 구성 세부 정보를 네트워크의 DHCP 사용 클라이언트에게 동적으로 할당하는 방법을 제공한다.\n 참고 : 위키백과 - DHCP   DHCP 동작 원리 아래 내용을 보면 DHCP 의 동작 원리를 쉽게 이해 할 수 있습니다.\n 출처 : https://www.netmanias.com/ko/post/blog/5348/dhcp-ip-allocation-network-protocol/understanding-the-basic-operations-of-dhcp  DHCP 설치 아래 커멘드를 통해 Package 를 설치합니다.\n$ sudo yum -y install dhcp 아래 커멘드를 통해 DHCP 서비스를 시작합니다.\n$ sudo systemctl enable --now dhcpd 아래 커멘드를 통해 DHCP 서비스 상태를 확인 할 수 있습니다.\n$ sudo systemctl status dhcpd ● dhcpd.service - DHCPv4 Server Daemon  Loaded: loaded (/usr/lib/systemd/system/dhcpd.service; enabled; vendor preset: disabled)  Active: active (running) since Tue 2020-10-13 21:47:50 KST; 1 months 4 days ago  Docs: man:dhcpd(8)  man:dhcpd.conf(5)  Main PID: 23447 (dhcpd)  Status: \u0026#34;Dispatching packets...\u0026#34;  CGroup: /system.slice/dhcpd.service  └─23447 /usr/sbin/dhcpd -f -cf /etc/dhcp/dhcpd.conf -user dhcpd -group dhcpd --no-pid  Nov 17 16:34:16 dhcp.okd.chhanz.com dhcpd[23447]: Dynamic and static leases present for 10.50.2.22. Nov 17 16:34:16 dhcp.okd.chhanz.com dhcpd[23447]: Remove host declaration boot.okd.chhanz.com or remove 10.50.2.22 Nov 17 16:34:16 dhcp.okd.chhanz.com dhcpd[23447]: from the dynamic address pool for 10.50.2.0/24 Nov 17 16:34:16 dhcp.okd.chhanz.com dhcpd[23447]: DHCPREQUEST for 10.50.2.22 from 00:50:56:91:c9:22 via ens160 Nov 17 16:34:16 dhcp.okd.chhanz.com dhcpd[23447]: DHCPACK on 10.50.2.22 to 00:50:56:91:c9:22 via ens160 Nov 17 16:34:20 dhcp.okd.chhanz.com dhcpd[23447]: Dynamic and static leases present for 10.50.2.42. Nov 17 16:34:20 dhcp.okd.chhanz.com dhcpd[23447]: Remove host declaration m2.okd.igts.com or remove 10.50.2.42 Nov 17 16:34:20 dhcp.okd.chhanz.com dhcpd[23447]: from the dynamic address pool for 10.50.2.0/24 Nov 17 16:34:20 dhcp.okd.chhanz.com dhcpd[23447]: DHCPREQUEST for 10.50.2.42 from 00:50:56:91:c9:42 via ens160 Nov 17 16:34:20 dhcp.okd.chhanz.com dhcpd[23447]: DHCPACK on 10.50.2.42 to 00:50:56:91:c9:42 via ens160 DHCP 설정 DHCP 설정 파일인 /etc/dhcp/dhcpd.conf 을 설정합니다.\n# Global configuration #################################### option domain-name \u0026#34;chhanz.com\u0026#34;; option domain-name-servers ns.chhanz.com; default-lease-time 3600; //기본 임대 시간 max-lease-time 7200; //최대 임대 시간 authoritative; # subnet configuration #################################### subnet 10.50.2.0 netmask 255.255.255.0 {  option routers 10.50.2.254;  option subnet-mask 255.255.255.0;  option domain-name \u0026#34;chhanz.com\u0026#34;;  option domain-name-servers 10.50.2.51, 1.1.1.1;  option time-offset -18000;  range 10.50.2.52 10.50.2.59; } 위와 같이 Global Configuration 에 공통 설정을 입력 할 수도 있고, subnet Configuration 에 별도로도 설정이 가능합니다.\ndhcpd.conf 항목 설명  option domain-name : Domain name을 지정합니다. option domain-name-servers : DNS 서버를 지정합니다. default-lease-time : 임대 요청 만료 시간을 초단위로 지정합니다. max-lease-time : 클라이언트가 가지고 IP를 가지고 있을 최대 시간을 초단위로 지정합니다. option routers : Gateway 주소를 지정합니다. range : 클라이언트에 할당할 IP의 범위를 지정합니다.  임대 IP 고정 할당 관련 DHCP 서버에서 MAC Address 기반으로 특정 IP 를 지정하여 할당하도록 설정이 가능합니다.\n/etc/dhcp/dhcpd.conf 하단에 아래와 같은 설정을 추가합니다.\nhost boot.okd.chhanz.com {  hardware ethernet 00:50:57:81:c8:50;  fixed-address 10.50.2.52; } 위와 같이 설정하는 경우, 00:50:57:81:c8:50 MAC Address 는 10.50.2.52 으로 IP 를 할당 받을 것 입니다.\nDHCP Client 테스트 아래와 같이 가상화 서버에서 VM 생성할 때, MAC 주소를 수동으로 설정하거나,\n물리 장비(baremetal) 인 경우, 미리 MAC 주소를 파악하여 DHCP 서버에 설정합니다.\nDHCP Client log 아래와 같은 과정으로 DHCP 서비스가 작동하는 것을 확인 할 수 있습니다.\n$ tailf /var/log/messages ... Nov 16 22:26:04 c7 dhcpd: DHCPDISCOVER from 00:50:57:81:c8:50 via ens160 Nov 16 22:26:04 c7 dhcpd: DHCPOFFER on 10.50.2.50 to 00:50:57:81:c8:50 via ens160 Nov 16 22:26:04 c7 dhcpd: DHCPREQUEST for 10.50.2.50 (10.50.2.52) from 00:50:57:81:c8:50 via ens160 Nov 16 22:26:04 c7 dhcpd: DHCPACK on 10.50.2.50 to 00:50:57:81:c8:50 via ens160 ... 참고 자료  https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/sec-dhcp-configuring-server https://www.netmanias.com/ko/post/blog/5348/dhcp-ip-allocation-network-protocol/understanding-the-basic-operations-of-dhcp  ","permalink":"https://chhanz88.github.io/post/2020-11-17-configuration-dhcp/","summary":"DHCP 란?  동적 호스트 구성 프로토콜(Dynamic Host Configuration Protocol, DHCP)은 호스트 IP 구성 관리를 단순화하는 IP 표준이다.\n동적 호스트 구성 프로토콜 표준에서는 DHCP 서버를 사용하여 IP 주소 및 관련된 기타 구성 세부 정보를 네트워크의 DHCP 사용 클라이언트에게 동적으로 할당하는 방법을 제공한다.\n 참고 : 위키백과 - DHCP   DHCP 동작 원리 아래 내용을 보면 DHCP 의 동작 원리를 쉽게 이해 할 수 있습니다.\n 출처 : https://www.netmanias.com/ko/post/blog/5348/dhcp-ip-allocation-network-protocol/understanding-the-basic-operations-of-dhcp  DHCP 설치 아래 커멘드를 통해 Package 를 설치합니다.","title":"[Linux] DHCP 서버 구성 (on CentOS7)"},{"content":"DNS 설치 (on CentOS 7) DNS 설치는 아래 커맨드를 통해 설치가 가능합니다.\n$ sudo yum -y install bind DNS 실행 아래 커맨드로 실행을 합니다.\n$ sudo systemctl enable --now named $ sudo systemctl status named ● named.service - Berkeley Internet Name Domain (DNS)  Loaded: loaded (/usr/lib/systemd/system/named.service; enabled; vendor preset: disabled)  Active: active (running) since Thu 2020-11-05 10:27:12 KST; 889ms ago  Process: 25609 ExecStart=/usr/sbin/named -u named -c ${NAMEDCONF} $OPTIONS (code=exited, status=0/SUCCESS)  Process: 25607 ExecStartPre=/bin/bash -c if [ ! \u0026#34;$DISABLE_ZONE_CHECKING\u0026#34; == \u0026#34;yes\u0026#34; ]; then /usr/sbin/named-checkconf -z \u0026#34;$NAMEDCONF\u0026#34;; else echo \u0026#34;Checking of zone files is disabled\u0026#34;; fi (code=exited, status=0/SUCCESS)  Main PID: 25611 (named)  CGroup: /system.slice/named.service  └─25611 /usr/sbin/named -u named -c /etc/named.conf  Nov 05 10:27:12 ns.chhanz.com named[25611]: network unreachable resolving \u0026#39;./DNSKEY/IN\u0026#39;: 2001:7fd::1#53 Nov 05 10:27:12 ns.chhanz.com named[25611]: network unreachable resolving \u0026#39;./NS/IN\u0026#39;: 2001:7fd::1#53 Nov 05 10:27:12 ns.chhanz.com named[25611]: network unreachable resolving \u0026#39;./DNSKEY/IN\u0026#39;: 2001:503:c27::2:30#53 Nov 05 10:27:12 ns.chhanz.com named[25611]: network unreachable resolving \u0026#39;./NS/IN\u0026#39;: 2001:503:c27::2:30#53 Nov 05 10:27:12 ns.chhanz.com named[25611]: network unreachable resolving \u0026#39;./DNSKEY/IN\u0026#39;: 2001:500:9f::42#53 Nov 05 10:27:12 ns.chhanz.com named[25611]: network unreachable resolving \u0026#39;./NS/IN\u0026#39;: 2001:500:9f::42#53 Nov 05 10:27:12 ns.chhanz.com named[25611]: network unreachable resolving \u0026#39;./DNSKEY/IN\u0026#39;: 2001:500:2d::d#53 Nov 05 10:27:12 ns.chhanz.com named[25611]: network unreachable resolving \u0026#39;./NS/IN\u0026#39;: 2001:500:2d::d#53 Nov 05 10:27:12 ns.chhanz.com named[25611]: managed-keys-zone: Key 20326 for zone . acceptance timer complete: key now trusted Nov 05 10:27:12 ns.chhanz.com named[25611]: resolver priming query complete DNS 설정 /etc/named.conf 를 수정합니다.\n$ sudo vi /etc/named.conf ...  options {  listen-on port 53 { 127.0.0.1; any; }; ## any 추가  listen-on-v6 port 53 { ::1; };  directory \u0026#34;/var/named\u0026#34;;  dump-file \u0026#34;/var/named/data/cache_dump.db\u0026#34;;  statistics-file \u0026#34;/var/named/data/named_stats.txt\u0026#34;;  memstatistics-file \u0026#34;/var/named/data/named_mem_stats.txt\u0026#34;;  recursing-file \u0026#34;/var/named/data/named.recursing\u0026#34;;  secroots-file \u0026#34;/var/named/data/named.secroots\u0026#34;;  allow-query { localhost; any; }; ## any 추가  }; ... 위와 같이 any 를 추가하여 모든 대역의 Query 를 허용합니다.\n(보안상의 이유로 특정 IP 대역만 Allow 할 수도 있습니다.)\nZone 추가 chhanz.com 이라는 신규 Domain 을 추가하기 위해서는 /etc/named.rfc1912.zones 을 수정합니다.\n해당 파일 하단에 아래와 같이 Zone file 정보를 입력합니다.\n... zone \u0026#34;chhanz.com\u0026#34; IN {  type master;  file \u0026#34;chhanz.com.zone\u0026#34;;  allow-update { none; }; }; ... Zone file 생성 Zone 에서 설정한 chhanz.com.zone 이라는 file 을 /var/named 에 생성합니다.\n해당 file 의 권한은 꼭 named 계정이 읽을 수 있도록 설정합니다.\n$TTL 1D @ IN SOA @ ns.chhanz.com. (  0 ; serial  1D ; refresh  1H ; retry  1W ; expire  3H ) ; minimum  IN NS ns.chhanz.com. ns IN A 10.50.2.51  dhcp.okd IN A 10.50.2.52 lb.okd IN A 10.50.2.50 boot.okd IN A 10.50.2.53 m1.okd IN A 10.50.2.54 m2.okd IN A 10.50.2.55 m3.okd IN A 10.50.2.56 w1.okd IN A 10.50.2.57 ; Zone file 설정값  참고 자료 : https://wiki.debian.org/Bind9  DNS 서비스 재시작 위와 같이 Zone 설정이 완료되면 아래 커맨드를 통해 DNS 서비스를 재시작 합니다.\n$ sudo systemctl restart named DNS 기능 테스트 해당 서버 /etc/resolv.conf 가 구성한 DNS 서버로 지정 해야됩니다.\n[root@ns ~]# cat /etc/resolv.conf # Generated by NetworkManager search chhanz.com nameserver 10.50.2.51 아래와 같이 DNS 작동을 확인합니다.\n[root@ns ~]# dig boot.okd.chhanz.com ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.4-P2-RedHat-9.11.4-16.P2.el7_8.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; boot.okd.chhanz.com ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 60569 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 1, ADDITIONAL: 2 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;boot.okd.chhanz.com. IN A ;; ANSWER SECTION: boot.okd.chhanz.com. 86400 IN A 10.50.2.53 ;; AUTHORITY SECTION: chhanz.com. 86400 IN NS ns.chhanz.com. ;; ADDITIONAL SECTION: ns.chhanz.com. 86400 IN A 10.50.2.51 ;; Query time: 0 msec ;; SERVER: 10.50.2.51#53(10.50.2.51) ;; WHEN: Fri Nov 06 15:07:55 KST 2020 ;; MSG SIZE rcvd: 97 DNS forwarders 설정 현재 테스트 환경의 Main DNS 는 윈도우 DNS 입니다.\n사용자는 DHCP 로 자동으로 IP 를 할당 받으면 윈도우 DNS 로 설정이 됩니다.\nDNS forwarder 를 설정하면 1차로 질의한 DNS 에 없는 Domain 은 forwarder 에 설정된 DNS 로 질의를 넘깁니다.\n그리하여 위와 같은 테스트 환경에서는 DHCP 수정이 없이 chhanz.com Domain 정보도 제공이 가능힙니다.\nWindows DNS 아래와 같이 Windows DNS 에서 DNS 속성 \u0026gt; 전달자 \u0026gt; Linux DNS 항목을 추가합니다.\n아래와 같이 Windows DNS 를 사용하는 시스템에서 전달자에 의해 Linux DNS 에서 제공하는 chhanz.com Domain 정보를 받아오는 것을 확인 할 수 있습니다.\n[root@class ~]# cat /etc/resolv.conf # Generated by NetworkManager search igts.com nameserver 10.50.1.11 nameserver 1.1.1.1 [root@class ~]# dig ns.chhanz.com ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.4-P2-RedHat-9.11.4-16.P2.el7_8.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; ns.chhanz.com ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 47100 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4000 ;; QUESTION SECTION: ;ns.chhanz.com. IN A ;; ANSWER SECTION: ns.chhanz.com. 85816 IN A 10.50.2.51 ;; Query time: 0 msec ;; SERVER: 10.50.1.11#53(10.50.1.11) ;; WHEN: Thu Nov 05 13:02:19 KST 2020 ;; MSG SIZE rcvd: 58 [root@class ~]# ping lb.okd.chhanz.com PING lb.okd.chhanz.com (10.50.2.50) 56(84) bytes of data. ^C --- lb.okd.chhanz.com ping statistics --- 2 packets transmitted, 0 received, 100% packet loss, time 1000ms [root@class ~]# Linux DNS Linux DNS 의 forwarders 는 아래와 같이 /etc/named.conf 에 forwarders 옵션을 수정합니다.\n... options {  directory \u0026#34;/var/named\u0026#34;;  forward only;  forwarders { 10.50.2.51; }; ## forwarder 설정  allow-query { any; }; }; ","permalink":"https://chhanz88.github.io/post/2020-11-06-configuration-dns/","summary":"DNS 설치 (on CentOS 7) DNS 설치는 아래 커맨드를 통해 설치가 가능합니다.\n$ sudo yum -y install bind DNS 실행 아래 커맨드로 실행을 합니다.\n$ sudo systemctl enable --now named $ sudo systemctl status named ● named.service - Berkeley Internet Name Domain (DNS)  Loaded: loaded (/usr/lib/systemd/system/named.service; enabled; vendor preset: disabled)  Active: active (running) since Thu 2020-11-05 10:27:12 KST; 889ms ago  Process: 25609 ExecStart=/usr/sbin/named -u named -c ${NAMEDCONF} $OPTIONS (code=exited, status=0/SUCCESS)  Process: 25607 ExecStartPre=/bin/bash -c if [ !","title":"[DNS] CentOS 7 - DNS 설치 및 forwarders 구성"},{"content":"[OpenShift] OpenShift 4.5 를 이용한 CI(Continuous Integration) 구성 이번 포스팅은 OpenShift 를 이용한 Continuous Integration(이하 CI)를 구성하도록 하겠습니다.\n 사전 구성 : gogs  Continuous Integration 란? CI는 개발자를 위한 자동화 프로세스인 지속적인 통합(Continuous Integration)을 의미합니다.\nCI를 성공적으로 구현할 경우 애플리케이션에 대한 새로운 코드 변경 사항이 정기적으로 빌드 및 테스트되어 공유 리포지토리에 통합되므로 여러 명의 개발자가 동시에 애플리케이션 개발과 관련된 코드 작업을 할 경우 서로 충돌할 수 있는 문제를 해결할 수 있습니다.\n 참고 자료 :\n https://blog.abiatechhub.com/devops-what-is-continous-integration-all-you-need-to-know/2020/by/zaghadon/ https://www.redhat.com/ko/topics/devops/what-is-ci-cd   gogs 계정 생성 아래 URL 로 접근하여 신규 계정을 생성합니다.\nhttp://gogs-mushroom.apps.ocp.igts.com/\n 아래 내용은 Git 사용에 미숙한 사용자를 대상으로 가이드 되어 있고,\nGit 사용에 능숙하다면 git 커맨드를 사용하여 저장소 구성 및 소스 생성하시면 됩니다.\n Git Repository 생성 \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt; \u0026lt;b\u0026gt; \u0026lt;?php $host=gethostname(); echo \u0026#34;pod name : \u0026#34;; echo $host; ?\u0026gt;\u0026lt;p\u0026gt; Image Version : original \u0026lt;/p\u0026gt; \u0026lt;/b\u0026gt; \u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Deploy APP Webhook 을 이용하여 Build 자동화 구현 지금까지 Update에 대해 수동으로 Build를 진행했습니다. 다음 과정은 source의 update가 확인되면 자동으로 OpenShift에서 해당 내용을 감지하고 Build하는 자동화 환경을 구현하려고 합니다.\n마치며 이번 포스팅은 OpenShift 내부에 gogs 를 이용하여 구성하였으나, OpenShift 가 외부 도메인으로 노출이 되어 있으면 Github or Gitlab 등에 같은 방법으로 CI 를 구성 할 수 있습니다.\n","permalink":"https://chhanz88.github.io/post/2020-10-16-ocp4-ci/","summary":"[OpenShift] OpenShift 4.5 를 이용한 CI(Continuous Integration) 구성 이번 포스팅은 OpenShift 를 이용한 Continuous Integration(이하 CI)를 구성하도록 하겠습니다.\n 사전 구성 : gogs  Continuous Integration 란? CI는 개발자를 위한 자동화 프로세스인 지속적인 통합(Continuous Integration)을 의미합니다.\nCI를 성공적으로 구현할 경우 애플리케이션에 대한 새로운 코드 변경 사항이 정기적으로 빌드 및 테스트되어 공유 리포지토리에 통합되므로 여러 명의 개발자가 동시에 애플리케이션 개발과 관련된 코드 작업을 할 경우 서로 충돌할 수 있는 문제를 해결할 수 있습니다.","title":"[OpenShift] OpenShift 를 이용한 CI(Continuous Integration) 구성"},{"content":"[oVirt] Install oVirt 4.3 (Self Hosted Engine)   oVirt Self Hosted engine 설치 가이드입니다.\noVirt 4.3 기준으로 작성 되었습니다.\n작성일 : 2019-11\n 마치며 해당 메뉴얼은 테스트 환경 기준으로 작성 되었습니다.\n운영 환경에 적용하기 위해서는 NAS or Storage 환경을 권장드립니다.\n","permalink":"https://chhanz88.github.io/post/2020-10-06-ovirt-install-self-hosted-engine/","summary":"[oVirt] Install oVirt 4.3 (Self Hosted Engine)   oVirt Self Hosted engine 설치 가이드입니다.\noVirt 4.3 기준으로 작성 되었습니다.\n작성일 : 2019-11\n 마치며 해당 메뉴얼은 테스트 환경 기준으로 작성 되었습니다.\n운영 환경에 적용하기 위해서는 NAS or Storage 환경을 권장드립니다.","title":"[oVirt] Install oVirt 4.3 (Self Hosted Engine)"},{"content":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Update APP 이 문서는 Rolling Update / Rollback APP 에 대한 방법을 포함하고 있습니다.\nUpdate Source 아래와 같이 Source 를 UPDATE 가 되었습니다.\n신규 Build 및 Image Push 는 build app flask 를 확인하여 진행합니다.\ndiff --git a/main.py b/main.py index 2b9f022..9b347b4 100755 --- a/main.py +++ b/main.py @@ -8,7 +8,7 @@ podname = os.uname()[1]   @app.route(\u0026#34;/\u0026#34;)  def index(): - return \u0026#34; Container LAB | POD Working : \u0026#34; + podname + \u0026#34; | v=1\\n\u0026#34; + return \u0026#34; Container LAB | POD Working : \u0026#34; + podname + \u0026#34; | v=2\\n\u0026#34;   if __name__ == \u0026#34;__main__\u0026#34;:  app.run(host=\u0026#34;0.0.0.0\u0026#34;, port=\u0026#34;80\u0026#34;) 시간 관계상 Push 된 이미지를 사용할 것입니다.\n Container Image Tag : han0495/flask-example-app:v2  Rolling Update 아래 명령을 통해 기존에 v1 에서 v2 로 image 를 변경하여 Rolling Update 해보겠습니다.\n 기존 정보 $ kubectl describe deployments flask-example-app ... Pod Template:  Labels: app=flask-example-app  Containers:  flask-example-app:  Image: han0495/flask-example-app:v1 \u0026lt;\u0026lt; Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt;  Environment: \u0026lt;none\u0026gt;  Mounts: \u0026lt;none\u0026gt;  Volumes: \u0026lt;none\u0026gt;  Change image $ kubectl set image deployments flask-example-app flask-example-app=han0495/flask-example-app:v2  Rolling Update 상태 확인  상태 확인  $ kubectl rollout status deployments flask-example-app Waiting for deployment \u0026#34;flask-example-app\u0026#34; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026#34;flask-example-app\u0026#34; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026#34;flask-example-app\u0026#34; rollout to finish: 1 old replicas are pending termination... deployment \u0026#34;flask-example-app\u0026#34; successfully rolled out  실제 서비스 확인  $ while true; do curl 192.168.200.110:32296; done  Container LAB | POD Working : flask-example-app-959c5f88d-k95wk | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-b62j5 | v=1  Container LAB | POD Working : flask-example-app-677cbdd665-mhpqs | v=2  Container LAB | POD Working : flask-example-app-677cbdd665-wwmf8 | v=2  Container LAB | POD Working : flask-example-app-959c5f88d-b62j5 | v=1  Container LAB | POD Working : flask-example-app-677cbdd665-mhpqs | v=2  Container LAB | POD Working : flask-example-app-677cbdd665-wwmf8 | v=2  Container LAB | POD Working : flask-example-app-677cbdd665-mhpqs | v=2  Container LAB | POD Working : flask-example-app-959c5f88d-k95wk | v=1  ...  변경 확인 $ kubectl describe deployments flask-example-app Name: flask-example-app Namespace: default CreationTimestamp: Wed, 09 Sep 2020 15:37:59 +0900 Labels: app=flask-example-app Annotations: deployment.kubernetes.io/revision: 2 Selector: app=flask-example-app Replicas: 5 desired | 5 updated | 5 total | 5 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template:  Labels: app=flask-example-app  Containers:  flask-example-app:  Image: han0495/flask-example-app:v2 \u0026lt;\u0026lt;\u0026lt; 변경됨  Port: \u0026lt;none\u0026gt;  Host Port: \u0026lt;none\u0026gt;  Environment: \u0026lt;none\u0026gt;  Mounts: \u0026lt;none\u0026gt;  Volumes: \u0026lt;none\u0026gt; Conditions:  Type Status Reason  ---- ------ ------  Available True MinimumReplicasAvailable  Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: flask-example-app-677cbdd665 (5/5 replicas created) Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Normal ScalingReplicaSet 19m deployment-controller Scaled up replica set flask-example-app-959c5f88d to 5  Normal ScalingReplicaSet 3m31s deployment-controller Scaled up replica set flask-example-app-677cbdd665 to 2  Normal ScalingReplicaSet 3m31s deployment-controller Scaled down replica set flask-example-app-959c5f88d to 4  Normal ScalingReplicaSet 3m30s deployment-controller Scaled up replica set flask-example-app-677cbdd665 to 3  Normal ScalingReplicaSet 2m59s deployment-controller Scaled down replica set flask-example-app-959c5f88d to 3  Normal ScalingReplicaSet 2m58s deployment-controller Scaled up replica set flask-example-app-677cbdd665 to 4  Normal ScalingReplicaSet 2m56s deployment-controller Scaled down replica set flask-example-app-959c5f88d to 2  Normal ScalingReplicaSet 2m55s deployment-controller Scaled up replica set flask-example-app-677cbdd665 to 5  Normal ScalingReplicaSet 2m50s deployment-controller Scaled down replica set flask-example-app-959c5f88d to 1  Normal ScalingReplicaSet 2m3s deployment-controller Scaled down replica set flask-example-app-959c5f88d to 0   Rollback APP Rollback 은 배포된 APP 에 문제가 있을 때, 다시 이전 이미지로 배포 해야 되는 경우 사용하는 방법입니다.\n(꼭 문제가 있어야 사용이 가능한 것은 아님, 주 목적은 이전 버전으로 Rollback 하기 위함입니다.)\n먼저 아래 명령어를 통해 Rolling Update 를 다시 진행합니다.\n$ kubectl set image deployments flask-example-app flask-example-app=han0495/flask-example-app:v99 어떤 일이 발생될까요?\n$ kubectl get pod NAME READY STATUS RESTARTS AGE flask-example-app-677cbdd665-mhpqs 1/1 Running 0 8m33s flask-example-app-677cbdd665-wwmf8 1/1 Running 0 8m33s flask-example-app-677cbdd665-x95tn 1/1 Running 0 8m31s flask-example-app-677cbdd665-754fq 1/1 Running 0 7m59s flask-example-app-79f5875dcf-26pvs 0/1 ImagePullBackOff 0 38s flask-example-app-79f5875dcf-thxmx 0/1 ImagePullBackOff 0 38s flask-example-app-79f5875dcf-jmwpv 0/1 ErrImagePull 0 38s Image 에 v99 라는 TAG 는 없으므로 ImagePull 에서 Error 가 발생되면서 서비스 배포가 정상적으로 진행이 안됩니다.\n  Rollback 진행\n 현재 상태 확인  $ kubectl rollout history deployment flask-example-app deployment.apps/flask-example-app REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; 2 \u0026lt;none\u0026gt; 3 \u0026lt;none\u0026gt; \u0026lt; 현재 버전  Rollback 진행  $ kubectl rollout undo deployment flask-example-app deployment.apps/flask-example-app rolled back  Rollback 확인  $ kubectl describe deployments.apps flask-example-app Name: flask-example-app Namespace: default CreationTimestamp: Wed, 09 Sep 2020 15:37:59 +0900 Labels: app=flask-example-app Annotations: deployment.kubernetes.io/revision: 4 Selector: app=flask-example-app Replicas: 5 desired | 5 updated | 5 total | 5 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template:  Labels: app=flask-example-app  Containers:  flask-example-app:  Image: han0495/flask-example-app:v2 \u0026lt;\u0026lt; Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt;   v1 로 돌아가기\n Rollback 수행  $ kubectl rollout undo deployment flask-example-app --to-revision=1  서비스 확인  $ while true; do curl 192.168.200.110:32296; done  Container LAB | POD Working : flask-example-app-959c5f88d-rjbqc | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-sgrfk | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-sgrfk | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-rjbqc | v=1  Container LAB | POD Working : flask-example-app-677cbdd665-wwmf8 | v=2 \u0026lt;\u0026lt; Container LAB | POD Working : flask-example-app-959c5f88d-sgrfk | v=1 Container LAB | POD Working : flask-example-app-677cbdd665-wwmf8 | v=2 \u0026lt;\u0026lt; Container LAB | POD Working : flask-example-app-959c5f88d-rjbqc | v=1 Container LAB | POD Working : flask-example-app-959c5f88d-l7zjp | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-sgrfk | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-l7zjp | v=1  Rollback 완료  $ kubectl describe deployments.apps flask-example-app Name: flask-example-app Namespace: default CreationTimestamp: Wed, 09 Sep 2020 15:37:59 +0900 Labels: app=flask-example-app Annotations: deployment.kubernetes.io/revision: 5 Selector: app=flask-example-app Replicas: 5 desired | 5 updated | 5 total | 5 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template:  Labels: app=flask-example-app  Containers:  flask-example-app:  Image: han0495/flask-example-app:v1  Port: \u0026lt;none\u0026gt;  Host Port: \u0026lt;none\u0026gt;  Environment: \u0026lt;none\u0026gt;  Mounts: \u0026lt;none\u0026gt;  Volumes: \u0026lt;none\u0026gt;   참고 문서  https://github.com/chhanz/kubernetes-hands-on-lab https://github.com/chhanz/kubernetes-hands-on-lab/blob/master/doc/update-app.md  ","permalink":"https://chhanz88.github.io/post/2020-09-28-update-app/","summary":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Update APP 이 문서는 Rolling Update / Rollback APP 에 대한 방법을 포함하고 있습니다.\nUpdate Source 아래와 같이 Source 를 UPDATE 가 되었습니다.","title":"[Kubernetes] Update App (Rolling Update / Rollback)"},{"content":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Scale APP 이 문서는 Pod 을 Scale-out 하는 방법에 대해 포함되어 있습니다.\nScale 개요  Before  After\n  Command 를 이용하여 Scale-out 기존에 1개의 Pod 으로 실행중이던 APP 을 5개의 Pod 으로 Scale-out 하도록 하겠습니다.\n$ kubectl scale deployment --replicas=5 flask-example-app  Scale 확인 $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE flask-example-app 5/5 5 5 126m  Pod 확인 $ kubectl get pod NAME READY STATUS RESTARTS AGE flask-example-app-959c5f88d-k95wk 1/1 Running 0 127m flask-example-app-959c5f88d-rb4rv 1/1 Running 0 2m21s flask-example-app-959c5f88d-9kl5k 1/1 Running 0 2m21s flask-example-app-959c5f88d-b62j5 1/1 Running 0 2m21s flask-example-app-959c5f88d-fbjtl 1/1 Running 0 2m21s  서비스 확인(Round-Robin) $ while true; do curl 192.168.200.110:32296; done $ while true; do curl \u0026lt;TEST-SERVER-IP\u0026gt;:\u0026lt;NODEPORT\u0026gt;; done \u0026lt;\u0026lt; TEST 환경에 맞게 수정합니다. 아래와 같이 Scale-out 되어 서비스 중인 것을 볼 수 있습니다. $ while true; do curl 192.168.200.110:32296; done  Container LAB | POD Working : flask-example-app-959c5f88d-9kl5k | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-k95wk | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-b62j5 | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-9kl5k | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-b62j5 | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-rb4rv | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-9kl5k | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-b62j5 | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-9kl5k | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-b62j5 | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-k95wk | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-rb4rv | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-k95wk | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-rb4rv | v=1  Container LAB | POD Working : flask-example-app-959c5f88d-9kl5k | v=1 ...   edit 옵션을 이용하여 수정하는 방법 $ kubectl edit deployment flask-example-app ... spec:  progressDeadlineSeconds: 600  replicas: 5 \u0026lt;\u0026lt; replicas 수정  revisionHistoryLimit: 10  selector:  matchLabels:  app: flask-example-app ... 참고 문서  https://github.com/chhanz/kubernetes-hands-on-lab https://github.com/chhanz/kubernetes-hands-on-lab/blob/master/doc/scale-app.md  ","permalink":"https://chhanz88.github.io/post/2020-09-27-scale-app/","summary":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Scale APP 이 문서는 Pod 을 Scale-out 하는 방법에 대해 포함되어 있습니다.\nScale 개요  Before  After","title":"[Kubernetes] Scale App (kubectl scale)"},{"content":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Expose APP 이 문서는 Service 생성에 대해 포함된 문서입니다.\nService 란? Kubernetes Pod 들은 언젠가는 죽게됩니다. 실제 Pod 들은 생명주기를 갖습니다.\n워커 노드가 죽으면, 노드 상에서 동작하는 Pod 들 또한 종료됩니다.\nKubernetes 에서 service 는 Pod 들에 접근 할 수 있는 정책을 정의하는 추상적 개념입니다.\n ClusterIP (기본값)  클러스터 내에서 내부 IP 에 대해 서비스를 노출합니다. 이 방식은 클러스터 내에서만 서비스가 접근될 수 있도록 합니다.   NodePort  NAT가 이용되는 클러스터 내에서 각각 선택된 노드들의 동일한 포트에 서비스를 노출 시켜줍니다.\n\u0026lt;NodeIP\u0026gt;:\u0026lt;NodePort\u0026gt;를 이용하여 클러스터 외부로부터 서비스가 접근할 수 있도해줍니다. ClusterIP 의 상위 집합입니다.   LoadBalancer  (지원 가능한 경우) 기존 클라우드에서 외부용 로드밸런서를 생성하고 고정된 공인 IP를 할당합니다.\nNodePort 의 상위 집합입니다.   ExternalName  이름으로 CNAME 레코드를 반환함으로써 임의의 이름(Spec 에서 externalName 으로 명시)을 이용하여 서비스를 노출시켜줍니다.\n프록시는 사용되지 않습니다. 이 방식은 kube-dns 버전 1.7 이상에서 지원 가능합니다. 외부 서비스를 쿠버네티스 내부에서 호출 하고자 할때 사용할 수 있습니다.    Expose APP 아래 명령어를 통해 APP 을 외부에 노출 할 수 있습니다.\n현재 Hands on 환경은 LoadBalancer or Ingress 가 사용이 불가능하므로 Nodeport 를 이용하여 TEST 해보도록 하겠습니다.\n expose 옵션으로 생성 $ kubectl expose deployment flask-example-app --port 80  create옵션으로 생성 $ kubectl create service clusterip flask-example-app --tcp=80   위와 명령으로 생성하는 경우, 기본적으로 ClusterIP 로 생성이 됩니다.\n아래 명령을 통해 ClusterIP 를 NodePort 로 변경하도록 하겠습니다.\n$ kubectl edit service flask-example-app 해당 yaml 구문에서 spec 부분을 보면 아래와 같습니다.\n... spec:  clusterIP: 10.43.65.110  ports:  - port: 80  protocol: TCP  targetPort: 80  selector:  app: flask-example-app  sessionAffinity: None  type: ClusterIP \u0026lt;\u0026lt;\u0026lt; ... 위와 같이 ClusterIP 부분은 NodePort 로 변경하고 저장을 합니다.\n(저장 및 종료, VIM or VI 과 동일함.)\nNodePort 로 변경이 되었는지 확인합니다.\nkubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 99m flask-example-app NodePort 10.43.65.110 \u0026lt;none\u0026gt; 80:32296/TCP 3m26s 위와 같이 32296 로 Port 할당된 것을 확인 할 수 있습니다.\n(해당 Port 할당은 따로 yaml 에서 지정을 안하면 랜덤 Mapping 입니다.)\nTEST SERVICE $ curl 192.168.200.110:32296  Container LAB | POD Working : flask-example-app-959c5f88d-k95wk | v=1 참고 문서  https://github.com/chhanz/kubernetes-hands-on-lab https://github.com/chhanz/kubernetes-hands-on-lab/blob/master/doc/expose-app.md  ","permalink":"https://chhanz88.github.io/post/2020-09-25-expose-app/","summary":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Expose APP 이 문서는 Service 생성에 대해 포함된 문서입니다.\nService 란? Kubernetes Pod 들은 언젠가는 죽게됩니다.","title":"[Kubernetes] Expose App (Service 생성)"},{"content":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Check The Pod 이 문서는 Pod 를 확인하는 방법에 대해 포함되어 있습니다.\nKubernetes Pod 앞선 Create the Deployment 를 통해 Deployment 가 생성이 되고 나면\nKubernetes 는 여러분의 애플리케이션 인스턴스에 Pod 를 생성했습니다.\nPod 는 하나 또는 그 이상의 애플리케이션 컨테이너 (도커 또는 rkt와 같은)들의 그룹을 나타내는 쿠버네티스의 추상적 개념으로 일부는 컨테이너에 대한 자원을 공유합니다.\nCheck The Pod  Pod 정보 확인 $ kubectl get pod NAME READY STATUS RESTARTS AGE flask-example-app-959c5f88d-k95wk 1/1 Running 0 24m  Pod 성능 사용량 확인 $ kubectl top pod NAME CPU(cores) MEMORY(bytes) flask-example-app-959c5f88d-k95wk 1m 2Mi  Pod 내 Container 의 log 확인 $ kubectl logs flask-example-app-959c5f88d-k95wk 실시간 확인을 위해서는 아래와 같이 -f 옵션을 추가합니다. $ kubectl logs -f flask-example-app-959c5f88d-k95wk  Pod 내 Container 에 명령어 수행 $ kubectl exec -ti flask-example-app-959c5f88d-k95wk -- pwd /usr/src/app  Pod 상세 정보 확인 - 1 $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES flask-example-app-959c5f88d-k95wk 1/1 Running 0 24m 10.42.0.36 fastvm-centos-7-7-110 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  Pod 상세 정보 확인 - 2 $ kubectl describe pod flask-example-app-959c5f88d-k95wk Name: flask-example-app-959c5f88d-k95wk Namespace: default Priority: 0 Node: fastvm-centos-7-7-110/192.168.200.110 Start Time: Wed, 09 Sep 2020 15:38:00 +0900 Labels: app=flask-example-app  pod-template-hash=959c5f88d Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.42.0.36 IPs:  IP: 10.42.0.36 Controlled By: ReplicaSet/flask-example-app-959c5f88d Containers:  flask-example-app:  Container ID: containerd://f99d19b1ac52ced19e300a294cd93ad62a97f6259cf6e426c63c545e0918a9d8  Image: han0495/flask-example-app:v1  Image ID: docker.io/han0495/flask-example-app@sha256:f3d45e996bc86a13e1a8a363d9736e18c1501804690733b86aa02df3f59bda10  Port: \u0026lt;none\u0026gt;  Host Port: \u0026lt;none\u0026gt;  State: Running  Started: Wed, 09 Sep 2020 15:38:01 +0900  Ready: True  Restart Count: 0  Environment: \u0026lt;none\u0026gt;  Mounts:  /var/run/secrets/kubernetes.io/serviceaccount from default-token-89vwb (ro) Conditions:  Type Status  Initialized True  Ready True  ContainersReady True  PodScheduled True Volumes:  default-token-89vwb:  Type: Secret (a volume populated by a Secret)  SecretName: default-token-89vwb  Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s  node.kubernetes.io/unreachable:NoExecute for 300s Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Normal Scheduled \u0026lt;unknown\u0026gt; default-scheduler Successfully assigned default/flask-example-app-959c5f88d-k95wk to fastvm-centos-7-7-110  Normal Pulled 29m kubelet, fastvm-centos-7-7-110 Container image \u0026#34;han0495/flask-example-app:v1\u0026#34; already present on machine  Normal Created 29m kubelet, fastvm-centos-7-7-110 Created container flask-example-app  Normal Started 29m kubelet, fastvm-centos-7-7-110 Started container flask-example-app  현재 실행중인 Pod 의 정보를 yaml 형식으로 출력 $ kubectl get pod flask-example-app-959c5f88d-k95wk -o yaml 아래와 같이 저장이 가능합니다. $ kubectl get pod flask-example-app-959c5f88d-k95wk -o yaml \u0026gt; flask-example-app.yml   Create Pod 아래 명령어를 통해 Pod 을 생성 할 수 있습니다.\n$ kubectl run pod-test-app --image=nginx pod/pod-test-app created  $ kubectl get pod NAME READY STATUS RESTARTS AGE flask-example-app-959c5f88d-k95wk 1/1 Running 0 38m pod-test-app 1/1 Running 0 66s \u0026lt;\u0026lt; 아래와 같이 yaml 을 이용해서 Pod 을 생성 할 수 있습니다.\n$ cat \u0026lt;\u0026lt; EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: labels: run: pod-test-app-2 name: pod-test-app-2 namespace: default spec: containers: - image: nginx name: pod-test-app-2 EOF or\n$ kubectl create -f pod-test-app-2.yml Delete Pod 아래 명령을 통해 Pod 삭제 할 수 있습니다.\n$ kubectl delete pod pod-test-app $ kubectl delete pod pod-test-app-2 연습 문제 아래 container 를 포함한 하나의 Pod 을 생성하시오.\n nginx redis memcached  연습문제 정답 apiVersion: v1 kind: Pod metadata:  labels:  run: pod-practice-app  name: pod-practice-app spec:  containers:  - image: nginx  name: nginx  - image: redis  name: redis  - image: memcached  name: memcached 참고 문서  https://github.com/chhanz/kubernetes-hands-on-lab https://github.com/chhanz/kubernetes-hands-on-lab/blob/master/doc/check-the-pod.md  ","permalink":"https://chhanz88.github.io/post/2020-09-24-check-the-app/","summary":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Check The Pod 이 문서는 Pod 를 확인하는 방법에 대해 포함되어 있습니다.\nKubernetes Pod 앞선 Create the Deployment 를 통해 Deployment 가 생성이 되고 나면","title":"[Kubernetes] App 확인 (Pod 확인)"},{"content":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Check The Node 이 문서는 Kubernetes Node 를 확인하는 방법에 대해 포함되어 있습니다.\nKubernetes Node Kubernetes Node 는 최소한 다음과 같이 동작합니다.\n Kubelet은, 쿠버네티스 마스터와 노드 간 통신을 책임지는 프로세스이며, 하나의 머신 상에서 동작하는 파드와 컨테이너를 관리합니다. (도커, rkt)와 같은 컨테이너 런타임은 레지스트리에서 컨테이너 이미지를 가져와 묶여 있는 것을 풀고 애플리케이션을 동작시키는 책임을 맡습니다.  Check Kubernetes Node  Node 정보 확인 $ kubectl get node NAME STATUS ROLES AGE VERSION fastvm-centos-7-7-110 Ready master 16d v1.18.8+k3s1  Node 상세 정보 확인 $ kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME fastvm-centos-7-7-110 Ready master 16d v1.18.8+k3s1 192.168.200.110 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 containerd://1.3.3-k3s2  Node 성능 사용량 확인 $ kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% fastvm-centos-7-7-110 127m 6% 573Mi 66%   Node Management  Node Schedule 관련 $ kubectl --help ...  cordon Mark node as unschedulable  uncordon Mark node as schedulable  drain Drain node in preparation for maintenance ...  Mark node as unschedulable $ kubectl cordon \u0026lt;node\u0026gt;  Mark node as schedulable $ kubectl uncordon \u0026lt;node\u0026gt;  Drain node in preparation for maintenance $ kubectl drain \u0026lt;node\u0026gt;   참고 문서  https://github.com/chhanz/kubernetes-hands-on-lab https://github.com/chhanz/kubernetes-hands-on-lab/blob/master/doc/check-the-node.md  ","permalink":"https://chhanz88.github.io/post/2020-09-24-check-the-node/","summary":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Check The Node 이 문서는 Kubernetes Node 를 확인하는 방법에 대해 포함되어 있습니다.\nKubernetes Node Kubernetes Node 는 최소한 다음과 같이 동작합니다.","title":"[Kubernetes] Node 상태 확인"},{"content":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Create the Deployment 이 문서는 Kubernetes 의 Deployment 를 생성하는 방법에 대해 포함되어 있습니다.\nDeployment 란? Deployment는 Kubernetes 가 애플리케이션의 인스턴스를 어떻게 생성하고 업데이트해야 하는지를 지시합니다.\nDeployment가 만들어지면, Kubernetes Master 가 해당 Deployment 에 포함된 애플리케이션 인스턴스가 클러스터의 개별 노드에서 실행되도록 스케줄합니다.\nCreate Deployment 아래 명령어와 같이 수행합니다.\n$ kubectl create deployment --image=han0495/flask-example-app:v1 flask-example-app Check Deployment Deployment 가 생성되고 Pod 가 정상적으로 실행중인지 확인합니다.\n$ kubectl get all NAME READY STATUS RESTARTS AGE pod/flask-example-app-959c5f88d-k95wk 1/1 Running 0 4s  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 5m37s  NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/flask-example-app 1/1 1 1 5s  NAME DESIRED CURRENT READY AGE replicaset.apps/flask-example-app-959c5f88d 1 1 1 5s Detail Check Deployment 실행중인 Deployment 를 자세히 확인 해보겠습니다.\n$ kubectl describe deployment flask-example-app Name: flask-example-app Namespace: default CreationTimestamp: Wed, 09 Sep 2020 15:37:59 +0900 Labels: app=flask-example-app Annotations: deployment.kubernetes.io/revision: 1 Selector: app=flask-example-app Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template:  Labels: app=flask-example-app  Containers:  flask-example-app:  Image: han0495/flask-example-app:v1  Port: \u0026lt;none\u0026gt;  Host Port: \u0026lt;none\u0026gt;  Environment: \u0026lt;none\u0026gt;  Mounts: \u0026lt;none\u0026gt;  Volumes: \u0026lt;none\u0026gt; Conditions:  Type Status Reason  ---- ------ ------  Available True MinimumReplicasAvailable  Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: flask-example-app-959c5f88d (1/1 replicas created) Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Normal ScalingReplicaSet 7m21s deployment-controller Scaled up replica set flask-example-app-959c5f88d to 1 참고 문서  https://kubernetes.io/ko/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/ https://github.com/chhanz/kubernetes-hands-on-lab https://github.com/chhanz/kubernetes-hands-on-lab/blob/master/doc/create-the-deployment.md  ","permalink":"https://chhanz88.github.io/post/2020-09-23-create-deployment/","summary":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Create the Deployment 이 문서는 Kubernetes 의 Deployment 를 생성하는 방법에 대해 포함되어 있습니다.\nDeployment 란?","title":"[Kubernetes] Deployment 생성"},{"content":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Build APP for Flask WEBAPP 이 문서는 Kubernetes Hands on LAB 에서 활용할 Flask WEBAPP 를 Build 하는 방법에 대해 포함 되어 있습니다.\nInstall git $ yum -y install git Clone source $ git clone https://github.com/chhanz/flask-example-app.git Check Dockerfile 해당 Source 는 아래와 같은 file 을 포함하고 있습니다.\nflask-example-app/ ├── Dockerfile \u0026gt; Build 를 위한 Dockerfile ├── main.py \u0026gt; Flask WEBAPP ├── README.md └── requirements.txt \u0026gt; Requirements Flask APP Dockerfile 는 아래와 같습니다.\nFROMpython:3.8.5-alpine3.12MAINTAINERcheolhee.han@ibm.comWORKDIR/usr/src/appCOPY . .RUN pip install --no-cache-dir -r requirements.txtCMD [ \u0026#34;python\u0026#34;, \u0026#34;./main.py\u0026#34; ]Build APP 아래 명령어를 통해 Build 를 수행합니다.\n  buildah 의 경우,\n$ buildah bud -t flask-test-app:v1 .   podman 의 경우,\n$ podman build -t flask-test-app:v1 .   TEST APP 아래 명령어를 통해 Container 를 시작합니다.\n$ podman run -d -p 39999:80 --name test flask-test-app:v1 $ curl 127.0.0.1:39999 Push image DockerHub 에 image 를 Push 하기위해 아래와 같이 Login 을 합니다.\n$ buildah login docker.io Username: \u0026lt; dockerhub id \u0026gt; Password: \u0026lt; dockerhub pw \u0026gt; Login Succeeded! Push 할 image 의 TAG 를 수정합니다.\n$ buildah tag flask-test-app:v1 docker.io/\u0026lt;dockerhub id\u0026gt;/flask-test-app:v1 아래 명령어를 통해 PUSH 를 합니다.\n$ buildah push docker.io/\u0026lt;dockerhub id\u0026gt;/flask-test-app:v1 Push 가 완료되면 Dockerhub 로 접속해서 확인합니다.\n마무리 다음 LAB 위해 Podman Container 를 종료합니다.\n$ podman rm -f $(podman ps -aq) 참고 문서  https://github.com/chhanz/flask-example-app  ","permalink":"https://chhanz88.github.io/post/2020-09-22-podman-build-flask-example-app/","summary":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Build APP for Flask WEBAPP 이 문서는 Kubernetes Hands on LAB 에서 활용할 Flask WEBAPP 를 Build 하는 방법에 대해 포함 되어 있습니다.","title":"[Container] podman 을 이용한 container image build"},{"content":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Build APP 이 문서는 Application 을 Build 하는 방법에 대해 설명하고 있습니다.\nBuildah 란? Buildah 는 Podman 의 Container image Build 및 push 등을 지원하는 Tool 로 podman build 을 사용할 경우,\n내부적으로 Buildah 를 이용하여 Container image 를 Build 하게 됩니다.\n또한 Buildah 를 이용하면 layer 별로 순차적으로 Build 를 수행 할 수 있습니다.\nBuildah 설치  CentOS 7 $ yum install -y buildah  CentOS 8 $ dnf install -y buildah   Podman 설치  CentOS 7 $ yum install -y podman  CentOS 8 $ dnf install -y podman   Build APP Build 과정을 이해하기 위해 간단한 APP 을 Container image 로 build 해보겠습니다.\n$ buildah from centos:7 $ buildah list \u0026lt; CONTAINER ID 확인 CONTAINER ID BUILDER IMAGE ID IMAGE NAME CONTAINER NAME bff73bfb1adc * 7e6257c9f8d8 docker.io/library/centos:7 centos-working-container $ buildah run bff73bfb1adc -- yum -y install epel-release $ buildah run bff73bfb1adc -- yum -y install sl $ buildah commit bff73bfb1adc sl-test-app RUN APP Build 된 APP 을 실행해보겠습니다.\n$ podman run sl-test-app /usr/bin/sl 화면에 무엇이 보이나요?\n마치며 위와 같이 간단하게 Container image 를 만들어보고 어떻게 image 가 만들어 지는지 확인 했습니다.\n참고 자료  Dockerfile reference https://github.com/chhanz/kubernetes-hands-on-lab/blob/master/doc/buildah.md  ","permalink":"https://chhanz88.github.io/post/2020-09-21-buildah/","summary":"목차  Build APP - Container image Deploy APP  Build APP Create The Deployment   Check APP  Check The Node Check The APP   Expose APP  Expose APP - NodePort   Scale APP  Scale APP   Update APP  Update APP - Rolling Update/Rollback    Build APP 이 문서는 Application 을 Build 하는 방법에 대해 설명하고 있습니다.\nBuildah 란? Buildah 는 Podman 의 Container image Build 및 push 등을 지원하는 Tool 로 podman build 을 사용할 경우,","title":"[Container] Container image build with buildah (podman builder)"},{"content":"Ansible 을 이용하여 k3s 배포해보자 Ansible.fast-k3s 란? 저는 평소엔 사내 교육이나 테스트를 위해 주로 Minikube 를 이용하였습니다.\nMinikube 도 Linux laptop 인지 Windows (version) laptop 인지 Mac 인지 확인해서 설치를 가이드 해야되는 불편함이 있었습니다.\n그래서 VMware (vCenter) 에 생성되어 있는 Template 를 이용하여 자동으로 Kubernetes 테스트 환경을 만드는 Playbook 을 만들었습니다.\nRequirements  현재는 VMware 만 VM Provisioning 하도록 만들어져 있습니다. 향후에 필요하다면 KVM / RHV / oVirt 등 개발 예정입니다.\n미지원하는 Hypervisor 는 k3s Tag 를 이용해서 k3s 만 배포하면 됩니다.\n  vCenter 6.7 (Tested) CentOS 7.7 Minimal Install Template Static IP Ansible \u0026gt;= 2.9.6 (Tested) https://github.com/chhanz/ansible.fast-k3s  Playbook 사용 방법   Ansible 로 VMware 통제하기 위해서는 아래와 같이 Python module 을 설치 해야됩니다.\n pyvmomi \u0026gt;= 7.0  $ pip install -r requirements.txt   git clone 및 variable 수정\n  $ git clone https://github.com/chhanz/ansible.fast-k3s.git $ cd ansible.fast-k3s $ vi vm_information.yml  --- ### variable information \u0026#39;vcenter\u0026#39; vcenter_server: \u0026#34;vc.changeme.com\u0026#34; vcenter_user: \u0026#34;Administrator@vsphere.local\u0026#34; vcenter_pass: \u0026#34;password\u0026#34; datacenter_name: \u0026#34;Datacenter\u0026#34; cluster_name: \u0026#34;Cluster\u0026#34; datastore_name: \u0026#34;Datastore-1\u0026#34; vm_network: \u0026#34;VM Network\u0026#34;  ### variable \u0026#39;virtual machine\u0026#34; vm_template: \u0026#34;c7.7-template\u0026#34; vm_netmask: \u0026#34;255.255.255.0\u0026#34; vm_gateway: \u0026#34;192.168.0.1\u0026#34; vm_dns: \u0026#34;1.1.1.1\u0026#34; 위와 같이 vm_information.yml 을 VM 을 배포할 vCenter 정보를 수정하고. 배포에 사용될 VM Template, VM 에 사용될 IP 정보를 입력합니다.\n Playbook 실행  deploy lab environment \u0026lsquo;k3s\u0026rsquo;  $ ansible-playbook -i inventory deploy-k3s.yml VM Provisioning 과 k3s 배포를 위한 Playbook.  (option) only deploy vm  $ ansible-playbook -i inventory deploy-k3s.yml -t vm VM Provisioning 배포를 위한 Playbook.  (option) only install k3s  $ ansible-playbook -i inventory deploy-k3s.yml -t k3s k3s 배포를 위한 Playbook.  배포 예제   Playbook 수행 결과\n  VM 생성 결과\n  k3s 배포 결과   [root@k3s-10-50-1-42 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k3s-10-50-1-42 Ready master 9m29s v1.18.8+k3s1 [root@k3s-10-50-1-42 ~]# Known Issue  k3s 는 공식적으로 Ubuntu 만 지원하는 것으로 되어 있습니다.\n하지만 아래와 같이 CentOS/RHEL 계열 추가 작업이 진행중이며, 설치는 가능하나 firewalld 및 SELinux 에 대해 검토 진행중인 것으로 보입니다.\n(https://github.com/rancher/k3s/issues/1371) vmware_guest module 을 이용하여 Ubuntu 를 배포하면 Static IP 설정을 못하는 이슈가 있습니다.\n이는 open-vm-tools 에서 아직 netplan 을 이용한 네트워크 설정이 아직 미지원이라 그렇습니다.\n(https://docs.ansible.com/ansible/latest/scenario_guides/vmware_scenarios/vmware_troubleshooting.html)  참고 문서  https://docs.ansible.com/ansible/latest/modules/vmware_guest_module.html https://rancher.com/docs/k3s/latest/en/installation/ https://github.com/chhanz/ansible.fast-k3s  ","permalink":"https://chhanz88.github.io/post/2020-09-05-fast-k3s-use-ansible/","summary":"Ansible 을 이용하여 k3s 배포해보자 Ansible.fast-k3s 란? 저는 평소엔 사내 교육이나 테스트를 위해 주로 Minikube 를 이용하였습니다.\nMinikube 도 Linux laptop 인지 Windows (version) laptop 인지 Mac 인지 확인해서 설치를 가이드 해야되는 불편함이 있었습니다.\n그래서 VMware (vCenter) 에 생성되어 있는 Template 를 이용하여 자동으로 Kubernetes 테스트 환경을 만드는 Playbook 을 만들었습니다.\nRequirements  현재는 VMware 만 VM Provisioning 하도록 만들어져 있습니다. 향후에 필요하다면 KVM / RHV / oVirt 등 개발 예정입니다.","title":"[Kubernetes] Ansible 으로 k3s 배포하기 (on VMware)"},{"content":"[Linux] ReaR 를 이용하여 OS Backup 구성 ReaR 란? ReaR 또는 Relax \u0026amp; Recover는 마이그레이션 및 재해 복구 도구입니다. ReaR는 실행중인 Linux 시스템에 대한 부팅 가능한 이미지를 생성하며 필요한 경우에는 백업된 이미지를 사용하여 시스템을 복구 할 수 있습니다. 백업된 이미지를 사용하여 OS를 다른 하드웨어로 복원 할 수도 있으므로 ReaR를 마이그레이션 도구로 사용할 수도 있습니다.\nReaR Backup 용 NFS 서버 구성 ReaR 로 Backup 되는 Boot ISO 및 Backup DATA 를 저장할 NFS 서버를 구성합니다.\n(NAS 등 기타 네트워크 저장 공간이 있다면 활용이 가능합니다.)\n$ yum install nfs-utils  $ mkdir /data $ cat /etc/exports /data *(rw,sync,no_root_squash)  $ systemctl enable --now nfs-server $ exports -v NFS 서버 상세 구성은 아래 문서를 참고합니다.\nhttps://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/nfs-serverconfig\nReaR 설치 아래와 같이 설치를 진행합니다.\n$ yum install rear ReaR 설정 아래와 같이 /etc/rear/local.conf 파일을 수정합니다.\n$ cat /etc/rear/local.conf # Default is to create Relax-and-Recover rescue media as ISO image # set OUTPUT to change that # set BACKUP to activate an automated (backup and) restore of your data # Possible configuration values can be found in /usr/share/rear/conf/default.conf # # This file (local.conf) is intended for manual configuration. For configuration # through packages and other automated means we recommend creating a new # file named site.conf next to this file and to leave the local.conf as it is. # Our packages will never ship with a site.conf.  OUTPUT=ISO OUTPUT_URL=nfs://10.10.10.1/data BACKUP=NETFS BACKUP_URL=nfs://10.10.10.1/data 세부 내용 설명  OUTPUT : BOOT 용 ISO 이미지 생성 OUTPUT_URL : 생성된 BOOT ISO 이미지 저장 위치 BACKUP : BACKUP DATA 를 저장 방식을 지정합니다. (NETFS 는 Network Filesystem 을 말합니다. 즉, NFS) BACKUP_URL : BACKUP DATA 가 저장될 NFS 정보를 입력합니다.   추가 OPTION 은 아래 문서 참고 합니다.\nhttps://github.com/rear/rear/blob/master/doc/user-guide/03-configuration.adoc\n BACKUP 진행 아래와 같이 명령을 통해 BACKUP 을 수행합니다.\n$ rear -d -v mkbackup 복구 방법 저장된 ISO 를 이용하여 CD BOOT 진행합니다.\nCD BOOT 이 완료가 되면 위와 같이 복구를 위한 prompt 가 BOOT 됩니다. (OPTION) 네트워크가 STATIC 환경이라면 위와 같이 IP 를 설정합니다. $ rear -v recover 명령을 통해 위와 같이 복구를 수행합니다. 복구가 완료되고 재부팅을 하면 복구된 서버에 접근이 가능합니다.\n참고 문서  https://access.redhat.com/solutions/2115051  ","permalink":"https://chhanz88.github.io/post/2020-08-12-linux-backup-rear/","summary":"[Linux] ReaR 를 이용하여 OS Backup 구성 ReaR 란? ReaR 또는 Relax \u0026amp; Recover는 마이그레이션 및 재해 복구 도구입니다. ReaR는 실행중인 Linux 시스템에 대한 부팅 가능한 이미지를 생성하며 필요한 경우에는 백업된 이미지를 사용하여 시스템을 복구 할 수 있습니다. 백업된 이미지를 사용하여 OS를 다른 하드웨어로 복원 할 수도 있으므로 ReaR를 마이그레이션 도구로 사용할 수도 있습니다.\nReaR Backup 용 NFS 서버 구성 ReaR 로 Backup 되는 Boot ISO 및 Backup DATA 를 저장할 NFS 서버를 구성합니다.","title":"[Linux] ReaR 를 이용하여 OS Backup 구성"},{"content":"[Linux] How to reposync only latest package reposync 를 받는데 모든 version 의 package 들이 있다보니 reposync 할 때, 시간도 오래 걸리고 용량도 큰 문제가 있었습니다.\n[root@fastvm-r76-34 ~]# yum repolist Loaded plugins: product-id, search-disabled-repos, subscription-manager repo id repo namestatus !rhel-7-server-rpms/7Server/x86_64 Red Hat Enterprise Linux 7 Server (RPMs) 29,237 ... 위와 같이 29237 개의 package 를 가지고 있습니다.\n아래와 같이 하면 최신 버전의 package 만 sync 할 수 있습니다.\n$ reposync -n -r rhel-7-server-rpms (1/5475): 389-ds-base-1.3.10.1-14.el7_8.x86_64.rpm | 1.7 MB 00:00:02 (2/5475): 389-ds-base-libs-1.3.10.1-14.el7_8.x86_64.rpm | 711 kB 00:00:02 (3/5475): ElectricFence-2.2.2-39.el7.i686.rpm | 35 kB 00:00:00 (4/5475): GConf2-3.2.6-8.el7.i686.rpm | 1.0 MB 00:00:00 (5/5475): GConf2-3.2.6-8.el7.x86_64.rpm | 1.0 MB 00:00:00 ... 무려 24000여개의 package 가 제외 되었습니다.\n최신 버전의 package 만 필요하다면 위와 같은 방법으로 sync 받으면 됩니다.\n참고 자료  https://access.redhat.com/discussions/2906821  ","permalink":"https://chhanz88.github.io/post/2020-07-30-linux-reposync/","summary":"[Linux] How to reposync only latest package reposync 를 받는데 모든 version 의 package 들이 있다보니 reposync 할 때, 시간도 오래 걸리고 용량도 큰 문제가 있었습니다.\n[root@fastvm-r76-34 ~]# yum repolist Loaded plugins: product-id, search-disabled-repos, subscription-manager repo id repo namestatus !rhel-7-server-rpms/7Server/x86_64 Red Hat Enterprise Linux 7 Server (RPMs) 29,237 ... 위와 같이 29237 개의 package 를 가지고 있습니다.\n아래와 같이 하면 최신 버전의 package 만 sync 할 수 있습니다.\n$ reposync -n -r rhel-7-server-rpms (1/5475): 389-ds-base-1.","title":"[Linux] How to reposync only latest package"},{"content":"CentOS6 에 OPENSSL 최신버전 설치 CentOS 6 에서는 openssl 1.0.1e-58.el6_10 까지만 지원하고 (Repository 에서 제공하는 Version)\nhttps://www.openssl.org 에서 제공하는 버전은 openssl-1.1.1g 입니다.\nCentOS 6 에서는 Source 설치를 해야지만 최신 버전 사용이 가능한 것입니다.\nRequirement package 아래와 같이 필수 패키지를 설치합니다.\n$ yum install gcc make gcc-c++ perl perl-Test-Harness perl-Test-Simple zlib-devel Source 설치 설치 방법은 간단합니다.\n$ wget https://www.openssl.org/source/openssl-1.1.1g.tar.gz $ cd openssl-1.1.1g $ ./config $ make $ make test $ make install 하지만 위와 같이 하면 make test 부분에서 ERROR 가 발생합니다.\n Test::More version 0.96 required\n 위와 같은 ERROR 이 발생하는데 CentOS6 에서 지원하는 perl 의 version 이 낮아서 그렇습니다.\nperl source 설치 아래와 같이 perl 을 source 설치합니다.\n$ wget https://www.cpan.org/src/5.0/perl-5.32.0.tar.gz $ tar -xzf perl-5.32.0.tar.gz $ cd perl-5.32.0 $ ./Configure -des -Dprefix=/usr/local/perl-5.32 $ make $ make test $ make install 섫치가 완료되면 기존 perl 실행 파일을 bakcup 받고 최신 버전 perl 로 PATH 설정을 합니다.\n$ mv /usr/bin/perl perl_5_10 $ ln -s /usr/local/localperl/bin/perl perl 아래와 같이 최신 버전 perl 을 확인 할 수 있습니다.\n[root@fastvm-centos-6-10-108 /]# perl -v  This is perl 5, version 32, subversion 0 (v5.32.0) built for x86_64-linux  Copyright 1987-2020, Larry Wall  Perl may be copied only under the terms of either the Artistic License or the GNU General Public License, which may be found in the Perl 5 source kit.  Complete documentation for Perl, including FAQ lists, should be found on this system using \u0026#34;man perl\u0026#34; or \u0026#34;perldoc perl\u0026#34;. If you have access to the Internet, point your browser at http://www.perl.org/, the Perl Home Page.  [root@fastvm-centos-6-10-108 /]# perl 설치 이후 재시도 라이브러리 PATH 설정을 합니다.\n$ cd /etc/ld.so.conf.d/ $ cat openssl-1.1.1g.conf /usr/local/ssl/lib  $ ldconfig -v openssl version 을 확인합니다.\n$ cd /usr/local/ssl/bin $ ./openssl version OpenSSL 1.1.1g 21 Apr 2020 PATH 를 변경합니다.\n$ ln -s /usr/local/ssl/bin/openssl /usr/bin/openssl $ ls -l $(which openssl) lrwxrwxrwx. 1 root root 26 Jul 21 23:01 /usr/bin/openssl -\u0026gt; /usr/local/ssl/bin/openssl 최종 확인합니다.\n[root@fastvm-centos-6-10-108 ~]# openssl version OpenSSL 1.1.1g 21 Apr 2020 [root@fastvm-centos-6-10-108 ~]# 설치가 완료는 되었으나, 개인적으론 CentOS 7 을 사용하는게 정신 건강에 좋을 것 같습니다. ㅎㅎㅎㅎ\n","permalink":"https://chhanz88.github.io/post/2020-07-21-centos6-install-latest-openssl/","summary":"CentOS6 에 OPENSSL 최신버전 설치 CentOS 6 에서는 openssl 1.0.1e-58.el6_10 까지만 지원하고 (Repository 에서 제공하는 Version)\nhttps://www.openssl.org 에서 제공하는 버전은 openssl-1.1.1g 입니다.\nCentOS 6 에서는 Source 설치를 해야지만 최신 버전 사용이 가능한 것입니다.\nRequirement package 아래와 같이 필수 패키지를 설치합니다.\n$ yum install gcc make gcc-c++ perl perl-Test-Harness perl-Test-Simple zlib-devel Source 설치 설치 방법은 간단합니다.\n$ wget https://www.openssl.org/source/openssl-1.1.1g.tar.gz $ cd openssl-1.1.1g $ ./config $ make $ make test $ make install 하지만 위와 같이 하면 make test 부분에서 ERROR 가 발생합니다.","title":"[Linux] CentOS6 OPENSSL 최신버전 설치"},{"content":"Python 으로 간단히 웹서비스를 구동해보자. Local PC 에서 jekyll 과 같이 내가 만든 정적 웹서비스를 테스트 할 수 있는 방법이 없을까 궁리하다가 찾은 방법입니다.\n실행 Web Source 가 있는 위치로 이동하여 아래와 같이 명령을 입력합니다.\n$ pwd /var/www/html/  $ cat index.html HELLO!!!  $ python -m SimpleHTTPServer Serving HTTP on 0.0.0.0 port 8000 ... 10.10.10.10 - - [20/Jan/2017 16:44:01] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - 10.10.10.10 - - [20/Jan/2017 16:46:22] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - 위와 같이 실행이 되면 http://127.0.0.1:8000 으로 웹서비스를 기동하고 테스트 할 수 있습니다.\n","permalink":"https://chhanz88.github.io/post/2020-07-05-python-simplehttpserver/","summary":"Python 으로 간단히 웹서비스를 구동해보자. Local PC 에서 jekyll 과 같이 내가 만든 정적 웹서비스를 테스트 할 수 있는 방법이 없을까 궁리하다가 찾은 방법입니다.\n실행 Web Source 가 있는 위치로 이동하여 아래와 같이 명령을 입력합니다.\n$ pwd /var/www/html/  $ cat index.html HELLO!!!  $ python -m SimpleHTTPServer Serving HTTP on 0.0.0.0 port 8000 ... 10.10.10.10 - - [20/Jan/2017 16:44:01] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - 10.10.10.10 - - [20/Jan/2017 16:46:22] \u0026#34;GET / HTTP/1.","title":"[Python] Python 으로 간단히 웹서비스 구동"},{"content":"CodeReady Workspace 란? Red Hat® CodeReady Workspaces는 팀을 위한 클라우드 네이티브 개발을 실용적으로 만들어주는 개발자 툴입니다.\n쿠버네티스와 컨테이너를 사용해 개발 또는 IT 팀의 누구에게든 일관적으로 사전 설정된 개발 환경을 제공합니다.\n개발자는 Red Hat OpenShift®에서 구동되는 컨테이너에서 코드를 작성하고, 빌드하고, 테스트할 수 있습니다.\n사용자 경험 또한 노트북에서 통합 개발 환경(IDE)을 사용하는 것만큼이나 빠르고 친숙합니다.\n https://www.redhat.com/ko/technologies/jboss-middleware/codeready-workspaces  CodeReady Workspace 배포 CodeReady 배포 할 Project 생성합니다.\nadmin 계정으로 로그인 후, OperatorHub 에서 CodeReady 를 검색합니다.\nInstall 을 누르고 설치를 시작합니다.\nNamespace 를 선택하고 Subscribe 를 선택합니다.\nInstalled Operators 에서 CodeReady 의 Status 가 Succeeded 가 될 때까지 기다립니다.\n위와 같이 install strategy completed with no errors Event 메뉴에서 메시지가 나올때까지 대기합니다.\nInstalled Operators 에서 CodeReady 를 선택 후 Create Instance 를 선택합니다.\n수정이 필요할 경우, 수정 후 Create 를 선택 합니다.\n위와 같이 CheCluster 생성이 된 것을 볼 수 있습니다.\nCheCluster 에서 Pod 들이 순차적으로 배포가 되고 실행이 완료되면 CodeReady 에 접근이 가능해집니다.\nCodeReady Workspace URL 을 통해 접근합니다.\nCodeReady Workspace 을 이용해서 개발해보자 초기 접속을 하면 사용할 계정을 생성합니다.\nSample 을 선택합니다.\n테스트를 위해 Python 을 선택하도록 하겠습니다.\n위와 같이 Sample Python code 가 있고, IDE 환경이 실행됩니다.\nDebug 메뉴를 통해 debug 할 수 있습니다.\n하단에 Debug Console 에서 debug 된 내용을 확인 할 수 있습니다.\nTerminal 에서 Run Task 를 선택하고 개발된 Code 를 Run 할 수 있습니다.\n하단에 RUN 내용이 출력됩니다.\nSample Code marks = [90, 25 ,67, 45, 80] number = 0  for mark in marks:  number = number + 1  if mark \u0026gt;= 60:  print(\u0026#34;%dis pass.\u0026#34; % number)  else:  print(\u0026#34;%dis fail.\u0026#34; % number) 위 Source 를 이용해서 간단하게 Python 프로그래밍을 해보았습니다.\n별도의 개발 환경 구축 없이 간편하게 WEB Console 을 통해 개발 할 수 있었습니다.\n참고 자료  https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.1/html/installation_guide/installing-codeready-workspaces-on-ocp-4#creating-the-codeready-workspaces-project-in-openshift-4-web-console_installing-codeready-workspaces-on-openshift-4-from-operatorhub CodeReady 배포시 TLS 에러로 인해 배포가 안 되는 경우,  https://access.redhat.com/solutions/4675271   https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.1/html/end-user_guide/navigating-codeready-workspaces-using-the-dashboard_crw https://www.eclipse.org/che/  ","permalink":"https://chhanz88.github.io/post/2020-07-01-ocp-codeready/","summary":"CodeReady Workspace 란? Red Hat® CodeReady Workspaces는 팀을 위한 클라우드 네이티브 개발을 실용적으로 만들어주는 개발자 툴입니다.\n쿠버네티스와 컨테이너를 사용해 개발 또는 IT 팀의 누구에게든 일관적으로 사전 설정된 개발 환경을 제공합니다.\n개발자는 Red Hat OpenShift®에서 구동되는 컨테이너에서 코드를 작성하고, 빌드하고, 테스트할 수 있습니다.\n사용자 경험 또한 노트북에서 통합 개발 환경(IDE)을 사용하는 것만큼이나 빠르고 친숙합니다.\n https://www.redhat.com/ko/technologies/jboss-middleware/codeready-workspaces  CodeReady Workspace 배포 CodeReady 배포 할 Project 생성합니다.\nadmin 계정으로 로그인 후, OperatorHub 에서 CodeReady 를 검색합니다.","title":"[OpenShift 4.4] CodeReady Workspaces 를 이용하여 통합 개발 환경(IDE) 구현"},{"content":" 해당 자료는 사내 교육용으로 제작된 자료입니다.\n자료 사용시 출처 부탁 드려요.\n 목차  Deploying Applications From Images Deploying Applications From Source Deploying Applications From Template  Deploying Applications From Template 이번 Lab 은 Template 로 생성된 App 을 배포 하도록 하겠습니다.\nDjango + pgsql 배포 (no pv) 신규 Project 생성\nFrom Catalog 를 선택합니다. Django + pgsql(Ephemeral) 선택합니다.\n필요한 옵션을 입력하고 APP 을 배포합니다. pgsql 이 배포됩니다.\nDjango 가 Build 됩니다.\nPage Views 수가 올라가면서 해당 데이터는 DB 에 저장됩니다.\n하지만 해당 Template 는 no PV 옵션으로 Pod 이 재생성되면 데이터는 삭제됩니다.\nDjango + pgsql 배포 (Use pv)  https://chhanz.github.io/kubernetes/2019/04/15/kubernetes-chapter-2-network-volume/  위와 같이 Template 를 이용하여 App 를 배포합니다.\n차이점은 PV 를 cluster-admin 이 PV 를 생성하고 제공해야됩니다.\nPV 를 사용하여 배포하겠습니다.\n위와 같이 PV 로 사용중인 NFS 에 DATA 가 저장 되는 것을 볼 수 있습니다.\n$ oc get pvc -A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE openshift-image-registry image-registry-storage Bound registry-pv0001 100Gi RWX 32h pv-test-project postgresql Bound pv0003 10Gi RWO 55s  $ oc get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv0001 10Gi RWO Recycle Available 98m pv0002 10Gi RWO Recycle Available 98m pv0003 10Gi RWO Recycle Bound pv-test-project/postgresql 98m pv0004 10Gi RWX Retain Available 98m pv0005 10Gi RWX Retain Available 98m pv0006 10Gi RWX Retain Available 98m registry-pv0001 100Gi RWX Retain Bound openshift-image-registry/image-registry-storage 32h 위와 같이 pvc 가 생성이 되고 pvc 조건에 맞는 pv 가 Bound 됩니다.\nPV 할당 정보 확인 ","permalink":"https://chhanz88.github.io/post/2020-06-24-ocp4-template/","summary":"해당 자료는 사내 교육용으로 제작된 자료입니다.\n자료 사용시 출처 부탁 드려요.\n 목차  Deploying Applications From Images Deploying Applications From Source Deploying Applications From Template  Deploying Applications From Template 이번 Lab 은 Template 로 생성된 App 을 배포 하도록 하겠습니다.\nDjango + pgsql 배포 (no pv) 신규 Project 생성\nFrom Catalog 를 선택합니다. Django + pgsql(Ephemeral) 선택합니다.\n필요한 옵션을 입력하고 APP 을 배포합니다. pgsql 이 배포됩니다.\nDjango 가 Build 됩니다.","title":"[OpenShift 4.4] Deploying Applications From Template"},{"content":" 해당 자료는 사내 교육용으로 제작된 자료입니다.\n자료 사용시 출처 부탁 드려요.\n 목차  Deploying Applications From Images Deploying Applications From Source Deploying Applications From Template  Deploying Applications From Source 이번 Lab 은 Source 를 이용하여 App 을 배포 하도록 하겠습니다.\nPHP WebApp 배포  Web Console 로 developer 계정으로 로그인합니다.\n 테스트를 위한 신규 Project 생성합니다. From Git 항목을 선택합니다. 소스 주소 https://github.com/chhanz/docker-swarm-demo.git 를 입력하고 사용할 Builder 로 PHP 를 선택합니다.\nBuilder 로 사용할 PHP Version 선택합니다. Build 가 시작되는 것을 확인 할 수 있습니다.\n상세 로그를 확인 할 수 있습니다.\nBuild 가 완료 되었습니다.(S2I)\n위와 같이 original 배포 완료가 된 것을 확인 할 수 있습니다. Scale Out 을 수행하여 APP 이 정상 작동하는지 확인 하도록 하겠습니다. Load Balancing 이 잘 되는 것을 확인 할 수 있습니다.\nSource 를 수정하여 Github 에 반영해보도록 하겠습니다. Github 에 Source Push 되었습니다. 신규 Version 으로 Start Build 합니다. Build 가 완료 되었습니다.\nVersion v2 배포 완료 되었습니다.\n","permalink":"https://chhanz88.github.io/post/2020-06-22-ocp4-deploy-source/","summary":"해당 자료는 사내 교육용으로 제작된 자료입니다.\n자료 사용시 출처 부탁 드려요.\n 목차  Deploying Applications From Images Deploying Applications From Source Deploying Applications From Template  Deploying Applications From Source 이번 Lab 은 Source 를 이용하여 App 을 배포 하도록 하겠습니다.\nPHP WebApp 배포  Web Console 로 developer 계정으로 로그인합니다.\n 테스트를 위한 신규 Project 생성합니다. From Git 항목을 선택합니다. 소스 주소 https://github.com/chhanz/docker-swarm-demo.git 를 입력하고 사용할 Builder 로 PHP 를 선택합니다.","title":"[OpenShift 4.4] Deploying Applications From Source"},{"content":" 해당 자료는 사내 교육용으로 제작된 자료입니다.\n자료 사용시 출처 부탁 드려요.\n 목차  Deploying Applications From Images Deploying Applications From Source Deploying Applications From Template  Deploying Applications From Images 이번 Lab 은 Container Image 로 생성된 App 을 배포 하도록 하겠습니다.\nDjango WebApp 배포  Web Console 로 developer 계정으로 로그인합니다.\n CLI 로 배포 $ oc login -u developer $ oc project django-project $ oc new-app openshiftkatacoda/blog-django-py --name blog-from-image $ oc expose svc/blog-from-image  $ oc get all NAME READY STATUS RESTARTS AGE pod/blog-django-py-6b787ccc9f-hl7tk 1/1 Running 0 16m pod/blog-from-image-1-74snj 1/1 Running 0 41s pod/blog-from-image-1-deploy 0/1 Completed 0 45s  NAME DESIRED CURRENT READY AGE replicationcontroller/blog-from-image-1 1 1 1 45s  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/blog-django-py ClusterIP 172.30.139.80 \u0026lt;none\u0026gt; 8080/TCP 16m service/blog-from-image ClusterIP 172.30.210.95 \u0026lt;none\u0026gt; 8080/TCP 48s  NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/blog-django-py 1/1 1 1 16m  NAME DESIRED CURRENT READY AGE replicaset.apps/blog-django-py-6b787ccc9f 1 1 1 16m replicaset.apps/blog-django-py-6f84ff6b79 0 0 0 16m  NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/blog-from-image 1 1 1 config,image(blog-from-image:latest)  NAME IMAGE REPOSITORY TAGS UPDATED imagestream.image.openshift.io/blog-django-py image-registry.openshift-image-registry.svc:5000/django-project/blog-django-py latest 16 minutes ago imagestream.image.openshift.io/blog-from-image image-registry.openshift-image-registry.svc:5000/django-project/blog-from-image latest 46 seconds ago  NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/blog-django-py blog-django-py-django-project.apps.ocp.chhan.com blog-django-py 8080-tcp None route.route.openshift.io/blog-from-image blog-from-image-django-project.apps.ocp.chhan.com blog-from-image 8080-tcp None 간단한 Container Image 를 Build 해봅시다.  참고 자료 : Best practices for writing Dockerfiles  FROMhttpd:2.4COPY index.html /usr/local/apache2/htdocs/EXPOSE80해당 경로에는 Dockerfile 및 index.html 파일이 필요합니다.\nBuild $ docker build -t han0495/sample-httpd . Sending build context to Docker daemon 3.072kB Step 1/3 : FROM httpd:2.4 2.4: Pulling from library/httpd afb6ec6fdc1c: Pull complete 5a6b409207a3: Pull complete 41e5e22239e2: Pull complete 9829f70a6a6b: Pull complete 3cd774fea202: Pull complete Digest: sha256:db9c3bca36edb5d961d70f83b13e65e552641e00a7eb80bf435cbe9912afcb1f Status: Downloaded newer image for httpd:2.4  ---\u0026gt; d4e60c8eb27a Step 2/3 : COPY index.html /usr/local/apache2/htdocs/  ---\u0026gt; caf363ed04d9 Step 3/3 : EXPOSE 80  ---\u0026gt; Running in d2052322cca7 Removing intermediate container d2052322cca7  ---\u0026gt; bf4d56b96457 Successfully built bf4d56b96457 Successfully tagged han0495/sample-httpd:latest Push $ docker login $ docker push \u0026lt;DockerHub 계정\u0026gt;/sample-httpd Deploy 배포에 문제가 생기는 경우, SCC 를 확인합니다.\n참고 자료  https://hub.docker.com/r/han0495/sample-httpd https://github.com/chhanz/sample-httpd-example https://chhanz.github.io/openshift/2019/11/25/openshift4-dev-env/ https://docs.openshift.com/container-platform/4.4/authentication/managing-security-context-constraints.html  ","permalink":"https://chhanz88.github.io/post/2020-06-18-ocp4-deploy-image/","summary":"해당 자료는 사내 교육용으로 제작된 자료입니다.\n자료 사용시 출처 부탁 드려요.\n 목차  Deploying Applications From Images Deploying Applications From Source Deploying Applications From Template  Deploying Applications From Images 이번 Lab 은 Container Image 로 생성된 App 을 배포 하도록 하겠습니다.\nDjango WebApp 배포  Web Console 로 developer 계정으로 로그인합니다.\n CLI 로 배포 $ oc login -u developer $ oc project django-project $ oc new-app openshiftkatacoda/blog-django-py --name blog-from-image $ oc expose svc/blog-from-image  $ oc get all NAME READY STATUS RESTARTS AGE pod/blog-django-py-6b787ccc9f-hl7tk 1/1 Running 0 16m pod/blog-from-image-1-74snj 1/1 Running 0 41s pod/blog-from-image-1-deploy 0/1 Completed 0 45s  NAME DESIRED CURRENT READY AGE replicationcontroller/blog-from-image-1 1 1 1 45s  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/blog-django-py ClusterIP 172.","title":"[OpenShift 4.4] Deploying Applications From Images"},{"content":" 해당 자료는 사내 교육용으로 제작된 자료입니다.\n자료 사용시 출처 부탁 드려요.\n  https://github.com/chhanz/container-hands-on  Container \u0026amp; Orchestration 교육 자료 목차  Docker Podman Kubernetes OpenShift  Docker Hands-on 환경  OS : CentOS 7.7\nDisable SELinux, Firewalld\n Install  Install Package  $ yum -y install docker  start service docker  $ systemctl enable --now docker RUN  start container nginx  $ docker run -d -ti --name nginx -p 80:80 nginx  check container nginx  $ docker ps -a  access web  $ crul http://192.168.200.100  stop container nginx  $ docker stop nginx  delete container nginx  $ docker rm nginx  container images  $ docker images Build  참고 소스 : https://github.com/chhanz/docker-swarm-demo\n  create Dockerfile  FROM php:7.2-apache MAINTAINER chhan \u0026lt;cheolhee.han@ibm.com\u0026gt; ADD htdocs/index.php /var/www/html/index.php EXPOSE 80  create index.php  ├── Dockerfile └── htdocs  └── index.php  index.php  \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;chhan sample page\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt; \u0026lt;b\u0026gt; \u0026lt;?php $host=gethostname(); echo \u0026#34;Container Name : \u0026#34;; echo $host; ?\u0026gt;\u0026lt;p\u0026gt; Image Version : original \u0026lt;/p\u0026gt; \u0026lt;/b\u0026gt; \u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  build image  $ docker build -t php-web:v1 . Sending build context to Docker daemon 69.63 kB Step 1/4 : FROM php:7.2-apache Trying to pull repository docker.io/library/php ... 7.2-apache: Pulling from docker.io/library/php 68ced04f60ab: Already exists 1d2a5d8fa585: Pull complete 5d59ec4ae241: Pull complete d42331ef4d44: Pull complete 408b7b7ee112: Pull complete 570cd47896d5: Pull complete 2419413b2a16: Pull complete ece88053da01: Pull complete b2131b538da3: Pull complete e26c9457b1ed: Pull complete 6b4ecf095186: Pull complete 6a41f34ff2f0: Pull complete b6669ed2ef9e: Pull complete 0441670f3790: Pull complete Digest: sha256:aa35f1d87dc285959f365c0181967956849734577db72b5b30de7cc73308b44f Status: Downloaded newer image for docker.io/php:7.2-apache  ---\u0026gt; ba07a75a195b Step 2/4 : MAINTAINER chhan \u0026lt;cheolhee.han@ibm.com\u0026gt;  ---\u0026gt; Running in ae0e738f0c73  ---\u0026gt; 642eae0a1cb5 Removing intermediate container ae0e738f0c73 Step 3/4 : ADD htdocs/index.php /var/www/html/index.php  ---\u0026gt; 2a23b4d2dc2d Removing intermediate container f685d41d2501 Step 4/4 : EXPOSE 80  ---\u0026gt; Running in 18a7ba3a06a8  ---\u0026gt; d4c7b832c710 Removing intermediate container 18a7ba3a06a8 Successfully built d4c7b832c710  change tag  $ docker tag php-web:v1 han0495/php-web:v1  docker login  $ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don\u0026#39;t have a Docker ID, head over to https://hub.docker.com to create one. Username: han0495 Password: Login Succeeded  push image  $ docker push han0495/php-web:v1 The push refers to a repository [docker.io/han0495/php-web] 89bb5e6b86ac: Pushed 6565bce2d5e5: Mounted from library/php 2159d2f64d7e: Mounted from library/php 7c111aa3fc84: Mounted from library/php 1bc9b7122630: Mounted from library/php bc4aa4d1d971: Mounted from library/php 8b81b9cd95de: Mounted from library/php 85dc2281e45a: Mounted from library/php 0fc284fc9cf5: Mounted from library/php 732057c800a3: Mounted from library/php 4cc11613548d: Mounted from library/php df6c050501b6: Mounted from library/php b4bfb20b5f05: Mounted from library/php 2e8cc9f5313f: Mounted from library/php f2cb0ecef392: Mounted from library/nginx v1: digest: sha256:0b13f585044db431341d5cce3b48df37e78598317fdf60f0135ffffb2c89a80c size: 3449  run  $ docker run -d -ti --name php-web -p 80:80 han0495/php-web:v1 Podman Hands-on 환경  OS : CentOS 8.1\nDisable SELinux, Firewalld\n Install $ dnf -y install podman RUN  start container httpd  $ podman run -d -ti --name web -p 80:80 httpd  check container nginx  $ podman ps -a  access web  $ curl 192.168.200.101 \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;It works!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;  참고 자료: https://chhanz.github.io/container/2020/03/02/podman/  Kubernetes Hands-on 환경  Kubernetes Master 3 Node cluster 구성\nKubernetes Worker 2 Node\nKubernetes v1.16.8\nDocker v18.09.7\n APP 배포(NodePort)  php-web.yaml Yaml 작성  apiVersion: apps/v1 kind: Deployment metadata:  labels:  run: php-web  name: php-web spec:  replicas: 5  selector:  matchLabels:  run: php-web  template:  metadata:  labels:  run: php-web  spec:  containers:  - image: han0495/php-web:v1  name: php-web  ---  apiVersion: v1 kind: Service metadata:  labels:  run: php-web  name: php-web  namespace: default spec:  clusterIP: 10.233.25.48  externalTrafficPolicy: Cluster  ports:  - nodePort: 30518  port: 80  protocol: TCP  targetPort: 80  selector:  run: php-web  sessionAffinity: None  type: NodePort  php-web 배포  $ kubectl create -f php-web.yaml  php-web 배포 확인  $ kubectl get all  php-web 서비스 확인  $ curl worker1.example.com:30518 $ curl worker2.example.com:30518 Kubernetes-dashboard 둘러보기 $ kubectl get all -l k8s-app=kubernetes-dashboard -A Kubernetes Node 장애 시나리오 $ kubectl drain worker2.example.com OpenShift Hands-on 환경  OpenShift 3 Node cluster 구성\nOpenShift Worker 2 Node\nOpenShift Infra 2 Node\nOpenShift v3.11\nDocker v1.13.1\n APP 배포(S2I) GIT Source $ oc new-app httpd~https://github.com/chhanz/sample-httpd-example.git $ oc expose service sample-httpd-example Local Source $ git clone https://github.com/chhanz/sample-httpd-example.git $ cd sample-httpd-example $ vi index.html  ## 변경된 Source Build $ oc start-build sample-httpd-example --from-dir=./ --commit=v2 Auto-scaling  Dockerfile sample source  FROMphp:5-apacheADD index.php /var/www/html/index.phpRUN chmod a+rx index.php index.php source\nCPU 부하 유발 source.  \u0026lt;?php  $x = 0.0001;  for ($i = 0; $i \u0026lt;= 1000000; $i++) {  $x += sqrt($x);  }  echo \u0026#34;OK!\u0026#34;; ?\u0026gt; Build image : https://hub.docker.com/r/han0495/hpa-example  APP 배포  php-apache.yml  $ oc create -f php-apache.yml $ oc expose service/php-apache HPA(Horizontal Pod Autoscaler) 생성 $ oc autoscale deployment php-apache --cpu-percent=25 --min=1 --max=10 부하 발생 $ while true; do wget -q -O- http://php-apache-test-project.apps.chhan.com; done  HPA 상태 확인  $ oc get hpa  Grafana Dashboard  ","permalink":"https://chhanz88.github.io/post/2020-05-21-container-hands-on/","summary":"해당 자료는 사내 교육용으로 제작된 자료입니다.\n자료 사용시 출처 부탁 드려요.\n  https://github.com/chhanz/container-hands-on  Container \u0026amp; Orchestration 교육 자료 목차  Docker Podman Kubernetes OpenShift  Docker Hands-on 환경  OS : CentOS 7.7\nDisable SELinux, Firewalld\n Install  Install Package  $ yum -y install docker  start service docker  $ systemctl enable --now docker RUN  start container nginx  $ docker run -d -ti --name nginx -p 80:80 nginx  check container nginx  $ docker ps -a  access web  $ crul http://192.","title":"[Container] Container \u0026 Orchestration 교육 자료 "},{"content":"Red Hat Ansible Tower Bundle 설치(v3.6.3) Ansible Tower 설치 준비 Ansible Tower 설치를 위해 아래와 같이 Repository 를 Enable 합니다.\n$ subscription-manager repos --enable=rhel-7-server-rpms $ subscription-manager repos --enable=rhel-7-server-ansible-2.9-rpms $ subscription-manager repos --enable=rhel-server-rhscl-7-rpms Download 된 Bundle 을 설치할 시스템에 Upload 하고 아래와 같이 진행니다.\n[root@fastvm-r77-99 ~]# tar xzvf ansible-tower-setup-bundle-3.6.3-1.tar.gz  [root@fastvm-r77-99 ~]# cd ansible-tower-setup-bundle-3.6.3-1 Ansible Tower 설치 inventory 수정 아래와 같이 Ansible Tower 에서 사용될 Password 를 지정합니다.\n$ vi inventory  ...  [all:vars] admin_password=\u0026#39;password\u0026#39; \u0026lt;\u0026lt; password 변경  pg_host=\u0026#39;\u0026#39; pg_port=\u0026#39;\u0026#39;  pg_database=\u0026#39;awx\u0026#39; pg_username=\u0026#39;awx\u0026#39; pg_password=\u0026#39;password\u0026#39; \u0026lt;\u0026lt; password 변경 pg_sslmode=\u0026#39;prefer\u0026#39;  # set to \u0026#39;verify-full\u0026#39; for client-side enforced SSL  rabbitmq_username=tower  rabbitmq_password=\u0026#39;password\u0026#39; \u0026lt;\u0026lt; password 변경 ... 필수 Package 설치 OS 설치 환경에 따라 차이가 있으나, setup.sh 에서 자동으로 설치가 진행 안되는 Package 는 수동으로 설치를 진행해야됩니다.\n$ yum -y install wget curl rsync Ansible Tower 배포 아래 bundle 디렉토리에 있는 setup.sh 스크립트를 구동하면 자동으로 ansible core 를 설치하고, 이후 Ansible Tower 를 배포하는 playbook 을 수행합니다.\n[root@fastvm-r77-99 ansible-tower-setup-bundle-3.6.3-1]# ./setup.sh 배포 완료 참고 자료  https://docs.ansible.com/ansible-tower/latest/html/quickinstall/index.html  ","permalink":"https://chhanz88.github.io/post/2020-05-12-install-ansible-tower/","summary":"Red Hat Ansible Tower Bundle 설치(v3.6.3) Ansible Tower 설치 준비 Ansible Tower 설치를 위해 아래와 같이 Repository 를 Enable 합니다.\n$ subscription-manager repos --enable=rhel-7-server-rpms $ subscription-manager repos --enable=rhel-7-server-ansible-2.9-rpms $ subscription-manager repos --enable=rhel-server-rhscl-7-rpms Download 된 Bundle 을 설치할 시스템에 Upload 하고 아래와 같이 진행니다.\n[root@fastvm-r77-99 ~]# tar xzvf ansible-tower-setup-bundle-3.6.3-1.tar.gz  [root@fastvm-r77-99 ~]# cd ansible-tower-setup-bundle-3.6.3-1 Ansible Tower 설치 inventory 수정 아래와 같이 Ansible Tower 에서 사용될 Password 를 지정합니다.\n$ vi inventory  .","title":"[Ansible] Red Hat Ansible Tower Bundle 설치(v3.6.3)"},{"content":"목차  Gitlab-CE 설치 Jenkins 설치 Nexus Repository Manager 설치 Jenkins CI Pipeline 구성  Jenkin 를 이용하여 CI 를 자동화해보자! 이번 포스팅에서는 Jenkins 를 이용하여 CI 를 자동화 하는 방법을 알아보도록 하겠습니다.\n이 구성은 Github 의 Actions 와 비슷하게 구성 할 수 있습니다.\nPipeline 시나리오  Gitlab 의 특정 Project 에서 Push or Merge 가 발생하면 Jenkins 에 CI 를 발생하도록 webhook 을 발생한다. Jenkins 는 설정된 Pipeline 에 맞게 CI 를 진행한다. CI 는 Clone source \u0026gt; docker build \u0026gt; docker run \u0026gt; docker rm 으로 진행 됩니다.  CI 설정 테스트 소스 CI 테스트용으로 사용될 소스는 아래와 같습니다.\n Dockerfile 과 index.html 로 구성된 소스입니다.  FROMhttpdCOPY index.html /usr/local/apache2/htdocs Dockerfile 내용입니다. httpd image 에 index.html 를 추가하는 Dockerfile 입니다.  Jenkins Token 생성 및 등록 Pipeline 생성 Pipeline Script 추가 node {  stage (\u0026#39;clone\u0026#39;) {  git branch: \u0026#39;master\u0026#39;, credentialsId: \u0026#39;chhanz\u0026#39;, url: \u0026#39;git@gitlab.example.com:chhanz/cicd-httpd-source.git\u0026#39;  }  stage (\u0026#39;docker build\u0026#39;) {  sh \u0026#39; docker build --tag sample-ci-httpd .\u0026#39;  }   stage (\u0026#39;run docker\u0026#39;) {  sh \u0026#39; docker run -d -ti --name jenkins-ci -p 33333:80 sample-ci-httpd\u0026#39;  }   stage (\u0026#39;check httpd\u0026#39;) {  sh \u0026#39;curl -s http://127.0.0.1:33333\u0026#39;  }   stage (\u0026#39;rm docker\u0026#39;) {  sh \u0026#39; docker rm -f jenkins-ci\u0026#39;  } } 위와 같이 Pipeline Script 를 구성하였습니다.\n 상세 Pipeline 순서    Clone Source Build Image Run Container Healthy check Delete Container   Build 결과 Gitlab 과 Jenkins 연동 지금까진 수동으로 Build 진행 되는 Pipeline 을 생성하였습니다.\n이제는 Gitlab Source 의 Push or Merge 가 발생되면 Jenkins 에서 Build 가 자동으로 진행 되도록 구성하여 CI 자동화를 완성하도록 하겠습니다.\nCI 자동화 테스트 실제로 소스를 변경하여 CI 가 자동화 되는지 확인하겠습니다.\n[root@fastvm-centos-7-7-91 cicd-httpd-source]# git add . [root@fastvm-centos-7-7-91 cicd-httpd-source]# git status # On branch master # Changes to be committed: # (use \u0026#34;git reset HEAD \u0026lt;file\u0026gt;...\u0026#34; to unstage) # #\tmodified: index.html # [root@fastvm-centos-7-7-91 cicd-httpd-source]# git commit -m \u0026#34;update index.html\u0026#34; [master ad71674] update index.html  1 file changed, 1 insertion(+), 1 deletion(-) [root@fastvm-centos-7-7-91 cicd-httpd-source]# git push Counting objects: 5, done. Delta compression using up to 4 threads. Compressing objects: 100% (3/3), done. Writing objects: 100% (3/3), 279 bytes | 0 bytes/s, done. Total 3 (delta 2), reused 0 (delta 0) To git@gitlab.example.com:chhanz/cicd-httpd-source.git  c633ff8..ad71674 master -\u0026gt; master 참고 자료  https://www.jenkins.io/doc/book/pipeline/ https://tech.osci.kr/2020/01/16/86039236/  ","permalink":"https://chhanz88.github.io/post/2020-05-04-jenkins-ci/","summary":"목차  Gitlab-CE 설치 Jenkins 설치 Nexus Repository Manager 설치 Jenkins CI Pipeline 구성  Jenkin 를 이용하여 CI 를 자동화해보자! 이번 포스팅에서는 Jenkins 를 이용하여 CI 를 자동화 하는 방법을 알아보도록 하겠습니다.\n이 구성은 Github 의 Actions 와 비슷하게 구성 할 수 있습니다.\nPipeline 시나리오  Gitlab 의 특정 Project 에서 Push or Merge 가 발생하면 Jenkins 에 CI 를 발생하도록 webhook 을 발생한다. Jenkins 는 설정된 Pipeline 에 맞게 CI 를 진행한다.","title":"[Devops] Jenkins CI Pipeline 구성"},{"content":"목차  Gitlab-CE 설치 Jenkins 설치 Nexus Repository Manager 설치  Nexus Repository Manager 3 (OSS Version) 설치 Private Container Image 저장소를 만들기 위해 Nexus Registory Manager(이하 Nexus) 구성을 하려고 합니다.\nNexus Data 저장 공간 생성  Nexus Data 공간을 생성합니다.  $ mkdir /data $ chown 200:200 /data UID/GID 는 200 으로 설정합니다.(Container Image 에 선언되어 있습니다.)\nUse Docker  Docker 를 이용할 경우, 아래와 같이 명령을 수행합니다.  $ docker run -d -p 8081:8081 -p 5000:5000 --name nexus -v /data:/nexus-data sonatype/nexus3 Use Podman  Podman 을 이용할 경우, 아래와 같이 명령을 수행합니다.  $ podman run -d -p 8081:8081 -p 5000:5000 --name nexus -v /data:/nexus-data sonatype/nexus3 Nexus 초기 설정 Private Container Image 저장소 생성 Container Image Push  아래와 같이 Docker Host 에서 /etc/docker/daemon.json 를 수정하여 Private Image Repository 를 사용 할 수 있도록 추가합니다.  $ cat /etc/docker/daemon.json {  \u0026#34;insecure-registries\u0026#34; : [\u0026#34;192.168.200.92:5000\u0026#34;] }  Docker login 을 진행합니다.  $ docker login 192.168.200.92:5000 Username: admin Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store  Login Succeeded  Container Image 를 Push 하기 위해 tag 를 수정합니다.  $ docker tag test-go-web 192.168.200.92:5000/test-go-web:latest $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 6a111295a403 8 days ago 386MB test-go-web latest 4aa4b4c6d791 8 days ago 7.49MB 192.168.200.92:5000/test-go-web latest 4aa4b4c6d791 8 days ago 7.49MB golang alpine 760fdda71c8f 3 weeks ago 370MB  Container Image 를 Push 합니다.  $ docker push 192.168.200.92:5000/test-go-web The push refers to repository [192.168.200.92:5000/test-go-web] 3417e2abcc0c: Pushed latest: digest: sha256:35e67bc46ce93066a042e97954afeff9a7f2c6498783040703a32efa1a4c4e21 size: 528 위와 같이 Push 가 완료가 되고, 아래와 같이 Nexus GUI 에서 Container Image 저장 상태를 확인 할 수 있습니다.\n참고 자료  https://www.jacobbaek.com/846 https://help.sonatype.com/repomanager3/security/realms https://blog.sonatype.com/using-nexus-3-as-your-repository-part-3-docker-images  ","permalink":"https://chhanz88.github.io/post/2020-04-17-install-nexus-ce/","summary":"목차  Gitlab-CE 설치 Jenkins 설치 Nexus Repository Manager 설치  Nexus Repository Manager 3 (OSS Version) 설치 Private Container Image 저장소를 만들기 위해 Nexus Registory Manager(이하 Nexus) 구성을 하려고 합니다.\nNexus Data 저장 공간 생성  Nexus Data 공간을 생성합니다.  $ mkdir /data $ chown 200:200 /data UID/GID 는 200 으로 설정합니다.(Container Image 에 선언되어 있습니다.)\nUse Docker  Docker 를 이용할 경우, 아래와 같이 명령을 수행합니다.  $ docker run -d -p 8081:8081 -p 5000:5000 --name nexus -v /data:/nexus-data sonatype/nexus3 Use Podman  Podman 을 이용할 경우, 아래와 같이 명령을 수행합니다.","title":"[Devops] Nexus Repository Manager 설치(use docker)"},{"content":"목차  Gitlab-CE 설치 Jenkins 설치 Nexus Repository Manager 설치  Jenkins RPM 설치 RPM 을 이용한 설치 과정입니다.\n 참고 문서 : https://pkg.jenkins.io/redhat/  RPM Version  필수 Package 설치  $ yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel  Jenkins repository 추가  $ wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat/jenkins.repo  Jenkins 설치  $ yum install jenkins  Jenkins 서비스 시작  $ systemctl enable jenkins $ systemctl start jenkins Jenkins WAR 설치 Warfile 을 배포하여 설치 하는 과정입니다.\nWAR Version  필수 Package 설치  $ yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel  Tomcat 설치  $ yum install tomcat  Warfile 배포  $ wget -O /var/lib/tomcat/webapps/ROOT.war http://mirrors.jenkins.io/war-stable/latest/jenkins.war  Tomcat 서비스 시작  $ systemctl enable --now tomcat Jenkins 초기화 작업 하기 작업은 RPM/WAR 방식이 동일합니다.\n","permalink":"https://chhanz88.github.io/post/2020-04-16-install-jenkins/","summary":"목차  Gitlab-CE 설치 Jenkins 설치 Nexus Repository Manager 설치  Jenkins RPM 설치 RPM 을 이용한 설치 과정입니다.\n 참고 문서 : https://pkg.jenkins.io/redhat/  RPM Version  필수 Package 설치  $ yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel  Jenkins repository 추가  $ wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat/jenkins.repo  Jenkins 설치  $ yum install jenkins  Jenkins 서비스 시작  $ systemctl enable jenkins $ systemctl start jenkins Jenkins WAR 설치 Warfile 을 배포하여 설치 하는 과정입니다.","title":"[Devops] Jenkins 설치(RPM/WAR)"},{"content":"SubPath 배포  sample.war 을 준비한다. /var/lib/tomcat/webapps 에 War File을 위치한다. Tomcat 서비스를 기동한다. http://localhost:8080/sample 으로 접근이 가능하다.  ROOT 로 서비스  sample.war 을 준비한다. /var/lib/tomcat/webapps 에 War File을 위치한다. /var/lib/tomcat/webapps/sample.war file 의 이름을 ROOT.war 로 변경한다. tomcat 서비스를 기동한다. http://localhost:8080/으로 접근이 가능하다.  ","permalink":"https://chhanz88.github.io/post/2020-03-12-deploy-warfile/","summary":"SubPath 배포  sample.war 을 준비한다. /var/lib/tomcat/webapps 에 War File을 위치한다. Tomcat 서비스를 기동한다. http://localhost:8080/sample 으로 접근이 가능하다.  ROOT 로 서비스  sample.war 을 준비한다. /var/lib/tomcat/webapps 에 War File을 위치한다. /var/lib/tomcat/webapps/sample.war file 의 이름을 ROOT.war 로 변경한다. tomcat 서비스를 기동한다. http://localhost:8080/으로 접근이 가능하다.  ","title":"[Tomcat] Tomcat War file 배포"},{"content":".bash_history 로그에 timestamp 추가하기  Add timestamp to .bash_history /etc/profile 에 해당 구문을 추가한다.\nHISTTIMEFORMAT=\u0026#34;[%Y-%m-%d %H:%M:%S] \u0026#34; export HISTTIMEFORMAT Result [root@fastvm-centos-7-7-30 ~]# history   1 [2020-01-13 17:34:40] history -ps  2 [2020-01-13 17:34:40] ls  3 [2020-01-13 17:34:40] ip a  4 [2020-01-13 17:34:40] lsblk  5 [2020-01-13 17:34:40] df  6 [2020-01-13 17:34:40] history -ps  7 [2020-01-13 17:34:40] history  8 [2020-01-13 17:34:40] ls  9 [2020-01-13 17:34:40] cd /etc  10 [2020-01-13 17:34:40] ls  11 [2020-01-13 17:34:40] cd profile.d/ ","permalink":"https://chhanz88.github.io/post/2020-03-10-bash-history-timestamp/","summary":".bash_history 로그에 timestamp 추가하기  Add timestamp to .bash_history /etc/profile 에 해당 구문을 추가한다.\nHISTTIMEFORMAT=\u0026#34;[%Y-%m-%d %H:%M:%S] \u0026#34; export HISTTIMEFORMAT Result [root@fastvm-centos-7-7-30 ~]# history   1 [2020-01-13 17:34:40] history -ps  2 [2020-01-13 17:34:40] ls  3 [2020-01-13 17:34:40] ip a  4 [2020-01-13 17:34:40] lsblk  5 [2020-01-13 17:34:40] df  6 [2020-01-13 17:34:40] history -ps  7 [2020-01-13 17:34:40] history  8 [2020-01-13 17:34:40] ls  9 [2020-01-13 17:34:40] cd /etc  10 [2020-01-13 17:34:40] ls  11 [2020-01-13 17:34:40] cd profile.","title":"[Linux] .bash_history 로그에 timestamp 추가하기"},{"content":"Podman 이란? Red Hat Enterprise Linux 8 / CentOS 8 부터는 Docker 대신 Podman 이라는 도구를 제공합니다.\nPodman 은 Docker 와 동일하게 단일 노드에서 pod, 컨테이너 이미지 및 컨테이너를 관리합니다.\nPod 라고 하는 컨테이너 및 컨테이너 그룹을 관리할 수 있는 libpod 라이브러리를 기반으로 합니다.\n RHEL 8 Release Note  이번 포스팅에서는 Podman 의 설치 및 기본 사용법에 대해 확인 해보겠습니다.\nDocker VS Podman Docker 와 Podman 은 아래와 같이 \u0026ldquo;컨테이너 Cli 가 컨테이너를 어떻게 생성하냐\u0026rdquo; 의 차이가 있습니다.\n- Docker 구조 -- Podman 구조 -위와 같은 구조적인 차이로 인해 예전에 Docker 에서는 systemd 를 이용하여 docker daemon을 재시작 하거나 중지를 하면 운영중인 컨테이너에 영향을 주었습니다.\n하지만 Podman 은 Docker 와 달리 서로 간섭이 없는 컨테이너를 생성 할 수 있습니다.\n일종의 PID 가 다른 프로세서라고 생각하면 됩니다.\nPodman 설치 Podman 설치는 Docker 설치 만큼 쉽습니다.\n참고로 현재 CentOS 7 에서는 Docker 와 Podman 을 동시에 지원하니 기존에 Docker 로 쓰던 컨테이너를 Podman 으로도 전환이 가능합니다.\n[root@fastvm-centos-7-7-41 ~]# yum -y install podman 위와 같이 명령 수행만 하면 설치는 완료됩니다.\n설치만 보면 Docker 보다 쉽습니다. 특이한 점은 systemd 설정 할 필요가 없습니다.\n[root@fastvm-centos-7-7-41 ~]# systemctl list-units | grep -i pod [root@fastvm-centos-7-7-41 ~]# 위와 같이 Docker 와 다르게 systemd 로 서비스를 시작할 필요가 없습니다.\nPodman 기본 사용법 Podman 의 기본 사용법을 알아보겠습니다.\n컨테이너 생성 [root@fastvm-centos-7-7-41 ~]# podman run --help Run a command in a new container Description:  Runs a command in a new container from the given image Usage:  podman run [flags] IMAGE [COMMAND [ARG...]] 위와 같은 구문을 이용하여 아래와 같이 컨테이너 생성이 가능합니다.\n[root@fastvm-centos-7-7-41 ~]# podman run -ti -d --name web httpd 72f4b9dde39323f8a98188ba96c36552e3d98f30931256504aca71d3024fe01b [root@fastvm-centos-7-7-41 ~]# 컨테이너 구동 확인 [root@fastvm-centos-7-7-41 ~]# podman ps --help List containers Description:  Prints out information about the containers Usage:  podman ps [flags] 위와 같은 구문을 통해 확인이 가능하며, 아래와 같이 컨테이너의 상태를 확인 할 수 있습니다.\n[root@fastvm-centos-7-7-41 ~]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 72f4b9dde393 docker.io/library/httpd:latest httpd-foreground 4 seconds ago Up 3 seconds ago web ea2aabb1fe18 docker.io/library/httpd:latest httpd-foreground 20 seconds ago Exited (0) 17 seconds ago relaxed_hypatia 컨테이너 중지 stop 이라는 옵션을 통해 컨테이너 중지가 가능하며 ps 구문을 통해 상태 확인이 가능합니다.\n[root@fastvm-centos-7-7-41 ~]# podman stop web 72f4b9dde39323f8a98188ba96c36552e3d98f30931256504aca71d3024fe01b  [root@fastvm-centos-7-7-41 ~]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 72f4b9dde393 docker.io/library/httpd:latest httpd-foreground 51 seconds ago Exited (0) 2 seconds ago web ea2aabb1fe18 docker.io/library/httpd:latest httpd-foreground About a minute ago Exited (0) About a minute ago relaxed_hypatia 컨테이너 시작 중지된 컨테이너는 start 옵션을 통해 컨테이너 시작이 가능합니다.\n[root@fastvm-centos-7-7-41 ~]# podman start web web  [root@fastvm-centos-7-7-41 ~]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 72f4b9dde393 docker.io/library/httpd:latest httpd-foreground About a minute ago Up 1 second ago web ea2aabb1fe18 docker.io/library/httpd:latest httpd-foreground About a minute ago Exited (0) About a minute ago relaxed_hypatia 컨테이너 이미지 확인 현재 Host 에 Pull 된 컨테이너 이미지를 확인 할 수 있습니다.\n[root@fastvm-centos-7-7-41 ~]# podman images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/httpd latest c5a012f9cf45 4 days ago 170 MB 컨테이너 이미지 Pull Red Hat registry 혹은 Docker Hub 의 컨테이너 이미지를 Pull 할 수 있습니다.\n[root@fastvm-centos-7-7-41 ~]# podman pull centos Trying to pull registry.access.redhat.com/centos...ERRO[0001] Error pulling image ref //registry.access.redhat.com/centos:latest: Error initializing source docker://registry.access.redhat.com/centos:latest: Error reading manifest latest in registry.access.redhat.com/centos: name unknown: Repo not found Failed Trying to pull docker.io/library/centos...Getting image source signatures Copying blob 8a29a15cefae done Copying config 470671670c done Writing manifest to image destination Storing signatures 470671670cac686c7cf0081e0b37da2e9f4f768ddc5f6a26102ccd1c6954c1ee  [root@fastvm-centos-7-7-41 ~]# podman images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/httpd latest c5a012f9cf45 5 days ago 170 MB docker.io/library/centos latest 470671670cac 6 weeks ago 245 MB 컨테이너 삭제 아래와 같이 rm 옵션을 이용하여 컨테이너를 삭제할 수 있습니다.\n[root@fastvm-centos-7-7-41 ~]# podman rm relaxed_hypatia ea2aabb1fe18e2937131bc76ae7dee0d95d92334865ff2938c67752142c27c21  [root@fastvm-centos-7-7-41 ~]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 72f4b9dde393 docker.io/library/httpd:latest httpd-foreground 2 minutes ago Up About a minute ago web 컨테이너 Port Mapping 지금까지 기본 사용법을 보면 Docker 와 매우 흡사한 옵션 사용법입니다.\nPort Mapping 또한 마찬가지입니다.\n [root@fastvm-centos-7-7-41 ~]# podman run -ti -d -p 80:80 --name web httpd 07e14dc469e6247c181daeac723c116a142f466a0273c638f4724bb75a173cbc  [root@fastvm-centos-7-7-41 ~]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 07e14dc469e6 docker.io/library/httpd:latest httpd-foreground 4 seconds ago Up 3 seconds ago 0.0.0.0:80-\u0026gt;80/tcp web  [root@fastvm-centos-7-7-41 ~]# curl 127.0.0.1 \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;It works!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; 위와 같이 -p 옵션이 동일하게 존재하며 사용이 가능합니다.\n이처럼 명령어들이 Docker 에서 사용하던 방식과 동일합니다.\n처음 접하거나 Docker 만 사용하던 사용자도 쉽게 접근이 가능합니다.\nPodman 에서만 제공되는 기능 Podman 에는 Podman 에서만 제공되는 특별한 기능이 추가되어 있습니다.\nsystemd generate 제공 containerd 로 하나로 관리되던 Docker 와 달리 각 프로세서로 생성이 되는 Podman 은 각 컨테이너 별로 systemd 의 서비스 파일을 생성하여 관리를 할 수 있습니다.\ngenerate 구문 [root@fastvm-centos-7-7-41 ~]# podman generate --help Generate structured data based for a containers and pods Usage:  podman generate [command] Available Commands:  kube Generate Kubernetes pod YAML from a container or pod  systemd Generate a systemd unit file for a Podman container 생성 방법  일반 컨테이너 생성  [root@fastvm-centos-7-7-41 ~]# podman run -d --name redis_server -p 6379:6379 redis systemd 서비스 파일 생성  [Unit] Description=Redis container  [Service] Restart=always ExecStart=/usr/bin/podman start -a redis_server ExecStop=/usr/bin/podman stop -t 2 redis_server  [Install] WantedBy=local.target 위와 같이 /etc/systemd/system/redis_server.service 파일을 생성하거나,\n[root@fastvm-centos-7-7-41 system]# podman generate systemd redis_server \u0026gt; /etc/systemd/system/redis_server.service  [root@fastvm-centos-7-7-41 system]# cat redis_server.service [Unit] Description=7eac708eb65114d7f37773315f6565c718a440df3cc2211ace17e85cd9699bc3 Podman Container [Service] Restart=on-failure ExecStart=/usr/bin/podman start 7eac708eb65114d7f37773315f6565c718a440df3cc2211ace17e85cd9699bc3 ExecStop=/usr/bin/podman stop -t 10 7eac708eb65114d7f37773315f6565c718a440df3cc2211ace17e85cd9699bc3 KillMode=none Type=forking PIDFile=/var/lib/containers/storage/overlay-containers/7eac708eb65114d7f37773315f6565c718a440df3cc2211ace17e85cd9699bc3/userdata/7eac708eb65114d7f37773315f6565c718a440df3cc2211ace17e85cd9699bc3.pid [Install] WantedBy=multi-user.target 위와 같이 generate 옵션을 통해 서비스 파일을 생성하고 Start 구문 및 Stop 구문, 기타 옵션을 컨테이너 실행 상황에 맞게 수정합니다.\nsystemd 를 이용하여 컨테이너 관리  컨테이너 시작 [root@fastvm-centos-7-7-41 ~]# systemctl start redis-container.service  [root@fastvm-centos-7-7-41 ~]# systemctl status redis-container.service ● redis-container.service - Redis container  Loaded: loaded (/etc/systemd/system/redis-container.service; enabled; vendor preset: disabled)  Active: active (running) since Mon 2020-03-02 21:34:15 KST; 3s ago  Main PID: 6508 (podman)  CGroup: /system.slice/redis-container.service  └─6508 /usr/bin/podman start -a redis_server  Mar 02 21:34:16 fastvm-centos-7-7-41 podman[6508]: 1:C 02 Mar 2020 12:34:16.140 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo Mar 02 21:34:16 fastvm-centos-7-7-41 podman[6508]: 1:C 02 Mar 2020 12:34:16.140 # Redis version=5.0.7, bits=64, commit=00000000, modified=0, pid=1, just started Mar 02 21:34:16 fastvm-centos-7-7-41 podman[6508]: 1:C 02 Mar 2020 12:34:16.140 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf Mar 02 21:34:16 fastvm-centos-7-7-41 podman[6508]: 1:M 02 Mar 2020 12:34:16.141 * Running mode=standalone, port=6379. Mar 02 21:34:16 fastvm-centos-7-7-41 podman[6508]: 1:M 02 Mar 2020 12:34:16.141 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. Mar 02 21:34:16 fastvm-centos-7-7-41 podman[6508]: 1:M 02 Mar 2020 12:34:16.141 # Server initialized Mar 02 21:34:16 fastvm-centos-7-7-41 podman[6508]: 1:M 02 Mar 2020 12:34:16.141 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add \u0026#39;vm.overcommit_memory = 1\u0026#39; to /etc/sysctl.conf and th...this to take effect. Mar 02 21:34:16 fastvm-centos-7-7-41 podman[6508]: 1:M 02 Mar 2020 12:34:16.141 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command \u0026#39;echo never \u0026gt; ... Mar 02 21:34:16 fastvm-centos-7-7-41 podman[6508]: 1:M 02 Mar 2020 12:34:16.142 * DB loaded from disk: 0.000 seconds Mar 02 21:34:16 fastvm-centos-7-7-41 podman[6508]: 1:M 02 Mar 2020 12:34:16.142 * Ready to accept connections Hint: Some lines were ellipsized, use -l to show in full.  [root@fastvm-centos-7-7-41 ~]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7eac708eb651 docker.io/library/redis:latest docker-entrypoint... 6 hours ago Up 8 seconds ago 0.0.0.0:6379-\u0026gt;6379/tcp redis_server 컨테이너 중지 [root@fastvm-centos-7-7-41 ~]# systemctl stop redis-container.service  [root@fastvm-centos-7-7-41 ~]# systemctl status redis-container.service ● redis-container.service - Redis container  Loaded: loaded (/etc/systemd/system/redis-container.service; enabled; vendor preset: disabled)  Active: inactive (dead)  Mar 02 21:34:33 fastvm-centos-7-7-41 podman[6508]: 1:signal-handler (1583152473) Received SIGTERM scheduling shutdown... Mar 02 21:34:33 fastvm-centos-7-7-41 podman[6508]: 1:M 02 Mar 2020 12:34:33.902 # User requested shutdown... Mar 02 21:34:33 fastvm-centos-7-7-41 podman[6508]: 1:M 02 Mar 2020 12:34:33.902 * Saving the final RDB snapshot before exiting. Mar 02 21:34:33 fastvm-centos-7-7-41 podman[6508]: 1:M 02 Mar 2020 12:34:33.904 * DB saved on disk Mar 02 21:34:33 fastvm-centos-7-7-41 podman[6508]: 1:M 02 Mar 2020 12:34:33.904 # Redis is now ready to exit, bye bye... Mar 02 21:34:34 fastvm-centos-7-7-41 podman[6625]: 2020-03-02 21:34:34.0168744 +0900 KST m=+0.172658043 container died 7eac708eb65114d7f37773315f6565c718a440df3cc2211ace17e85cd9699bc3 (image=docker.io/library/redis:latest, name=redis_server) Mar 02 21:34:34 fastvm-centos-7-7-41 podman[6625]: 2020-03-02 21:34:34.018181553 +0900 KST m=+0.173965114 container stop 7eac708eb65114d7f37773315f6565c718a440df3cc2211ace17e85cd9699bc3 (image=docker.io/library/redis:latest, name=redis_server) Mar 02 21:34:34 fastvm-centos-7-7-41 podman[6625]: 7eac708eb65114d7f37773315f6565c718a440df3cc2211ace17e85cd9699bc3 Mar 02 21:34:34 fastvm-centos-7-7-41 podman[6508]: time=\u0026#34;2020-03-02T21:34:34+09:00\u0026#34; level=error msg=\u0026#34;Error forwarding signal 15 to container 7eac708eb65114d7f37773315f6565c718a440df3cc2211ace17e85cd9699bc3\u0026#34;: can only kill running containers. 7eac708eb65114d7f37773315f65... Mar 02 21:34:34 fastvm-centos-7-7-41 systemd[1]: Stopped Redis container. Hint: Some lines were ellipsized, use -l to show in full.  [root@fastvm-centos-7-7-41 ~]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7eac708eb651 docker.io/library/redis:latest docker-entrypoint... 6 hours ago Exited (0) 6 seconds ago 0.0.0.0:6379-\u0026gt;6379/tcp redis_server kubernetes yaml generate 제공 Kubernetes 에서 바로 사용이 가능한 Yaml 파일 형식을 생성합니다.\n Yaml 파일 생성  [root@fastvm-centos-7-7-41 ~]# podman generate kube web # Generation of Kubernetes YAML is still under development! # # Save the output of this file and use kubectl create -f to import # it into Kubernetes. # # Created with podman-1.4.4 apiVersion: v1 kind: Pod metadata:  creationTimestamp: \u0026#34;2020-03-01T12:53:22Z\u0026#34;  labels:  app: web  name: web spec:  containers:  - command:  - httpd-foreground  env:  - name: PATH  value: /usr/local/apache2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin  - name: TERM  value: xterm  - name: HOSTNAME  - name: container  value: podman  - name: HTTPD_PATCHES  - name: HTTPD_PREFIX  value: /usr/local/apache2  - name: HTTPD_VERSION  value: 2.4.41  - name: HTTPD_SHA256  value: 133d48298fe5315ae9366a0ec66282fa4040efa5d566174481077ade7d18ea40  image: docker.io/library/httpd:latest  name: web  ports:  - containerPort: 80  hostPort: 80  protocol: TCP  resources: {}  securityContext:  allowPrivilegeEscalation: true  capabilities: {}  privileged: false  readOnlyRootFilesystem: false  stdin: true  tty: true  workingDir: /usr/local/apache2 status: {} Kubernetes 에 배포  [root@master1 ~]# kubectl create -f podman-gen-web.yml pod/web created  [root@master1 ~]# kubectl get pod NAME READY STATUS RESTARTS AGE web 1/1 Running 0 5s Pod 상세 내용 확인  [root@master1 ~]# kubectl describe pod web Name: web Namespace: default Priority: 0 Node: worker2.example.com/192.168.200.55 Start Time: Sun, 01 Mar 2020 21:57:52 +0900 Labels: app=web Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.233.95.5 Containers:  web:  Container ID: docker://b98767502847b34d4653a9a89dc7284b9307d5e02afe7ed1d4118bdf43e7689f  Image: docker.io/library/httpd:latest  Image ID: docker-pullable://httpd@sha256:946c54069130dbf136903fe658fe7d113bd8db8004de31282e20b262a3e106fb  Port: 80/TCP  Host Port: 80/TCP  Command:  httpd-foreground  State: Running  Started: Sun, 01 Mar 2020 21:57:56 +0900  Ready: True  Restart Count: 0  Environment:  PATH: /usr/local/apache2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin  TERM: xterm  HOSTNAME:  container: podman  HTTPD_PATCHES:  HTTPD_PREFIX: /usr/local/apache2  HTTPD_VERSION: 2.4.41  HTTPD_SHA256: 133d48298fe5315ae9366a0ec66282fa4040efa5d566174481077ade7d18ea40  Mounts:  /var/run/secrets/kubernetes.io/serviceaccount from default-token-lr7dh (ro) Conditions:  Type Status  Initialized True  Ready True  ContainersReady True  PodScheduled True Volumes:  default-token-lr7dh:  Type: Secret (a volume populated by a Secret)  SecretName: default-token-lr7dh  Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s  node.kubernetes.io/unreachable:NoExecute for 300s Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Normal Scheduled 100s default-scheduler Successfully assigned default/web to worker2.example.com  Normal Pulling 99s kubelet, worker2.example.com Pulling image \u0026#34;docker.io/library/httpd:latest\u0026#34;  Normal Pulled 96s kubelet, worker2.example.com Successfully pulled image \u0026#34;docker.io/library/httpd:latest\u0026#34;  Normal Created 96s kubelet, worker2.example.com Created container web  Normal Started 96s kubelet, worker2.example.com Started container web 참고 자료  https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/managing_containers/running_containers_as_systemd_services_with_podman https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/8/html/8.0_release_notes/notable_changes_to_containers https://podman.io/getting-started/ 사진 출처 1 : https://developers.redhat.com/blog/2019/02/21/podman-and-buildah-for-docker-users/ 사진 출처 2 : https://www.redhat.com/en/blog/why-red-hat-investing-cri-o-and-podman  ","permalink":"https://chhanz88.github.io/post/2020-03-02-podman/","summary":"Podman 이란? Red Hat Enterprise Linux 8 / CentOS 8 부터는 Docker 대신 Podman 이라는 도구를 제공합니다.\nPodman 은 Docker 와 동일하게 단일 노드에서 pod, 컨테이너 이미지 및 컨테이너를 관리합니다.\nPod 라고 하는 컨테이너 및 컨테이너 그룹을 관리할 수 있는 libpod 라이브러리를 기반으로 합니다.\n RHEL 8 Release Note  이번 포스팅에서는 Podman 의 설치 및 기본 사용법에 대해 확인 해보겠습니다.\nDocker VS Podman Docker 와 Podman 은 아래와 같이 \u0026ldquo;컨테이너 Cli 가 컨테이너를 어떻게 생성하냐\u0026rdquo; 의 차이가 있습니다.","title":"[Container] Podman 설치 및 사용법"},{"content":"목차  Gitlab-CE 설치 Jenkins 설치 Nexus Repository Manager 설치  Gitlab CE Gitlab 이란? 위키백과에서는 아래와 같이 설명하고 있습니다.\n깃랩(GitLab)은 깃랩 사(GitLab Inc.)가 개발한 위키와 이슈 추적 기능을 갖춘 웹 기반의 데브옵스 시스템으로써, 오픈 소스 라이선스 및 사유 소프트웨어 라이선스를 사용한다. 2019년 현재, 깃 저장소와 이슈 추적 기능을 가춘 유일한 단일 어플리케이션의 (Single Application) 데브옵스 솔루션이다. 시중에 유통되고 있는 많은 데브옵스 솔루션들은 자신들의 특화된 영역 이외는 API를 이용한 연동 만을 제공하지만 깃랩은 단일 어플리케이션으로써 데브옵스의 전 영역의 기능들을 모두 제공하고 있다.  출처: 위키백과-깃랩  이번 포스팅에서는 형상관리 도구로 사용되는 Gitlab 을 설치해보고 간단하게 사용 방법에 대해 알아보도록 하겠습니다.\nGitlab CE 설치 Gitlab 설치를 할 시스템은 아래와 같습니다.\n Memory 최소 4GB 이상 CentOS 7 Latest Version Disk 100GB 이상 Firewalld, SELinux Disabled. (필요에 따라 Enable 합니다.)  필수 패키지 설치 아래와 같이 필수 패키지를 설치합니다.\n[root@fastvm-centos-7-7-90 ~]# yum -y install wget bash-completion git policycoreutils-python Postfix 설치 및 설정 Gitlab system 에서 Notification 을 수행하기 위해 Postfix 를 설치하고 간단하게 설정합니다.\n[root@fastvm-centos-7-7-90 ~]# yum -y install postfix  [root@fastvm-centos-7-7-90 ~]# systemctl status postfix ● postfix.service - Postfix Mail Transport Agent  Loaded: loaded (/usr/lib/systemd/system/postfix.service; enabled; vendor preset: disabled)  Active: active (running) since Fri 2020-02-14 15:05:26 KST; 1min 51s ago  Main PID: 1605 (master)  CGroup: /system.slice/postfix.service  ├─1605 /usr/libexec/postfix/master -w  ├─1622 pickup -l -t unix -u  └─1623 qmgr -l -t unix -u  Feb 14 15:05:25 fastvm-centos-7-7-90 systemd[1]: Starting Postfix Mail Transport Agent... Feb 14 15:05:26 fastvm-centos-7-7-90 postfix/master[1605]: daemon started -- version 2.10.1, configuration /etc/postfix Feb 14 15:05:26 fastvm-centos-7-7-90 systemd[1]: Started Postfix Mail Transport Agent. [root@fastvm-centos-7-7-90 ~]# Postfix 설정 변경을 합니다.\n/etc/postfix/main.cf 를 수정합니다.\n## Example  myhostname = gitlab.example.com  mydomain = example.com  myorigin = $myhostname  inet_interfaces = all  mydestination = $myhostname, localhost.$mydomain, localhost, $mydomain  relay_domains = $mydestination 위 설정은 예제이며, 테스트 환경 또는 운영 환경에 맞게 설정합니다.\nGitlab CE 설치 시작 Gitlab CE repository 를 설치합니다.\n# curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash 아래와 같이 Gitlab 을 설치합니다.\n# EXTERNAL_URL=\u0026#34;https://gitlab.example.com\u0026#34; yum install -y gitlab-ce 설치가 완료되면 아래와 같이 gitlab-ctl 명령을 통해 서비스 상태를 확인 할 수 있습니다.\n[root@fastvm-centos-7-7-90 ~]# gitlab-ctl status run: alertmanager: (pid 4417) 152397s; run: log: (pid 3459) 152540s run: crond: (pid 4406) 152403s; run: log: (pid 4405) 152404s run: gitaly: (pid 4432) 152397s; run: log: (pid 2861) 152667s run: gitlab-exporter: (pid 4451) 152396s; run: log: (pid 3360) 152556s run: gitlab-workhorse: (pid 4465) 152396s; run: log: (pid 3094) 152594s run: grafana: (pid 4478) 152396s; run: log: (pid 3661) 152496s run: logrotate: (pid 9250) 1193s; run: log: (pid 3194) 152572s run: nginx: (pid 4496) 152395s; run: log: (pid 3124) 152588s run: node-exporter: (pid 4542) 152394s; run: log: (pid 3324) 152562s run: postgres-exporter: (pid 4586) 152394s; run: log: (pid 3482) 152535s run: postgresql: (pid 4593) 152393s; run: log: (pid 2924) 152661s run: prometheus: (pid 4602) 152393s; run: log: (pid 3423) 152546s run: redis: (pid 4616) 152393s; run: log: (pid 2725) 152673s run: redis-exporter: (pid 4621) 152392s; run: log: (pid 3384) 152552s run: registry: (pid 4628) 152392s; run: log: (pid 3298) 152565s run: sidekiq: (pid 4638) 152391s; run: log: (pid 3080) 152600s run: unicorn: (pid 4644) 152391s; run: log: (pid 3064) 152606s [root@fastvm-centos-7-7-90 ~]# Gitlab Web UI 접속 https://gitlab.example.com 으로 접속합니다.\n위와 같이 초기 접속시에는 root 관리자 비밀번호 설정을 합니다.\nGitlab 사용자 생성 Gitlab Project 생성 Mytestproject 를 생성 해보겠습니다.\n[root@fastvm-centos-7-7-90 ~]# git clone git@gitlab.example.com:chhanz/mytestproject.git Cloning into \u0026#39;mytestproject\u0026#39;... The authenticity of host \u0026#34;gitlab.example.com (192.168.200.90)\u0026#34; cant be established. ECDSA key fingerprint is SHA256:f1oXJM1Hqui9COg3tZROl/8z2qvr6fY6MhYdbhxNk0c. ECDSA key fingerprint is MD5:a7:16:d4:d2:46:90:31:e8:96:19:3a:f4:a1:39:f8:96. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added \u0026#34;gitlab.example.com,192.168.200.90\u0026#34; (ECDSA) to the list of known hosts. remote: Enumerating objects: 3, done. remote: Counting objects: 100% (3/3), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Receiving objects: 100% (3/3), done. [root@fastvm-centos-7-7-90 ~]# 위와 같이 소스를 clone 합니다.\n[root@fastvm-centos-7-7-90 mytestproject]# ls -la  total 4 drwxr-xr-x 3 root root 35 Feb 14 16:45 . dr-xr-x---. 5 root root 219 Feb 14 16:45 .. drwxr-xr-x 8 root root 163 Feb 14 16:45 .git -rw-r--r-- 1 root root 17 Feb 14 16:45 README.md [root@fastvm-centos-7-7-90 mytestproject]# [root@fastvm-centos-7-7-90 mytestproject]# echo \u0026#34;TEST :: COMMIT\u0026#34; \u0026gt;\u0026gt; README.md  [root@fastvm-centos-7-7-90 mytestproject]# cat README.md  # Mytestproject  TEST :: COMMIT [root@fastvm-centos-7-7-90 mytestproject]# 테스트를 위해 README.md 에 내용을 추가합니다.\n[root@fastvm-centos-7-7-90 mytestproject]# git status # On branch master # Changes not staged for commit: # (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) # (use \u0026#34;git checkout -- \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) # # modified: README.md # no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) [root@fastvm-centos-7-7-90 mytestproject]# git add . [root@fastvm-centos-7-7-90 mytestproject]# git commit -m \u0026#34;first commit \u0026#34; [master d4da160] first commit  1 file changed, 1 insertion(+) [root@fastvm-centos-7-7-90 mytestproject]# git push -u origin master Counting objects: 5, done. Writing objects: 100% (3/3), 271 bytes | 0 bytes/s, done. Total 3 (delta 0), reused 0 (delta 0) To git@gitlab.example.com:chhanz/mytestproject.git  f11547a..d4da160 master -\u0026gt; master [root@fastvm-centos-7-7-90 mytestproject]# 변경된 소스에 대해 commit 후, push 를 진행합니다.\nWeb IDE 제공 PR 기능 참고 자료  Gitlab 설치 : https://about.gitlab.com/install/#centos-7?version=ce  ","permalink":"https://chhanz88.github.io/post/2020-02-16-install-gitlab/","summary":"목차  Gitlab-CE 설치 Jenkins 설치 Nexus Repository Manager 설치  Gitlab CE Gitlab 이란? 위키백과에서는 아래와 같이 설명하고 있습니다.\n깃랩(GitLab)은 깃랩 사(GitLab Inc.)가 개발한 위키와 이슈 추적 기능을 갖춘 웹 기반의 데브옵스 시스템으로써, 오픈 소스 라이선스 및 사유 소프트웨어 라이선스를 사용한다. 2019년 현재, 깃 저장소와 이슈 추적 기능을 가춘 유일한 단일 어플리케이션의 (Single Application) 데브옵스 솔루션이다. 시중에 유통되고 있는 많은 데브옵스 솔루션들은 자신들의 특화된 영역 이외는 API를 이용한 연동 만을 제공하지만 깃랩은 단일 어플리케이션으로써 데브옵스의 전 영역의 기능들을 모두 제공하고 있다.","title":"[Devops] Gitlab CE 설치"},{"content":"목차  IBM Cloud Kubernetes Service 생성 IBM Cloud Cli 및 Kubectl 설치 Deploy Sample Service Delete Kubernetes Cluster 참고 자료  관리형 Kubernetes Service  각 Public Cloud Provider 에서는 관리형 Kubernetes Service를 제공하고 있습니다.\nAWS 에서는 Amazon Elastic Kubernetes Service(AWS EKS),\nAzure 에서는 Azure Kubernetes Service(AKS)의 이름으로 서비스를 하고 있으며,\nIBM 에서는 IBM Cloud Kubernetes Service(IKS) 라는 서비스를 제공하고 있습니다.\n 이번 포스팅에서는 무료로 관리형 Kubernetes 서비스를 이용 할 수 있는 IBM Cloud Kubernetes Service(이하 IKS) 를 이용하여\nCluster 를 생성하고 Sample 서비스를 배포해보도록 하겠습니다.\nIBM Cloud Kubernetes Service 생성 IKS 서비스 생성을 위해서는 IBM Cloud 계정 생성이 필요하며, 해당 계정에 신용카드 등록이 필요합니다.\n(신용카드 등록은 하지만 Free 요금제 사용으로 추가 과금은 없습니다.)\n또한 IKS Free 요금제의 경우, Cluster 의 사용 기간이 한달이며, 한달 후엔 Cluster 사용이 중지됩니다.\nIBM Cloud Cli 및 Kubectl 설치 IBM Cloud Cli 설치에 사용되는 운영체제는 CentOS 7.7 을 사용 하였습니다.\n IBM Cloud Cli 설치  $ curl -sL https://ibm.biz/idt-installer | bash 자동으로 필요한 패키지 설치합니다.\n설치가 완료된 후, ssh 재접속을 하면 ibmcloud(ic) 라는 명령어가 사용 가능합니다.\n[root@fastvm-centos-7-7-31 ~]# ic NAME:  ibmcloud - A command line tool to interact with IBM Cloud  Find more information at: https://ibm.biz/cli-docs  USAGE:  [environment variables] ibmcloud [global options] command [arguments...] [command options]  VERSION:  0.22.0+5ccbbea-2020-01-21T09:34:49+00:00  COMMANDS:  api Set or view target API endpoint  login Log user in  target Set or view the targeted region, account, resource group, org or space  config Write default values to the config  update Update CLI to the latest version  logout Log user out  regions List all the regions  version Print the version  resource Manage resource groups and resources  iam Manage identities and access to resources  dev Create, develop, deploy, and monitor applications  app [Deprecated] Manage Cloud Foundry applications and application related domains and routes.  service [Deprecated] Manage Cloud Foundry services.  billing Retrieve usage and billing information  plugin Manage plug-ins and plug-in repositories  cf Run Cloud Foundry CLI with IBM Cloud CLI context  catalog Manage catalog  account Manage accounts, users, orgs and spaces  enterprise Manage enterprise, account groups and accounts.  cfee Manage Cloud Foundry Enterprise Environments  cloud-functions, wsk, functions, fn Manage Cloud Functions  cos Interact with IBM Cloud Object Storage services  cr Manage IBM Cloud Container Registry content and configuration.  cs, ks, oc Manage IBM Cloud Kubernetes Service clusters.  sl Manage Classic infrastructure services  help, h Show help  Enter \u0026#39;ibmcloud help [command]\u0026#39; for more information about a command.  ENVIRONMENT VARIABLES:  IBMCLOUD_COLOR=false Do not colorize output  IBMCLOUD_ANALYTICS=false Do not collect usage statistics for analytics  IBMCLOUD_VERSION_CHECK=false Do not check latest version for update  IBMCLOUD_HTTP_TIMEOUT=5 A time limit for HTTP requests  IBMCLOUD_API_KEY=api_key_value API Key used for login  IBMCLOUD_TRACE=true Print API request diagnostics to stdout  IBMCLOUD_TRACE=path/to/trace.log Append API request diagnostics to a log file  IBMCLOUD_HOME=path/to/dir Path to config directory  GLOBAL OPTIONS:  --version, -v Print the version  --help, -h Show help  [root@fastvm-centos-7-7-31 ~]#  Login IBM Cloud\n아래 명령을 입력하여 Login 을 합니다.  $ ibmcloud login -a cloud.ibm.com -r us-south 아래와 같은 Login 과정이 필요합니다.\n[root@fastvm-centos-7-7-31 ~]# ibmcloud login -a cloud.ibm.com -r us-south API endpoint: https://cloud.ibm.com  Email\u0026gt; \u0026#34;계정명@이메일\u0026#34;  Password\u0026gt; \u0026#34;패스워드\u0026#34; Authenticating... OK  Targeted account CheolHee Han`s Account (\u0026#34;Account ID\u0026#34;) \u0026lt;-\u0026gt; \u0026#34;Account ID\u0026#34;  Targeted region us-south   API endpoint: https://cloud.ibm.com Region: us-south User: \u0026#34;계정명@이메일\u0026#34; Account: CheolHee Han`s Account (\u0026#34;Account ID\u0026#34;) \u0026lt;-\u0026gt; \u0026#34;Account ID\u0026#34; Resource group: No resource group targeted, use \u0026#39;ibmcloud target -g RESOURCE_GROUP\u0026#39; CF API endpoint: Org: Space:  Tip: If you are managing Cloud Foundry applications and services - Use \u0026#39;ibmcloud target --cf\u0026#39; to target Cloud Foundry org/space interactively, or use \u0026#39;ibmcloud target --cf-api ENDPOINT -o ORG -s SPACE\u0026#39; to target the org/space. - Use \u0026#39;ibmcloud cf\u0026#39; if you want to run the Cloud Foundry CLI with current IBM Cloud CLI context.  Kubeconfig 설정\n아래 명령을 입력하고 생성된 Export 명령을 입력합니다.  [root@fastvm-centos-7-7-31 ~]# ibmcloud ks cluster config --cluster \u0026#34;IKS Cluster ID\u0026#34; Kubernetes version 1.16 has removed deprecated APIs. For more information, see \u0026lt;http://ibm.biz/k8s-1-16-apis\u0026gt;  If you have clusters that run Kubernetes versions 1.11, 1.12 or 1.13, update them now to continue receiving important security updates and support. Kubernetes version 1.13 is deprecated and will be unsupported 22 February 2020. Versions 1.12 and earlier are already unsupported. For more information and update actions, see \u0026lt;https://ibm.biz/iks-versions\u0026gt;  WARNING: The behavior of this command in your current CLI version is deprecated, and becomes unsupported when CLI version 1.0 is released in March 2020. To use the new behavior now, set the \u0026#39;IKS_BETA_VERSION\u0026#39; environment variable. In bash, run \u0026#39;export IKS_BETA_VERSION=1\u0026#39;. Note: Changing the beta version can include other breaking changes. For more information, see http://ibm.biz/iks-cli-v1  OK The configuration for \u0026#34;IKS Cluster ID\u0026#34; was downloaded successfully.  Export environment variables to start using Kubernetes.  export KUBECONFIG=/root/.bluemix/plugins/container-service/clusters/\u0026#34;IKS Cluster ID\u0026#34;/kube-config-hou02-chhanz-kubecluster.yml  [root@fastvm-centos-7-7-31 ~]# export KUBECONFIG=/root/.bluemix/plugins/container-service/clusters/\u0026#34;IKS Cluster ID\u0026#34;/kube-config-hou02-chhanz-kubecluster.yml  Kubernetes 상태 확인\n아래와 같이 kubectl 명령을 통해 확인이 가능합니다.  [root@fastvm-centos-7-7-31 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION 10.76.68.233 Ready \u0026lt;none\u0026gt; 107s v1.15.8+IKS [root@fastvm-centos-7-7-31 ~]# Deploy Sample Service IKS 에 Web 서비스를 올려보도록 하겠습니다.\n sample-httpd 배포\n아래와 같이 sample-httpd Container 이미지를 IKS 에 배포합니다.  [root@fastvm-centos-7-7-31 ~]# kubectl run sample-httpd --image=han0495/sample-httpd  kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. deployment.apps/sample-httpd created  [root@fastvm-centos-7-7-31 ~]# kubectl get all NAME READY STATUS RESTARTS AGE pod/sample-httpd-86b74f6f64-7gcmj 1/1 Running 0 7s \u0026lt;\u0026lt; \u0026#39;Pod 배포 완료\u0026#39; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 172.21.0.1 \u0026lt;none\u0026gt; 443/TCP 33m  NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/sample-httpd 1/1 1 1 8s  NAME DESIRED CURRENT READY AGE replicaset.apps/sample-httpd-86b74f6f64 1 1 1 8s  Expose Service\n아래 명령을 통해 sample-httpd 를 외부에 서비스 가능하도록 svc 를 생성합니다.  [root@fastvm-centos-7-7-31 ~]# kubectl expose deployment.apps/sample-httpd --port=80 --type=NodePort service/sample-httpd exposed  [root@fastvm-centos-7-7-31 ~]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 172.21.0.1 \u0026lt;none\u0026gt; 443/TCP 37m sample-httpd NodePort 172.21.162.229 \u0026lt;none\u0026gt; 80:31154/TCP 5s \u0026lt;\u0026lt; \u0026#39;expose 완료\u0026#39;  접속 테스트\nIKS Cluster 메뉴에서 Worker nodes 에서 Public IP 확인이 가능합니다.  Delete Kubernetes Cluster 참고 자료  Source 자료  sample-httpd-github : https://github.com/chhanz/sample-httpd-example   Docker Image 정보  sample-httpd-dockerhub : https://hub.docker.com/r/han0495/sample-httpd    ","permalink":"https://chhanz88.github.io/post/2020-01-27-create-cluster-ibm-kubernetes-service/","summary":"목차  IBM Cloud Kubernetes Service 생성 IBM Cloud Cli 및 Kubectl 설치 Deploy Sample Service Delete Kubernetes Cluster 참고 자료  관리형 Kubernetes Service  각 Public Cloud Provider 에서는 관리형 Kubernetes Service를 제공하고 있습니다.\nAWS 에서는 Amazon Elastic Kubernetes Service(AWS EKS),\nAzure 에서는 Azure Kubernetes Service(AKS)의 이름으로 서비스를 하고 있으며,\nIBM 에서는 IBM Cloud Kubernetes Service(IKS) 라는 서비스를 제공하고 있습니다.\n 이번 포스팅에서는 무료로 관리형 Kubernetes 서비스를 이용 할 수 있는 IBM Cloud Kubernetes Service(이하 IKS) 를 이용하여","title":"[Kubernetes] IBM Cloud Kubernetes Service 를 Free Tier 로 사용해보자!"},{"content":"목차  Red Hat Virtualization Host 설치 Red Hat Virtualization Standalone Manager 설치 Red Hat Virtualization Host 추가 및 VM 생성  Red Hat Virtualization Host 추가 이전 포스트에서는 RHVM 을 설치하였습니다.\n첫번째 포스트에서 설치한 RHVH 를 RHVM 에 연결하여 Manager 에서 Hypervisor 를 관리 할 수 있도록 하겠습니다.\nRHVH 호스트 추가 Storage Domain 추가 RHV 상태 확인 VM 생성 참고 문서  https://access.redhat.com/documentation/ko-kr/red_hat_virtualization/4.1/html/installation_guide/index https://virt-manager.org/download/ https://rizvir.com/articles/ovirt-mac-console/  ","permalink":"https://chhanz88.github.io/post/2020-01-18-install-rhvm-admin/","summary":"목차  Red Hat Virtualization Host 설치 Red Hat Virtualization Standalone Manager 설치 Red Hat Virtualization Host 추가 및 VM 생성  Red Hat Virtualization Host 추가 이전 포스트에서는 RHVM 을 설치하였습니다.\n첫번째 포스트에서 설치한 RHVH 를 RHVM 에 연결하여 Manager 에서 Hypervisor 를 관리 할 수 있도록 하겠습니다.\nRHVH 호스트 추가 Storage Domain 추가 RHV 상태 확인 VM 생성 참고 문서  https://access.redhat.com/documentation/ko-kr/red_hat_virtualization/4.1/html/installation_guide/index https://virt-manager.org/download/ https://rizvir.com/articles/ovirt-mac-console/  ","title":"[RHV] Red Hat Virtualization Host 추가 및 VM 생성"},{"content":"목차  Red Hat Virtualization Host 설치 Red Hat Virtualization Standalone Manager 설치 Red Hat Virtualization Host 추가 및 VM 생성  Red Hat Virtualization Manager 요구 사항 Red Hat Virtualization Manager(이하 RHVM)은 아래와 같은 하드웨어 요구 사항이 있습니다.\n참고 자료 : RHV Document\nRed Hat Virtualization Manager 설치 RHVM은 설치 방법이 두가지가 있습니다.\nStandalone 방식, Self-Hosted Engine 방식이 있습니다.\n이 방식에 대해 간단히 설명을 하면,\n Standalone : RHVM 단독 시스템(baremetal)\nSelf-Hosted Engine : RHVM을 Hypervisior 에 VM으로 배포하고 기동하는 방식\n(VMware vCenter Appliance 설치 방식과 동일한 개념입니다.)\n 이번 테스트는 Standalone 으로 설치 예정입니다.\n테스트 시스템 기본 설정 내용  /etc/hosts 설정\n이번 테스트 시스템은 내부 테스트용으로 DNS 미사용으로 인해 /etc/hosts에 아래와 같이 RHVM 설치에 필요한 DNS 정보를 입력합니다.  [root@rhvm ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6  # RHV 192.168.10.225 rhvm.test.com 192.168.10.226 rhvh.test.com # Test-Infra 192.168.10.228 nfs.test.com 192.168.10.229 repo.test.com [root@rhvm ~]# RHV Subscription 등록 및 Repository 연결  [root@rhvm ~]# yum repolist Loaded plugins: search-disabled-repos repo id repo name status jb-eap-7.1-for-rhel-7-server-rpms jb-eap-7.1-for-rhel-7-server-rpms 1,783 rhel-7-server-rhv-4-tools-rpms rhel-7-server-rhv-4-tools-rpms 45 rhel-7-server-rhv-4.1-manager-rpms rhel-7-server-rhv-4.1-manager-rpms 762 rhel-7-server-rhv-4.1-rpms rhel-7-server-rhv-4.1-rpms 773 rhel-7-server-rpms rhel-7-server-rpms 26,742 rhel-7-server-supplementary-rpms rhel-7-server-supplementary-rpms 351 repolist: 30,456 [root@rhvm ~]# firewalld 및 SELinux 설정\nfirewalld 및 SELinux 설정은 Enable \u0026amp; Enforcing 이 권고이나, 이번 테스트에서는 Disable 하겠습니다.  RHVM 설치  RHVM Package 설치  $ yum -y install rhevm  RHVM Engine 설치 시작  $ engine-setup 설치 과정 아래와 같이 어떻게 설치를 할 것인지 대화형 방식으로 질의하며 설치 진행 됩니다.\n[root@rhvm ~]# engine-setup [ INFO ] Stage: Initializing [ INFO ] Stage: Environment setup  Configuration files: [\u0026#39;/etc/ovirt-engine-setup.conf.d/10-packaging-wsp.conf\u0026#39;, \u0026#39;/etc/ovirt-engine-setup.conf.d/10-packaging.conf\u0026#39;]  Log file: /var/log/ovirt-engine/setup/ovirt-engine-setup-20200117235529-j88sh2.log  Version: otopi-1.6.3 (otopi-1.6.3-1.el7ev) [ INFO ] Stage: Environment packages setup [ INFO ] Stage: Programs detection [ INFO ] Stage: Environment setup [ INFO ] Stage: Environment customization   --== PRODUCT OPTIONS ==--   Configure Engine on this host (Yes, No) [Yes]: \u0026#34;Yes 입력\u0026#34;  Configure Image I/O Proxy on this host? (Yes, No) [Yes]: \u0026#34;Yes 입력\u0026#34;  Configure WebSocket Proxy on this host (Yes, No) [Yes]: \u0026#34;Yes 입력\u0026#34;  Please note: Data Warehouse is required for the engine.  If you choose to not configure it on this host, you have to configure it on a remote host,  and then configure the engine on this host so that it can access the database of the remote Data Warehouse host.  Configure Data Warehouse on this host (Yes, No) [Yes]: \u0026#34;Yes 입력\u0026#34;  Configure VM Console Proxy on this host (Yes, No) [Yes]: \u0026#34;Yes 입력\u0026#34;   --== PACKAGES ==--  [ INFO ] Checking for product updates... [ INFO ] No product updates found   --== NETWORK CONFIGURATION ==--   Host fully qualified DNS name of this server [rhvm.test.com]: \u0026#34;rhvm.test.com 입력\u0026#34;  --== DATABASE CONFIGURATION ==--   Where is the DWH database located? (Local, Remote) [Local]: \u0026lt;Local 입력\u0026gt;  Setup can configure the local postgresql server automatically for the DWH to run. This may conflict with existing applications.  Would you like Setup to automatically configure postgresql and create DWH database, or prefer to perform that manually? (Automatic, Manual) [Automatic]: \u0026#34;Automatic 입력\u0026#34;  Where is the Engine database located? (Local, Remote) [Local]: \u0026lt;Local 입력\u0026gt;  Setup can configure the local postgresql server automatically for the engine to run. This may conflict with existing applications.  Would you like Setup to automatically configure postgresql and create Engine database, or prefer to perform that manually? (Automatic, Manual) [Automatic]: \u0026#34;Automatic 입력\u0026#34;   --== OVIRT ENGINE CONFIGURATION ==--   Engine admin password: \u0026#34;Password 입력\u0026#34;  Confirm engine admin password: \u0026#34;Password 입력\u0026#34; [WARNING] Password is weak: it is too short  Use weak password? (Yes, No) [No]: \u0026#34;Yes 입력\u0026#34;  Application mode (Virt, Gluster, Both) [Both]: \u0026#34;Both 입력\u0026#34;   --== STORAGE CONFIGURATION ==--   Default SAN wipe after delete (Yes, No) [No]: \u0026#34;No 입력\u0026#34;   --== PKI CONFIGURATION ==--   Organization name for certificate [test.com]: \u0026#34;test.com 입력\u0026#34;   --== APACHE CONFIGURATION ==--   Setup can configure the default page of the web server to present the application home page. This may conflict with existing applications.  Do you wish to set the application as the default page of the web server? (Yes, No) [Yes]: \u0026#34;Yes 입력\u0026#34;  Setup can configure apache to use SSL using a certificate issued from the internal CA.  Do you wish Setup to configure that, or prefer to perform that manually? (Automatic, Manual) [Automatic]: \u0026#34;Automatic 입력\u0026#34;   --== SYSTEM CONFIGURATION ==--   Configure an NFS share on this server to be used as an ISO Domain? (Yes, No) [No]: \u0026#34;No 입력\u0026#34;   --== MISC CONFIGURATION ==--   Please choose Data Warehouse sampling scale:  (1) Basic  (2) Full  (1, 2)[1]: \u0026#34;1 선택\u0026#34;   --== END OF CONFIGURATION ==--  [ INFO ] Stage: Setup validation   --== CONFIGURATION PREVIEW ==--   Application mode : both  Default SAN wipe after delete : False  Update Firewall : False  Host FQDN : rhvm.test.com  Configure local Engine database : True  Set application as default page : True  Configure Apache SSL : True  Engine database secured connection : False  Engine database user name : engine  Engine database name : engine  Engine database host : localhost  Engine database port : 5432  Engine database host name validation : False  Engine installation : True  PKI organization : test.com  DWH installation : True  DWH database secured connection : False  DWH database host : localhost  DWH database user name : ovirt_engine_history  DWH database name : ovirt_engine_history  DWH database port : 5432  DWH database host name validation : False  Configure local DWH database : True  Configure Image I/O Proxy : True  Configure VMConsole Proxy : True  Configure WebSocket Proxy : True   Please confirm installation settings (OK, Cancel) [OK]: \u0026#34;OK 입력 후, 설치 시작\u0026#34;  [ INFO ] Stage: Transaction setup [ INFO ] Stopping engine service [ INFO ] Stopping ovirt-fence-kdump-listener service [ INFO ] Stopping dwh service [ INFO ] Stopping Image I/O Proxy service [ INFO ] Stopping vmconsole-proxy service [ INFO ] Stopping websocket-proxy service [ INFO ] Stage: Misc configuration [ INFO ] Stage: Package installation [ INFO ] Stage: Misc configuration [ INFO ] Upgrading CA [ INFO ] Initializing PostgreSQL [ INFO ] Creating PostgreSQL \u0026#39;engine\u0026#39; database [ INFO ] Configuring PostgreSQL [ INFO ] Creating PostgreSQL \u0026#39;ovirt_engine_history\u0026#39; database [ INFO ] Configuring PostgreSQL [ INFO ] Creating CA [ INFO ] Creating/refreshing Engine database schema [ INFO ] Creating/refreshing DWH database schema [ INFO ] Configuring Image I/O Proxy [ INFO ] Setting up ovirt-vmconsole proxy helper PKI artifacts [ INFO ] Setting up ovirt-vmconsole SSH PKI artifacts [ INFO ] Configuring WebSocket Proxy [ INFO ] Creating/refreshing Engine \u0026#39;internal\u0026#39; domain database schema [ INFO ] Generating post install configuration file \u0026#39;/etc/ovirt-engine-setup.conf.d/20-setup-ovirt-post.conf\u0026#39; [ INFO ] Stage: Transaction commit [ INFO ] Stage: Closing up [ INFO ] Starting engine service [ INFO ] Starting dwh service [ INFO ] Restarting ovirt-vmconsole proxy service   --== SUMMARY ==--  [ INFO ] Restarting httpd  In order to configure firewalld, copy the files from  /etc/ovirt-engine/firewalld to /etc/firewalld/services  and execute the following commands:  firewall-cmd --permanent --add-service ovirt-postgres  firewall-cmd --permanent --add-service ovirt-https  firewall-cmd --permanent --add-service ovirt-fence-kdump-listener  firewall-cmd --permanent --add-service ovirt-imageio-proxy  firewall-cmd --permanent --add-service ovirt-websocket-proxy  firewall-cmd --permanent --add-service ovirt-http  firewall-cmd --permanent --add-service ovirt-vmconsole-proxy  firewall-cmd --reload  The following network ports should be opened:  tcp:2222  tcp:443  tcp:5432  tcp:54323  tcp:6100  tcp:80  udp:7410  An example of the required configuration for iptables can be found at:  /etc/ovirt-engine/iptables.example  Please use the user \u0026#39;admin@internal\u0026#39; and password specified in order to login  Web access is enabled at:  http://rhvm.test.com:80/ovirt-engine  https://rhvm.test.com:443/ovirt-engine  Internal CA 8D:97:91:6E:9B:BD:60:A1:96:2E:5B:26:4E:43:90:04:AF:2D:19:D0  SSH fingerprint: SHA256:6hFsMoHw32C85HfGc/N+GMv4SieCKv4PtBSgz43Jr3I   --== END OF SUMMARY ==--  [ INFO ] Stage: Clean up  Log file is located at /var/log/ovirt-engine/setup/ovirt-engine-setup-20200117235529-j88sh2.log [ INFO ] Generating answer file \u0026#39;/var/lib/ovirt-engine/setup/answers/20200117235943-setup.conf\u0026#39; [ INFO ] Stage: Pre-termination [ INFO ] Stage: Termination [ INFO ] Execution of setup completed successfully 설치에는 약 20분 정도 소요가 됩니다.(설치 환경에 따라 차이 있음.)\nRHVM 서비스 확인 아래와 같이 RHVM Engine 의 서비스 상태를 확인 할 수 있습니다.\n[root@rhvm ~]# systemctl status ovirt-engine ● ovirt-engine.service - oVirt Engine  Loaded: loaded (/usr/lib/systemd/system/ovirt-engine.service; enabled; vendor preset: disabled)  Active: active (running) since Fri 2020-01-17 23:59:41 KST; 4min 30s ago  Main PID: 19043 (ovirt-engine.py)  CGroup: /system.slice/ovirt-engine.service  ├─19043 /usr/bin/python /usr/share/ovirt-engine/services/ovirt-engine/ovirt-engine.py --redirect-output --systemd=notify s...  └─19112 ovirt-engine -server -XX:+TieredCompilation -Xms3971M -Xmx3971M -Djava.awt.headless=true -Dsun.rmi.dgc.client.gcIn...  Jan 17 23:59:40 rhvm.test.com systemd[1]: Starting oVirt Engine... Jan 17 23:59:40 rhvm.test.com ovirt-engine.py[19043]: 2020-01-17 23:59:40,871+0900 ovirt-engine: INFO _detectJBossVersion:187 D...0\u0026#39;, \u0026#39;- Jan 17 23:59:41 rhvm.test.com ovirt-engine.py[19043]: 2020-01-17 23:59:41,651+0900 ovirt-engine: INFO _detectJBossVersion:207 R...: \u0026#39;[]\u0026#39; Jan 17 23:59:41 rhvm.test.com systemd[1]: Started oVirt Engine. Hint: Some lines were ellipsized, use -l to show in full. RHVM Web Console 접속 아래와 같이 https://rhvm.test.com:443/ovirt-engine 를 통해 접근이 가능합니다.\n관리 포탈 을 선택합니다.\n설치 과정에서 입력한 admin 계정의 패스워드를 입력합니다.\n아래와 같이 RHVM 에 접속 할 수 있습니다.\n참고자료  https://access.redhat.com/documentation/ko-kr/red_hat_virtualization/4.1/html/installation_guide/part-installing_red_hat_enterprise_virtualization  ","permalink":"https://chhanz88.github.io/post/2020-01-17-install-rhvm/","summary":"목차  Red Hat Virtualization Host 설치 Red Hat Virtualization Standalone Manager 설치 Red Hat Virtualization Host 추가 및 VM 생성  Red Hat Virtualization Manager 요구 사항 Red Hat Virtualization Manager(이하 RHVM)은 아래와 같은 하드웨어 요구 사항이 있습니다.\n참고 자료 : RHV Document\nRed Hat Virtualization Manager 설치 RHVM은 설치 방법이 두가지가 있습니다.\nStandalone 방식, Self-Hosted Engine 방식이 있습니다.\n이 방식에 대해 간단히 설명을 하면,\n Standalone : RHVM 단독 시스템(baremetal)","title":"[RHV] Red Hat Virtualization Standalone Manager 설치"},{"content":"목차  Red Hat Virtualization Host 설치 Red Hat Virtualization Standalone Manager 설치 Red Hat Virtualization Host 추가 및 VM 생성  Red Hat Virtualization Host 란? RHVH (Red Hat Virtualization Host)는 Red Hat Enterprise Linux 기반의 최소 운영 체제이며 Red Hat Virtualization 환경에서 하이퍼바이저 역할을 하는 물리적 시스템을 간단하게 설정할 수 있도록 설계되었습니다.\n참고 자료 : RHV Document\nRHVH 설치 준비된 RHVH 4.1 DVD 이미지를 통해 부팅을 합니다.\n언어를 선택합니다.\n아래 화면은 RHVH 설치 화면입니다.\n기본적으로 DATE \u0026amp; TIME, KEYBOARD, INSTALLATION DESTNITION 을 설정합니다.\nDATE \u0026amp; TIME 을 설정합니다.\nKEYBOARD 를 설정합니다.\nINSTALLATION DESTNITION 를 설정합니다.\n필요에 따라 Network 및 Hostname 설정을 합니다.\n설치 준비가 완료되면 설치를 시작합니다.\nroot패스워드를 설정합니다.\n설치가 완료되면 재부팅을 합니다.\nRHVH 가 부팅이 완료되면 nodectl check 라는 명령을 통해 시스템 상태 확인이 가능합니다.\n참고문서  https://access.redhat.com/documentation/ko-kr/red_hat_virtualization/4.1/html/installation_guide/red_hat_virtualization_hosts  ","permalink":"https://chhanz88.github.io/post/2020-01-03-install-rhvh/","summary":"목차  Red Hat Virtualization Host 설치 Red Hat Virtualization Standalone Manager 설치 Red Hat Virtualization Host 추가 및 VM 생성  Red Hat Virtualization Host 란? RHVH (Red Hat Virtualization Host)는 Red Hat Enterprise Linux 기반의 최소 운영 체제이며 Red Hat Virtualization 환경에서 하이퍼바이저 역할을 하는 물리적 시스템을 간단하게 설정할 수 있도록 설계되었습니다.\n참고 자료 : RHV Document\nRHVH 설치 준비된 RHVH 4.1 DVD 이미지를 통해 부팅을 합니다.\n언어를 선택합니다.","title":"[RHV] Red Hat Virtualization Host 설치"},{"content":"KVM nested virtualization KVM nested virtualization 를 구성하여 VM CPU 가상화를 활성화 할 수 있습니다.\n이와 같이 구성 할 경우, 가상화 VM 내에서 한번더 가상화 구성이 가능합니다.\nkvm-intel module 설정 아래와 같이 kvm-intel module parameter 를 추가합니다.\n[root@kvm ~]# cat /etc/modprobe.d/kvm-nested-module.conf options kvm-intel nested=1 이후 시스템을 재부팅하고 KVM nested virtualization 구성이 되었는지 확인합니다.\n[root@kvm ~]# cat /sys/module/kvm_intel/parameters/nested Y kvm-intel module 상세 확인 [root@kvm ~]# modinfo kvm-intel filename: /lib/modules/3.10.0-1062.4.3.el7.x86_64/kernel/arch/x86/kvm/kvm-intel.ko.xz license: GPL author: Qumranet retpoline: Y rhelversion: 7.7 srcversion: B7943A94D7B227C40ACD718 alias: x86cpu:vendor:*:family:*:model:*:feature:*0085* depends: kvm intree: Y vermagic: 3.10.0-1062.4.3.el7.x86_64 SMP mod_unload modversions signer: CentOS Linux kernel signing key sig_key: 3F:59:AE:15:77:D5:87:23:18:4F:6D:BA:B1:8A:D5:F5:9D:E4:1D:39 sig_hashalgo: sha256 parm: vpid:bool parm: flexpriority:bool parm: ept:bool parm: unrestricted_guest:bool parm: eptad:bool parm: emulate_invalid_guest_state:bool parm: vmm_exclusive:bool parm: fasteoi:bool parm: enable_apicv:bool parm: enable_shadow_vmcs:bool parm: nested:bool parm: pml:bool parm: preemption_timer:bool parm: ple_gap:uint parm: ple_window:uint parm: ple_window_grow:uint parm: ple_window_shrink:uint parm: ple_window_max:uint VM 가상화 활성화 아래와 같이 virt-manager 에서 CPU 메뉴에서 Copy Host CPU Configuration 을 선택합니다.\n이후 해당 VM 에서 가상화 활성화 여부를 확인합니다.\n참고 문서  https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/sect-nested_virt_setup https://docs.fedoraproject.org/en-US/quick-docs/using-nested-virtualization-in-kvm/  ","permalink":"https://chhanz88.github.io/post/2019-12-30-enable-kvm-nested-virtualization/","summary":"KVM nested virtualization KVM nested virtualization 를 구성하여 VM CPU 가상화를 활성화 할 수 있습니다.\n이와 같이 구성 할 경우, 가상화 VM 내에서 한번더 가상화 구성이 가능합니다.\nkvm-intel module 설정 아래와 같이 kvm-intel module parameter 를 추가합니다.\n[root@kvm ~]# cat /etc/modprobe.d/kvm-nested-module.conf options kvm-intel nested=1 이후 시스템을 재부팅하고 KVM nested virtualization 구성이 되었는지 확인합니다.\n[root@kvm ~]# cat /sys/module/kvm_intel/parameters/nested Y kvm-intel module 상세 확인 [root@kvm ~]# modinfo kvm-intel filename: /lib/modules/3.10.0-1062.4.3.el7.x86_64/kernel/arch/x86/kvm/kvm-intel.ko.xz license: GPL author: Qumranet retpoline: Y rhelversion: 7.","title":"[Linux] KVM nested virtualization"},{"content":"noVNC 구성 noVNC 란? noVNC는 HTML VNC 클라이언트 프로그램입니다.\nnoVNC 구성  install requirement package  $ yum -y groupinstall \u0026#34;GNOME Desktop\u0026#34; $ yum -y install epel-release  install noVNC package  $ yum -y install novnc python-websockify numpy tigervnc-server  start service vncserver  $ vncserver :1 vncserver 가 시작되면 vnc 접속용 password 입력을 요구합니다. 해당 password 는 noVNC 에 접근 할 때 필요한 password 입니다.\n start service websockify  $ websockify -D --web=/usr/share/novnc/ 6080 localhost:5901 위와 같이 명령을 수행하면 6080 포트로 noVNC 서비스가 작동합니다.\nnoVNC 이용 noVNC 가 설치된 서버의 아이피로 접속합니다.\n ex) http://192.168.100.100:6080\n 아래와 같이 접속이 되며 vnc password 를 입력을 요청합니다.\nvnc password 를 입력하면 아래와 같이 VNC Viewer 클라이언트 프로그램 없이 웹브라우저를 통해 VNC 접속이 가능합니다.\n참고 자료  noVNC : https://github.com/novnc/noVNC  ","permalink":"https://chhanz88.github.io/post/2019-12-13-linux-configure-novnc/","summary":"noVNC 구성 noVNC 란? noVNC는 HTML VNC 클라이언트 프로그램입니다.\nnoVNC 구성  install requirement package  $ yum -y groupinstall \u0026#34;GNOME Desktop\u0026#34; $ yum -y install epel-release  install noVNC package  $ yum -y install novnc python-websockify numpy tigervnc-server  start service vncserver  $ vncserver :1 vncserver 가 시작되면 vnc 접속용 password 입력을 요구합니다. 해당 password 는 noVNC 에 접근 할 때 필요한 password 입니다.\n start service websockify  $ websockify -D --web=/usr/share/novnc/ 6080 localhost:5901 위와 같이 명령을 수행하면 6080 포트로 noVNC 서비스가 작동합니다.","title":"[Linux] noVNC 구성"},{"content":"OpenShift 3.X 에서는 minishift 를 Laptop 에 배포하여 OpenShift 를 체험하고 테스트 간단하게 할 수 있습니다.\n그런데 \u0026quot; OpenShift 4.X 에서는 minishift 와 같은 테스트 환경을 구축 할 수 없을까? \u0026ldquo; 라는 생각에서 검색을 시작했습니다.\nOpenShift 4 on Laptop OpenShift 4 부터는 Red Hat OpenShift Cluster Manager 에서 각종 환경에 배포 할 수 있는 가이드를 제공하고 있습니다.\n저는 Linux 를 선택하고 libvirt 를 사용하는 KVM 환경에 배포 해보도록 하겠습니다.\nInstall crc command $ https://mirror.openshift.com/pub/openshift-v4/clients/crc/latest/crc-linux-amd64.tar.xz $ xz -d crc-linux-amd64.tar.xz;tar xvf crc-linux-amd64.tar $ sudo cp crc-linux-1.1.0-amd64/crc /usr/bin/ 위 과정을 통해 crc 명령을 설치합니다. 그리고 crc 명령을 통해 OpenShift 를 설치합니다.\ncrc setup 다음은 OpenShift 4 의 GUI 에 접근하여 테스트를 해보고 OpenShift 4 를 경험해 보겠습니다.\nOpenShift4 Web Console 이용하기 OpenShift Web Console 로 접근하여 git source 를 이용하여 서비스 배포를 해보겠습니다. ( S2I 기능 테스트 )\nSource To Image(S2I) 참고 자료  https://github.com/chhanz/sample-httpd-example https://cloud.redhat.com/openshift/install/crc/installer-provisioned  ","permalink":"https://chhanz88.github.io/post/2019-11-29-openshift4-dev-env/","summary":"OpenShift 3.X 에서는 minishift 를 Laptop 에 배포하여 OpenShift 를 체험하고 테스트 간단하게 할 수 있습니다.\n그런데 \u0026quot; OpenShift 4.X 에서는 minishift 와 같은 테스트 환경을 구축 할 수 없을까? \u0026ldquo; 라는 생각에서 검색을 시작했습니다.\nOpenShift 4 on Laptop OpenShift 4 부터는 Red Hat OpenShift Cluster Manager 에서 각종 환경에 배포 할 수 있는 가이드를 제공하고 있습니다.\n저는 Linux 를 선택하고 libvirt 를 사용하는 KVM 환경에 배포 해보도록 하겠습니다.\nInstall crc command $ https://mirror.","title":"[OpenShift] KVM 환경에 OpenShift 4 구동해보자!"},{"content":"[Linux] Squid 를 이용한 Proxy 서버 구성  주로 on-premise 환경에서 운영되는 시스템은 인터넷이 안되는 시스템이 많습니다.\n이런 상황이다 보니, Yum 을 통한 Package 관리가 쉽게 되지 않습니다.\n그렇다고 모든 시스템은 인터넷이 가능하게 만들어 보안에 취약해지는 환경을 만들수는 없습니다.\n아래와 같이 Proxy 서버를 이용하여 하나의 시스템을 통해 모든 시스템이 인터넷이 가능하도록 구성 할 수 있습니다.\n주 된 목적은 Yum 을 통해 Package 관리가 가능하도록 구성하는 것이 목적입니다.\n 목표 구성도 위와 같이 Server Farm 의 시스템은 인터넷이 안되는 환경입니다.\n그 시스템이 Internet Zone 의 Internet 사용이 가능한 시스템을 통해 외부 Yum Package 를 가져오는 환경입니다.\nInstall Squid 다음 명령을 통해 Squid Package 를 설치합니다.\n$ yum -y install squid Configuration Squid Config file 아래와 같이 Proxy 정책을 설정합니다.\n(아래 샘플은 모든 정책을 활성화 하였습니다.)\n$ vi /etc/squid/squid.conf ... 생략 ... # Only allow cachemgr access from localhost #http_access allow localhost manager #http_access deny manager http_access allow all # 모든 IP 에 대해 Allow 정책 추가 # And finally deny all other access to this proxy #http_access deny all # deny 정책 해제 ... 생략 ... # Squid normally listens to port 3128 http_port 8080 # Proxy 로 사용하길 원하는 Port 지정 #http_port 3128 Squid 서비스 시작 Squid 서비스를 시작합니다.\n$ systemctl start squid $ systemctl status squid 정상적으로 Proxy 서비스가 안된다고 판단되는 경우, 아래와 같이 커널 파라미터를 추가합니다.\nnet.ipv6.conf.all.disable_ipv6 = 1 net.ipv4.ip_forward = 1 Proxy 서비스 테스트  Server Farm 내부 시스템 : fastvm-centos-7-5-150  [root@fastvm-centos-7-5-150 ~]# yum repolist Loaded plugins: fastestmirror Determining fastest mirrors Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=7\u0026amp;arch=x86_64\u0026amp;repo=os\u0026amp;infra=stock error was 14: curl#6 - \u0026#34;Could not resolve host: mirrorlist.centos.org; Unknown error\u0026#34; Loading mirror speeds from cached hostfile Loading mirror speeds from cached hostfile Loading mirror speeds from cached hostfile repo id repo name status base/7/x86_64 CentOS-7 - Base 0 extras/7/x86_64 CentOS-7 - Extras 0 updates/7/x86_64 CentOS-7 - Updates 0 repolist: 0 [root@fastvm-centos-7-5-150 ~]# 위와 같이 Yum 을 사용 할 수 없습니다.\nyum.conf 수정 /etc/yum.conf 파일에 아래와 같이 proxy 옵션을 추가합니다.\n[root@fastvm-centos-7-5-150 ~]# cat /etc/yum.conf [main] cachedir=/var/cache/yum/$basearch/$releasever keepcache=0 debuglevel=2 logfile=/var/log/yum.log exactarch=1 obsoletes=1 gpgcheck=1 plugins=1 installonly_limit=5 bugtracker_url=http://bugs.centos.org/set_project.php?project_id=23\u0026amp;ref=http://bugs.centos.org/bug_report_page.php?category=yum distroverpkg=centos-release proxy=http://192.168.200.151:8080 # Proxy 서버 정보 추가 proxy 옵션 추가 후, Yum 사용이 가능한지 확인합니다.\n[root@fastvm-centos-7-5-150 ~]# yum repolist Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile  * base: data.aonenetworks.kr  * extras: data.aonenetworks.kr  * updates: data.aonenetworks.kr base | 3.6 kB 00:00:00 extras | 2.9 kB 00:00:00 updates | 2.9 kB 00:00:00 (1/4): base/7/x86_64/group_gz | 165 kB 00:00:00 (2/4): extras/7/x86_64/primary_db | 153 kB 00:00:00 (3/4): updates/7/x86_64/primary_db | 2.8 MB 00:00:00 (4/4): base/7/x86_64/primary_db | 6.0 MB 00:00:00 repo id repo name status base/7/x86_64 CentOS-7 - Base 10,097 extras/7/x86_64 CentOS-7 - Extras 305 updates/7/x86_64 CentOS-7 - Updates 711 repolist: 11,113 [root@fastvm-centos-7-5-150 ~]#  위와 같이 Proxy 서버 를 통해 인터넷을 활용하는 것을 확인 할 수 있습니다.\n추가로 위와 같이 구성 할 경우, Yum 만 인터넷이 사용이 가능하며 그 외 부분은 인터넷 사용이 불가능합니다.\n[root@fastvm-centos-7-5-150 ~]# ping 1.1.1.1 connect: Network is unreachable [root@fastvm-centos-7-5-150 ~]# ","permalink":"https://chhanz88.github.io/post/2019-11-12-configuration-squid-proxy/","summary":"[Linux] Squid 를 이용한 Proxy 서버 구성  주로 on-premise 환경에서 운영되는 시스템은 인터넷이 안되는 시스템이 많습니다.\n이런 상황이다 보니, Yum 을 통한 Package 관리가 쉽게 되지 않습니다.\n그렇다고 모든 시스템은 인터넷이 가능하게 만들어 보안에 취약해지는 환경을 만들수는 없습니다.\n아래와 같이 Proxy 서버를 이용하여 하나의 시스템을 통해 모든 시스템이 인터넷이 가능하도록 구성 할 수 있습니다.\n주 된 목적은 Yum 을 통해 Package 관리가 가능하도록 구성하는 것이 목적입니다.\n 목표 구성도 위와 같이 Server Farm 의 시스템은 인터넷이 안되는 환경입니다.","title":"[Linux] Squid 를 이용한 Proxy 서버 구성"},{"content":"Converting VMware ESXi Linux VM to KVM (virt-v2v)  1. Install package virt-v2v $ yum install virt-v2v 위와 같이 virt-v2v package 를 설치 합니다.\n2. ESXi - KVM 간 ssh key 인증 설정 KVM 호스트에서 ESXi 의 데이터스토어에 Password 없이 접근이 가능하도록 설정을 해야 virt-v2v 를 이용하여 VM 을 이관 할 수 있습니다.\n+ VMware ESXi ssh Daemon 시작 위와 같이 ESXi Web Console 에서 작업 \u0026gt; 서비스 \u0026gt; SSH 사용 선택\n+ KVM, ssh-keygen 명령 수행 [root@testh ~]# ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:bj7DLUsbaRSeVu299zzPOKKpuK88cnjZi0UxxZSSxqE root@testh.example.com The key`s randomart image is: +---[RSA 2048]----+ | ..=o. | | .=.o. | | E.+.. . | | . * . . | | S . . | | = . . | | . +O. . .| | o.=O*o... .*.| | +*=BBo. ...B| +----[SHA256]-----+ [root@testh ~]#  생성된 key 확인 진행  [root@testh ~]# cat /root/.ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDYVu7UYN39ue2UqAlH1D+wNTnKM6z8/JloAG0Om8w9vUTGuowXLHTRV9CGIcQ4NfGiUI/lqI2XV+ZY4XeGCE2H5LWDagRGCutJ9vh5/D3FGIEMOGez7qXdJm8/3xrhRa56rY9ie1NFGLpoi9Lkka/6Z48CJ20vvb9vJWZdX91WGkT8G0L5nv3B3JzCrY4VxYOYmxTwmu13DOeW0bbLs08pWWrbL10cIIP76NhZJG98wulQQYDo9091lqOGUsMQ1WQudw3ggIMxZ3N9eU+0nrpfsO5yKAOlRn9fQUYXFWaED1FKyOFYwYq26x8R9o/CMyQEfETv2Em2+sto7aSyK1eh root@testh.example.com + ESXi 에 KVM ssh key 등록 [root@testh ~]# scp /root/.ssh/id_rsa.pub 192.168.13.6:/etc/ssh/keys-root/ Password: id_rsa.pub 100% 404 1.3MB/s 00:00 [root@testh ~]# key 를 ESXi 에 /etc/ssh/keys-root/ 에 복사합니다.\n[root@testh ~]# ssh root@192.168.13.6 Password: The time and date of this login have been sent to the system logs.  VMware offers supported, powerful system administration tools. Please see www.vmware.com/go/sysadmintools for details.  The ESXi Shell can be disabled by an administrative user. See the vSphere Security documentation for more information. [root@esxi4:~] [root@esxi4:~] cd /etc/ssh/keys-root/ [root@esxi4:/etc/ssh/keys-root] ls id_rsa.pub [root@esxi4:/etc/ssh/keys-root] mv id_rsa.pub authorized_keys [root@esxi4:/etc/ssh/keys-root] chmod 600 authorized_keys 위와 같이 authorized_keys 로 이름을 수정하거나, id_rsa.pub key 의 내용을 authorized_keys 에 추가하는 방법도 괜찮습니다.\n3. virt-v2v 수행 [root@testh ~]# export LIBGUESTFS_BACKEND=direct [root@testh ~]# virt-v2v -i vmx -it ssh \u0026#34;ssh://root@192.168.13.6/vmfs/volumes/NT_datastore/mtest1/mtest1.vmx\u0026#34; [ 0.0] Opening the source -i vmx ssh://root@192.168.13.6/vmfs/volumes/NT_datastore/mtest1/mtest1.vmx [ 0.7] Creating an overlay to protect the source from being modified [ 1.6] Opening the overlay [ 8.1] Inspecting the overlay [ 17.2] Checking for sufficient free disk space in the guest [ 17.2] Estimating space required on target for each disk [ 17.2] Converting CentOS Linux release 7.4.1708 (Core) to run on KVM virt-v2v: This guest has virtio drivers installed. [ 83.0] Mapping filesystem data to avoid copying unused and blank areas [ 83.9] Closing the overlay [ 84.0] Assigning disks to buses [ 84.0] Checking if the guest needs BIOS or UEFI to boot [ 84.0] Initializing the target -o libvirt -os default [ 84.0] Copying disk 1/2 to /var/lib/libvirt/images/mtest1-sda (raw)  (100.00/100%) [ 193.6] Copying disk 2/2 to /var/lib/libvirt/images/mtest1-sdb (raw)  (100.00/100%) [ 346.7] Creating output metadata 풀 default가 새로고침 되었습니다  mtest1에서 정의된 도메인 /tmp/v2vlibvirtc84a5d.xml  [ 346.8] Finishing off  [root@testh ~]# virsh list --all  Id 이름 상태 ----------------------------------------------------  2 centos7.0 실행중  - mtest1 종료  - testvm 종료  [root@testh ~]# virsh edit mtest1 도메인 mtest1 XML 설정이 편집되었습니다. 위와 같이 convert 작업을 수행합니다.\nconvert 작업 이후, virsh edit \u0026lt;VM name\u0026gt; 명령을 수행하여 아래와 같이 KVM 환경에 맞게 네트워크 정보를 수정합니다.\n+ 예제 ... 중략 ...  --\u0026lt;interface type=\u0026#39;network\u0026#39;\u0026gt; ++\u0026lt;interface type=\u0026#39;bridge\u0026#39;\u0026gt; -- \u0026lt;source network=\u0026#39;default\u0026#39;/\u0026gt; ++ \u0026lt;source bridge=\u0026#39;br0\u0026#39;/\u0026gt;  \u0026lt;model type=\u0026#39;virtio\u0026#39;/\u0026gt;  ... 중략 ...  VM 시작  [root@testh ~]# virsh start mtest1 도메인 mtest1가 시작됨 VM 부팅 후 /etc/sysconfig/network-scripts/ifcfg-XXX 파일을 KVM 환경에 맞게 수정합니다.\n참고 자료   https://access.redhat.com/articles/1351473 http://libguestfs.org/virt-v2v-input-vmware.1.html https://kb.vmware.com/s/article/1002866  ","permalink":"https://chhanz88.github.io/post/2019-10-22-virt-v2v/","summary":"Converting VMware ESXi Linux VM to KVM (virt-v2v)  1. Install package virt-v2v $ yum install virt-v2v 위와 같이 virt-v2v package 를 설치 합니다.\n2. ESXi - KVM 간 ssh key 인증 설정 KVM 호스트에서 ESXi 의 데이터스토어에 Password 없이 접근이 가능하도록 설정을 해야 virt-v2v 를 이용하여 VM 을 이관 할 수 있습니다.\n+ VMware ESXi ssh Daemon 시작 위와 같이 ESXi Web Console 에서 작업 \u0026gt; 서비스 \u0026gt; SSH 사용 선택","title":"[Linux] Converting VMware ESXi Linux VM to KVM (virt-v2v)"},{"content":"[Kubernetes] CKA(Certified Kubernetes Administrator) 시험 합격 후기  안녕하세요. chhanz 입니다.\n2019년 취득 목표로 잡고 공부하던 CKA, Certified Kubernetes Administrator 자격증을 드디어 취득하였습니다.\nCKA 자격증을 취득하고 느낀점은 커뮤니티에 올려주신 많은 합격 후기들이 \u0026ldquo;너무 도움이 많이 되었다.\u0026rdquo; 라는 점입니다.\n그리하여 제가 공부를 어떻게 했고, 시험이 어떤식으로 진행이 되었는지 공유해드리겠습니다.\n공부 준비  다양한 리소스를 이용하여 공부를 하였으나, 제가 제일 유용하였고 도움이 되었다고 생각되는 자료 리스트입니다.\n Kubernetes Tutorials\n언제나 기본기는 중요합니다. hands-on lab 도 제공하여 처음 기본을 잡는데 유용하였습니다.(katacoda practice lab) Kubernetes in Action\n스터디 그룹에서 번역의 상태가\u0026hellip;\u0026hellip;, 하지만 내용은 너무 좋은 책입니다.\nKubernetes Korea Group의 Kubernetes Architecture Study 모임 에서 진행된 스터디 자료를 참조하시면 더욱 좋습니다. Certified Kubernetes Administrator (CKA) with Practice Tests\nUdemy 에서 유료로 제공되는 인터넷 강의입니다. 영문으로 구성된 강의이며, 영문 자막을 제공하고 있습니다.\n강의 자체를 100% 소화를 못하더라도 중간중간 제공되는 Practice Lab 을 풀어보는 것을 좋습니다. 해당 Lab 시스템이 실제 시험과 매우 비슷한 환경입니다. 다만 실제 시험과 달리 성능이 매우 느립니다. ;( Kubernetes the hard way\nKubernetes 를 공부하기 시작하면 많은 분들이 이 자료를 추천합니다.\n저의 경우, Open Infrastructur \u0026amp; Cloud Native Days Korea 2019 에서 @jmyung님께서 진행하신 kubernetes-the-hard-way-modified 를 현장에서 실습하고\nGCP 기반으로 되어 있는 내용을 VM 기반으로 변경하여 설치하면서 다시 한번 리뷰 하였습니다.\n@jmyung 님이 현장에서 해주신 hands-on lab 이 매우 도움이 많이 되었습니다. 감사합니다!!! :) CKA-StudyGuide\n분야별 Example Lab 문제를 제공합니다.\n시험보는 느낌으로 테스트 해보시는 것도 도움이 됩니다.  시험 준비  시험 신청 저는 Open Infrastructur \u0026amp; Cloud Native Days Korea 2019 의 CNCF 부스에서 15% 할인 쿠폰을 받아서 조금 저렴한 가격에 시험을 등록 하였습니다.\n시험 준비 CNCF Portal 에서 후보자가 확인하고 준비할 사항에 대해 링크 및 설명을 제공합니다.\n하나씩 체크하면서 준비합니다.\n추가로 연결되는 사이트에서 시험 가능한 시간을 선택하면 됩니다.\n시험 당일 시험 보기전 사전에 확인하는 사항이 매우 많습니다.\n저의 경우, 시험은 아침 10시 15분이였습니다. 그리고 시험 시작 15분 전인 10시에 exam classroom 을 입장 할 수 있도록 링크가 활성화 되었습니다.\nexam classroom 에 입장을 하면 Live Chat 을 이용하여 시험 감독관의 시험 환경 확인이 시작됩니다.\n생각보다 이 과정은 오래걸립니다. 저는 약 15분 동안 확인하고 추가 질문하는데 사용하였습니다.\n미리 시험 시작 15분전에 입장하시고 시험 환경 확인을 받는 것을 추천 드림니다.\n시험 환경 확인  시험 감독관은 Webcam 을 통해 본인 확인 및 시험 환경을 확인합니다.  Webcam 에 본인 신분 확인을 할 수 있도록 여권을 보여달라고 요청합니다. 키보드가 있는 위치를 볼 수 있도록 Webcam 을 돌려달라고 합니다. 시험을 볼 수 있는 주변 환경을 Webcam 을 통해 천천히 보여달라고 합니다. 책상 전체 및 책상 밑 부붙을 보여달라고 합니다.     시험에 필요한 물품이 아닌 것은 사전에 정리하는 것이 좋을 것 같습니다.\n 시험 보는 동안의 주의 사항    저는 집과 가까운 인근 스터디룸을 대여하여 시험을 보았습니다.\n평소와 달리 노트북 모니터로 보니 글씨가 작은 듯하여 자세히 보기 위해 자주 노트북 가까이 갔더니 Webcam 기준에서는 화면 밑으로 제 모습이 사라지는 경우가 있었던 것 같습니다.\n바로 Live Chat 으로 주의를 주며, 화면 안쪽에 있어 달라고 요청합니다.\n  Windows 기준, Terminal 에서는 ctrl+insert , shift+insert 를 사용해야되고 Web Page 에서는 ctrl+c/v 를 사용 하였습니다.\n마우스가 Terminal 로 들어가면 ctrl+c/v 를 사용 못하고 Terminal 밖으로 마우스가 이동하면 ctrl+c/v 가 사용 가능 했습니다.\n그리고 ctrl+w 는 tab 제거 단축키이므로 입력하면 안됩니다.\n  시험보는 도중, command 가 입력이 잘 안된다는 느낌이 들면서 Connection Lost 라는 문구가 나오면서 Web page 와 Terminal 창이 꺼졌습니다.\n매우 당황했습니다. ㅠㅠ\nLive Chat 을 통해 문제에 대해 알렸습니다. 이후 감독관이 조치를 해주었습니다.\n당황하지 말고 Live Chat 으로 요청하시면 될 것 같습니다.\n  합격  총 3시간의 시험 시간 중, 2시간 정도는 문제 풀이 / 30분은 검토 진행 하였습니다.\nCKA 시험의 경우, 불합격하더라도 재시험의 기회를 주기 때문에\n불합격하면 부족한 부분을 더 공부해서 재시험에 도전 해야겠다는 생각을 하며 Request Break Exam 하였습니다.\n그리고 약 30시간 정도 후, 시험 합격 결과가 통보 되었습니다.\n아래와 같이 합격하여 재시험은 볼 필요가 없어졌습니다. ^^/\n마치며 여러가지의 공부 자료들이 많이 도움이 되었지만,\nKubernetes Korea User Group 및 Openinfra day 에서 공유 해주신 자료와 정보\n마지막으로 Kubernetes Korea Group의 Kubernetes Architecture Study 모임 에서의 100일간 스터디 그룹이 많은 도움이 되었습니다.\n감사합니다.\n","permalink":"https://chhanz88.github.io/post/2019-09-30-cka-exam-review/","summary":"[Kubernetes] CKA(Certified Kubernetes Administrator) 시험 합격 후기  안녕하세요. chhanz 입니다.\n2019년 취득 목표로 잡고 공부하던 CKA, Certified Kubernetes Administrator 자격증을 드디어 취득하였습니다.\nCKA 자격증을 취득하고 느낀점은 커뮤니티에 올려주신 많은 합격 후기들이 \u0026ldquo;너무 도움이 많이 되었다.\u0026rdquo; 라는 점입니다.\n그리하여 제가 공부를 어떻게 했고, 시험이 어떤식으로 진행이 되었는지 공유해드리겠습니다.\n공부 준비  다양한 리소스를 이용하여 공부를 하였으나, 제가 제일 유용하였고 도움이 되었다고 생각되는 자료 리스트입니다.\n Kubernetes Tutorials\n언제나 기본기는 중요합니다. hands-on lab 도 제공하여 처음 기본을 잡는데 유용하였습니다.","title":"[Kubernetes] CKA(Certified Kubernetes Administrator) 시험 합격 후기"},{"content":"CentOS 8 custom image 생성  드디어 CentOS 8 이 release 되었습니다. (2019-09-24)\n CentOS 8 release news\n 이것 저것 새로운 기능들을 테스트 해보고 싶은 욕망에 fast-vm 에서 사용할 custom image 를 생성해 보도록 하겠습니다.\nCustom image 생성 아직 CentOS 8 의 fast-vm public image 는 아직 추가가 안 되었습니다.\n하지만 RHEL 8 을 통해 CentOS 8 image 를 생성 할 수 있습니다.\n 여담으로 fast-vm 의 개발자, @ondrej 에게 공식 public image upload 를 요청 하였습니다.\n테스트 로그도 첨부 하였죠! issue #10 - request add CentOS 8 fast-vm-public-image\n Download CentOS 8 iso image $ cd /tmp;wget http://mirror.kakao.com/centos/8.0.1905/isos/x86_64/CentOS-8-x86_64-1905-dvd1.iso Download rhel 8 hackfile, xml, kickstart-ks $ wget https://raw.githubusercontent.com/OndrejHome/fast-vm-public-images/master/centos/xml/centos-6.3-current.xml $ wget https://raw.githubusercontent.com/OndrejHome/fast-vm-public-images/master/rhel/ks/virt-install-rhel-8.sh $ wget https://raw.githubusercontent.com/OndrejHome/fast-vm-public-images/master/rhel/hacks/6g_rhel-8-hacks.sh $ wget https://raw.githubusercontent.com/OndrejHome/fast-vm-public-images/master/rhel/ks/rhel-8.ks Change rhel 8 to CentOS 7 (filename 및 내용 변경) [root@test-vm-host centos-8.0]# diff centos-8.ks rhel-8.ks 35,37c35,37 ++ volgroup c8vg --pesize=4096 pv.1 ++ logvol swap --fstype=\u0026#34;swap\u0026#34; --size=256 --name=swap_lv --vgname=c8vg ++ logvol / --fstype=\u0026#34;xfs\u0026#34; --size=5000 --name=root_lv --vgname=c8vg --- -- volgroup r8vg --pesize=4096 pv.1 -- logvol swap --fstype=\u0026#34;swap\u0026#34; --size=256 --name=swap_lv --vgname=r8vg -- logvol / --fstype=\u0026#34;xfs\u0026#34; --size=5000 --name=root_lv --vgname=r8vg  [root@test-vm-host centos-8.0]# diff virt-install-centos-8.sh virt-install-rhel-8.sh 9c9 ++ --name centos-8-fastvm-install \\ --- -- --name rhel-8-fastvm-install \\ 21c21 ++ virsh --connect qemu:///system undefine centos-8-fastvm-install --- -- virsh --connect qemu:///system undefine rhel-8-fastvm-install  [root@test-vm-host centos-8.0]# diff 6g_centos-8-hacks.sh 6g_rhel-8-hacks.sh 28c28 ++ guestfish -a \u0026#34;/dev/$THINPOOL_VG/$VM_NAME\u0026#34; -m /dev/c8vg/root_lv -m /dev/sda1:/boot --selinux \u0026lt;\u0026lt;EOF --- -- guestfish -a \u0026#34;/dev/$THINPOOL_VG/$VM_NAME\u0026#34; -m /dev/r8vg/root_lv -m /dev/sda1:/boot --selinux \u0026lt;\u0026lt;EOF Create empty image $ fast-vm import_custom_image 6 centos-8.0 empty centos-6.3-current.xml virt-install 을 이용하여 base image 설치 $ ./virt-install-centos-8.sh /dev/c7vg/fastvm-centos-8.0 /tmp/CentOS-8-x86_64-1905-dvd1.iso centos-8.ks  설치 시작   kickstart 를 기준으로 자동 설치   설치 완료  Export image $ fast-vm export_image centos-8.0 gz Remove empty image $ fast-vm remove_image centos-8.0 Import custom image ## 6gb image 를 import 하기 위함. $ mv centos-8.0.img.gz 6g__centos-8.0.img.gz  ## import custom image  $ fast-vm import_image centos-8.0 6g__centos-8.0.img.gz centos-6.3-current.xml 6g_centos-8-hacks.sh install libguestfs requirements Import 한 image 를 이용하여 vm 을 생성하니 아래와 같은 에러가 발생 되었습니다.\n hackfile 에서 libguestfs 확인 부분 삭제 case  이 문제는 RHEL 8, CentOS 8 일부 시스템(RHEL/CentOS 7.x, libguestfs-appliance-1.38.0)에서 xfs 의 새로운 기능으로 인해 libguestfs 가 정상적으로 작동하지 못하여 발생되는 원인입니다.\n설치 방법은 아래와 같습니다.\nfast-vm 가이드 문서 : https://github.com/OndrejHome/fast-vm-public-images/tree/master/rhel\n[root@test-vm-host centos-8.0]# mkdir /var/tmp/fedora29 [root@test-vm-host centos-8.0]# cd /var/tmp/fedora29/ [root@test-vm-host fedora29]# wget http://ftp.linux.cz/pub/linux/people/ondrej_famera/fastvm-images/appliance-1.39.11.tar.xz [root@test-vm-host fedora29]# tar xvf appliance-1.39.11.tar.xz  appliance/ appliance/README.fixed appliance/root appliance/initrd appliance/kernel [root@test-vm-host fedora29]# echo \u0026#39;x /var/tmp/fedora29\u0026#39; \u0026gt; /etc/tmpfiles.d/fast-vm-fedora29-appliance.conf vm 생성 테스트 [root@test-vm-host fast-vm]# fast-vm create centos-8.0 77 [77][inf] using file /etc/fast-vm/config-centos-8.0.xml as libvirt XML [77][inf] using file /etc/fast-vm/hacks-centos-8.0.sh as hack file [77][inf] defining virtual machine \u0026#39;fastvm-centos-8.0-77\u0026#39; in libvirt fastvm-centos-8.0-77에서 정의된 도메인 /tmp/tmp.xkf30zZawx.xml  Domain title updated successfully [77][inf] creating disk \u0026#39;fastvm-centos-8.0-77\u0026#39; [77][inf] adding static lease for 192.168.200.77 into libvirts DHCP [77][inf] applying hacks from /etc/fast-vm/hacks-centos-8.0.sh *stdin*:3: libguestfs: error: sh: mv: \u0026#39;/etc/sysconfig/network-scripts/ifcfg-ens3\u0026#39; and \u0026#39;/etc/sysconfig/network-scripts/ifcfg-ens3\u0026#39; are the same file [77][wrn] there was issue applying hacks to this machine, check syslog for more details [77][inf] applying hacks finished [77][ok] VM \u0026#39;fastvm-centos-8.0-77\u0026#39; created  [root@test-vm-host fast-vm]# fast-vm console 77  도메인 fastvm-centos-8.0-77에 연결되었습니다 Escape character is ^]  ...  CentOS Linux 8 (Core) Kernel 4.18.0-80.el8.x86_64 on an x86_64  fastvm-centos-8-0-77 login: root Password: Last login: Wed Sep 25 15:42:04 on ttyS0 [root@fastvm-centos-8-0-77 ~]# cat /etc/redhat-release  CentOS Linux release 8.0.1905 (Core) [root@fastvm-centos-8-0-77 ~]# uname -a  Linux fastvm-centos-8-0-77 4.18.0-80.el8.x86_64 #1 SMP Tue Jun 4 09:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux  [root@fastvm-centos-8-0-77 ~]# 위와 같이 정상적으로 CentOS 8 을 사용 할 수 있게 되었습니다.\n참고 자료  fast-vm guide : https://www.famera.cz/blog/fast-vm/user_guide.html fast-vm public image github : https://github.com/OndrejHome/fast-vm-public-images  ","permalink":"https://chhanz88.github.io/post/2019-09-25-fastvm-public-image/","summary":"CentOS 8 custom image 생성  드디어 CentOS 8 이 release 되었습니다. (2019-09-24)\n CentOS 8 release news\n 이것 저것 새로운 기능들을 테스트 해보고 싶은 욕망에 fast-vm 에서 사용할 custom image 를 생성해 보도록 하겠습니다.\nCustom image 생성 아직 CentOS 8 의 fast-vm public image 는 아직 추가가 안 되었습니다.\n하지만 RHEL 8 을 통해 CentOS 8 image 를 생성 할 수 있습니다.\n 여담으로 fast-vm 의 개발자, @ondrej 에게 공식 public image upload 를 요청 하였습니다.","title":"[fast-vm] CentOS 8 custom image 생성"},{"content":"nvidia container runtime 설정  nvidia container runtime 을 설정하기 위해서는 꼭 docker-ce 로 docker 가 설치가 되어 있어야 됩니다.\n(일반 RHEL/CentOS에서 제공되는 docker package로는 설치 불가)\ndocker-ce 설치  yum-utils 설치  $ yum -y install yum-utils  docker-ce Repository 연결  $ yum-config-manager \\  \u0026gt; --add-repo \\ \u0026gt; https://download.docker.com/linux/centos/docker-ce.repo Loaded plugins: fastestmirror adding repo from: https://download.docker.com/linux/centos/docker-ce.repo grabbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo repo saved to /etc/yum.repos.d/docker-ce.repo $  docker-ce 설치  $ yum install docker-ce  docker 서비스 시작  $ systemctl enable docker Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service. $ systemctl start docker  docker 기본 runtime 확인  # docker info ... 중략 Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: 894b81a4b802e4eb2a91d1ce216b8817763c29fb runc version: 425e105d5a03fabd737a126ad93d62a9eeede87f init version: fec3683 ... 중략 위와 같이 Default 로 지정된 runtime 은 runc 입니다.\nnvidia container runtime 설치  nvidia container runtime repository 연결  $ distribution=$(. /etc/os-release;echo $ID$VERSION_ID) $ curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.repo | \\  sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo 참고문서 : https://nvidia.github.io/nvidia-container-runtime/\n nvidia container runtime 설치  $ yum install nvidia-container-runtime  Daemon configuration file 수정 및 systemd 수정  Daemon configuration file 수정    $ vi /etc/docker/daemon.json {  \u0026#34;default-runtime\u0026#34;: \u0026#34;nvidia\u0026#34;,  \u0026#34;runtimes\u0026#34;: {  \u0026#34;nvidia\u0026#34;: {  \u0026#34;path\u0026#34;: \u0026#34;/usr/bin/nvidia-container-runtime\u0026#34;,  \u0026#34;runtimeArgs\u0026#34;: []  }  } } * _systemd_ 수정  $ mkdir -p /etc/systemd/system/docker.service.d $ tee /etc/systemd/system/docker.service.d/override.conf \u0026lt;\u0026lt;EOF [Service] ExecStart= ExecStart=/usr/bin/dockerd --host=fd:// --add-runtime=nvidia=/usr/bin/nvidia-container-runtime EOF $ systemctl daemon-reload $ systemctl restart docker  nvidia container runtime 설치 확인  $ docker info ... 중략  Runtimes: nvidia runc  Default Runtime: nvidia  Init Binary: docker-init  containerd version: 894b81a4b802e4eb2a91d1ce216b8817763c29fb  runc version: 425e105d5a03fabd737a126ad93d62a9eeede87f  init version: fec3683 ... 중략 위와 같이 사용가능한 docker runtime 은 nvidia, runc 이고 Default runtime 은 nvidia 입니다.\n앞으로 docker run 명령을 통해 생성되는 container 는 nvidia-container-runtime 을 이용하여 생성될 것입니다.\n마치며  상기 내용은 NVIDIA DGX Station 시스템에 Red Hat 운영체제 설치하고 CUDA 설정을 하면서 경험한 내용입니다.\n[root@localhost sosreport-test-2019-xx-xx-xxxxxx]# cat proc/driver/nvidia/gpus/*/information Model: Tesla V100-DGXS-16GB IRQ: 144 GPU UUID: GPU-11111111-1111-1111-1111-111111111111 Video BIOS: 88.00.24.00.01 Bus Type: PCIe DMA Size: 47 bits DMA Mask: 0x7fffffffffff Bus Location: 0000:07:00.0 Device Minor: 0 Blacklisted: No Model: Tesla V100-DGXS-16GB IRQ: 145 GPU UUID: GPU-22222222-2222-2222-2222-222222222222 Video BIOS: 88.00.24.00.01 Bus Type: PCIe DMA Size: 47 bits DMA Mask: 0x7fffffffffff Bus Location: 0000:08:00.0 Device Minor: 1 Blacklisted: No Model: Tesla V100-DGXS-16GB IRQ: 146 GPU UUID: GPU-33333333-3333-3333-3333-333333333333 Video BIOS: 88.00.24.00.01 Bus Type: PCIe DMA Size: 47 bits DMA Mask: 0x7fffffffffff Bus Location: 0000:0e:00.0 Device Minor: 2 Blacklisted: No Model: Tesla V100-DGXS-16GB IRQ: 147 GPU UUID: GPU-44444444-4444-4444-4444-444444444444 Video BIOS: 88.00.24.00.01 Bus Type: PCIe DMA Size: 47 bits DMA Mask: 0x7fffffffffff Bus Location: 0000:0f:00.0 Device Minor: 3 Blacklisted: No Tesla V100 이 무려 4장이나 설치된 어마어마한 장비였습니다\u0026hellip;\u0026hellip;. :(\n참고 문서   NVIDIA DGX Station : https://www.nvidia.com/ko-kr/data-center/dgx-station/ NVIDIA Repository : https://nvidia.github.io/nvidia-container-runtime/ Github nvidia-container-runtime : https://github.com/NVIDIA/nvidia-container-runtime Install docker-ce : https://docs.docker.com/install/linux/docker-ce/centos/  ","permalink":"https://chhanz88.github.io/post/2019-09-20-docker-configuration-nvidia-runtime/","summary":"nvidia container runtime 설정  nvidia container runtime 을 설정하기 위해서는 꼭 docker-ce 로 docker 가 설치가 되어 있어야 됩니다.\n(일반 RHEL/CentOS에서 제공되는 docker package로는 설치 불가)\ndocker-ce 설치  yum-utils 설치  $ yum -y install yum-utils  docker-ce Repository 연결  $ yum-config-manager \\  \u0026gt; --add-repo \\ \u0026gt; https://download.docker.com/linux/centos/docker-ce.repo Loaded plugins: fastestmirror adding repo from: https://download.docker.com/linux/centos/docker-ce.repo grabbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo repo saved to /etc/yum.repos.d/docker-ce.repo $  docker-ce 설치  $ yum install docker-ce  docker 서비스 시작  $ systemctl enable docker Created symlink from /etc/systemd/system/multi-user.","title":"[Docker] nvidia container runtime 설정"},{"content":"kubeadm 을 이용한 Kubernetes 설치  kubeadm 을 이용하여 kubernetes 테스트 환경을 쉽고 빠르게 구축하도록 하겠습니다.\n준비 사항 이번 테스트 환경은 1 Master node, 2 Worker node 로 구성을 할 예정입니다. 상세 내역은 아래와 같습니다.\n OS Version : CentOS 7.6 Docker Version : v18.6.1\nKubernetes Version : v1.15.3\n 사전 준비 Kubernetes 설치를 위해 사전 준비 작업이 필요합니다.\n위 작업은 운영체제 설정 작업 및 Package 설치 작업이며,\n해당 작업은 Kubernetes Preinstaller 를 이용하여 손쉽게 작업하도록 하겠습니다.\n Kubernetes Preinstaller : https://github.com/chhanz/k8s-install-script  해당 source 는\n$ git clone https://github.com/chhanz/k8s-install-script.git git clone 명령을 통해 로컬에 복제합니다.\n Preinstaller File list (v1.15.3)  ├── README.md  ├── inventory ├── old_bash-1.12  ## Old Bash Script (v1.12) │ ├── 0_preinstall_base.sh │ ├── 1_mk_master.yml │ ├── 2_master1_install.sh │ ├── 3_deploy_key.sh │ ├── 4_deploy_install_script.sh │ ├── 5_network_plugin.sh  │ ├── kube-flannel.yml │ ├── kubeadm-config_master1.yaml │ ├── kubeadm-config_master2.yaml │ ├── kubeadm-config_master3.yaml │ ├── node2_deploy.sh │ └── node3_deploy.sh └── preinstall.yaml  ## Preinstaller (ansible)  1 directory, 15 files VM 이 접근이 가능한 시스템 혹은 PC등, ansible 수행이 가능한 위치에서 inventory 작성을 하고 playbook 을 실행하면 됩니다.\nInventory 변경 [all]  fastvm-centos-7-6-22 ansible_host=192.168.200.22 fastvm-centos-7-6-23 ansible_host=192.168.200.23 fastvm-centos-7-6-24 ansible_host=192.168.200.24  [all:vars] ansible_ssh_user=root ansible_ssh_pass=password docker_package_version=docker-ce-18.06.1.ce kubernetes_package_version=1.15.3-0 위와 같이 VM IP 정보, root 접속 정보, docker \u0026amp; kubernetes version 정보를 입력합니다.\nPlaybook 수행 $ ansible-playbook -i inventory preinstall.yaml 해당 명령을 통해 preinstall playbook 을 수행합니다.\n아래와 같이 완료가 되면\n해당 시스템을 Reboot 하여, preinstaller playbook 을 통해 설정된 값이 적용되도록 합니다.\nPlaybook 수행 내역 해당 preinstall playbook 은 아래와 같은 항목을 자동으로 설정합니다.\n  firewalld 중지 및 Disabled SELinux Disabled SWAP 중지 및 해제 Kernel Parameter 설정 Docker-CE, Kubernetes Repository 등록 Requirement Package 설치 systemd 설정   Kubernetes 설치 kubeadm 을 이용하여 설치하기 위해 아래 명령을 Master node 에서 수행합니다.\n$ kubeadm init --apiserver-advertise-address=\u0026lt;\u0026lt;control-plane node IP\u0026gt;\u0026gt; 수행이 완료가 되면 아래와 같이 Worker node 연결을 위한 token 이 생성됩니다.\nkubeconfig 생성 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config kubeconfig 를 생성하면 kubectl 명령이 정상적으로 작동 할 것 입니다.\nPod Network 설치 CNI 를 설치하기 위해 하기 문서를 참고합니다.\n참고 문서 : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\n이번 포스팅의 테스트에서는 Calico CNI 를 사용하도록 하겠습니다.\n아래와 같이 cailco.yaml 을 배포합니다.\n# kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml  configmap/calico-config created customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-node created daemonset.apps/calico-node created serviceaccount/calico-node created deployment.apps/calico-kube-controllers created serviceaccount/calico-kube-controllers created 배포가 완료가 되면 아래와 같이 calico 관련 Pod 이 생성이 되고 Running 상태가 되면 CNI 설치가 완료가 된 것 입니다.\n$ kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE pod/calico-kube-controllers-65b8787765-brdv7 1/1 Running 0 117s pod/calico-node-lggxn 1/1 Running 0 117s pod/coredns-5c98db65d4-m6svd 1/1 Running 0 23m pod/coredns-5c98db65d4-x7gcb 1/1 Running 0 23m pod/etcd-fastvm-centos-7-6-22 1/1 Running 1 22m pod/kube-apiserver-fastvm-centos-7-6-22 1/1 Running 1 22m pod/kube-controller-manager-fastvm-centos-7-6-22 1/1 Running 5 22m pod/kube-proxy-82v78 1/1 Running 1 23m pod/kube-scheduler-fastvm-centos-7-6-22 1/1 Running 5 22m Worker node 연결 Master node 가 초기화가 되고, 마지막에 생성된 token 값을 연결할 Worker node 에 입력합니다.\n$ kubeadm join 192.168.200.22:6443 --token cdw8cj.4guf1fr9e7shc7u8 \\  --discovery-token-ca-cert-hash sha256:bc3604fb648338821d84ddf5b5259064ae5ceb2ee159f708d6741b2d1e7c65a2 위와 같이 Worker node 가 연결 되는 것을 확인하였습니다. 사용할 모든 Worker node 에 수행합니다.\n배포 완료 APP 배포 테스트 구성이 완료된 kubernetes 가 정상적으로 작동하는지 APP 을 배포하여 테스트하도록 하겠습니다.\n Pod 생성  $ kubectl run test-httpd --image=httpd deployment.apps/test-httpd created  Pod 생성 확인  $ kubectl get po NAME READY STATUS RESTARTS AGE test-httpd-7dd7c96c8f-hvbzl 1/1 Running 0 31s  Service 생성  $ kubectl expose deployment.apps/test-httpd --port 80 service/test-httpd exposed  Service 작동 확인  $ curl 10.97.176.30 \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;It works!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; 이처럼 손쉽게 kubernetes 테스트 환경을 구축 하였습니다.\n참고 문서  Kubernetes Preinstaller github : https://github.com/chhanz/k8s-install-script kubeadm을 이용한 설치 : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ calico CNI 설치 : https://docs.projectcalico.org/v3.8/getting-started/kubernetes/installation/calico  ","permalink":"https://chhanz88.github.io/post/2019-09-04-kubernetes-install-1_15_3/","summary":"kubeadm 을 이용한 Kubernetes 설치  kubeadm 을 이용하여 kubernetes 테스트 환경을 쉽고 빠르게 구축하도록 하겠습니다.\n준비 사항 이번 테스트 환경은 1 Master node, 2 Worker node 로 구성을 할 예정입니다. 상세 내역은 아래와 같습니다.\n OS Version : CentOS 7.6 Docker Version : v18.6.1\nKubernetes Version : v1.15.3\n 사전 준비 Kubernetes 설치를 위해 사전 준비 작업이 필요합니다.\n위 작업은 운영체제 설정 작업 및 Package 설치 작업이며,\n해당 작업은 Kubernetes Preinstaller 를 이용하여 손쉽게 작업하도록 하겠습니다.","title":"[Kubernetes] kubeadm 을 이용한 Kubernetes 설치"},{"content":"Fast-VM  Fast-VM 이란?  Fast-VM 라는 Open Source Solution 을 알게된 것은 Ondrej Faměra 라는 친구를 만나면서 입니다.\nThank You. Ondrej ^o^\n Fast-VM 은 Ondrej Faměra 가 만든 libvirtd 기반의 가상화 Provisioning Solution 입니다.\n기존의 libvirtd 기반의 가상화는 virt-manager 혹은 virsh 을 통해 VM 생성 및 운영을 하였습니다.\nFast-VM 을 이용하면 fast-vm 이라는 명령어 하나로 VM을 생성하고 관리 할 수 있습니다.\n다양한 Linux 배포판을 설치하고 테스트를 해야되는 저는 fast-vm 을 통해 여러가지 스트레스들이 사라졌습니다. ^^\nFast-VM 설치 fast-vm 의 설치 환경은 아래와 같습니다.\n  CentOS/RHEL 7.6 and Fedora 28, 29, 30, RHEL system KVM 가상화를 사용 할 수 있도록 CPU 가상화 기능 활성화 LVM Pool 공간으로 사용될 Free VG 공간 Package 설치를 위한 Public Network 연결 허용   fast-vm 설치는 ansible 을 이용하여 간편하게 설치가 가능합니다.\n[root@fastvm-host ~]# yum -y install ansible 먼저 ansible 을 설치합니다.\n[root@fastvm-host ~]# ansible-galaxy install ondrejhome.fast-vm-server - downloading role \u0026#39;fast-vm-server\u0026#39;, owned by ondrejhome - downloading role from https://github.com/OndrejHome/ansible.fast-vm-server/archive/v6.tar.gz - extracting ondrejhome.fast-vm-server to /etc/ansible/roles/ondrejhome.fast-vm-server - ondrejhome.fast-vm-server (v6) was installed successfully [root@fastvm-host ~]# ansible-galaxy 명령을 통해 fast-vm playbook 을 Download 합니다.\n[root@fastvm-host defaults]# vgs  VG #PV #LV #SN Attr VSize VFree  centos 1 2 0 wz--n- 278.36g \u0026lt;120.37g [root@fastvm-host defaults]# 위와 같이 Free Size 의 VG 를 준비합니다.\n[root@fastvm-host defaults]# pwd /etc/ansible/roles/ondrejhome.fast-vm-server/defaults  [root@fastvm-host defaults]# cat main.yml --- ### areas that can be configured by this role  ## configure repositories needed for fast-vm installation config_repositories: true ## install packages needed by fast-vm and fast-vm itself install_fastvm: true ## configure libvirt for fast-vm access (change groups and permissions settings) config_libvirt_access: true ## configure libvirt network for fast-vm (libvirtd service will be enabled after boot) config_libvirt_network: true ## configure storage for fast-vm (create thinpool LV) config_storage: true ## configure sudoers for fast-vm config_sudoers: true ## generate /etc/fast.conf configuration file config_fastvm_conf: true ## install OVMF UEFI firmware needed by UEFI fast-vm machines install_ovmf: true ## install and configure fence_virtd that can be used to fence the fast-vm VMs using fence_xvm install_fence_virtd: true ## install custom version of qemu-kvm,qemu-img and seabios-bin to support LSI and MEGASAS emaulated drivers install_custom_qemu: true  ### variable that are required by some areas from above # all variable has list of \u0026#39;required by\u0026#39; on which above options needs them  ## group with access to fast-vm # required by: config_libvirt_access, config_sudoers, config_fastvm_conf fastvm_group: libvirt  ## name of VG where fast-vm thinpool LV is located # required by: config_storage, config_fastvm_conf fastvm_vg: centos  ## name of fast-vm thinpool LV # required by: config_storage, config_fastvm_conf fastvm_lv: fast-vm-pool  ## size of fast-vm thinpool LV # required by: config_storage, config_fastvm_conf fastvm_lv_size: 100G  ## fast-vm network subnet number # required by: config_libvirt_network, config_fastvm_conf fastvm_net: 100  ## name of fast-vm NAT libvirt network # required by: config_libvirt_network, config_fastvm_conf, install_fence_virt fastvm_net_name: fast-vm-nat  ## prefix for fast-vm VMs # required by: config_fastvm_conf fastvm_vm_prefix: \u0026#39;fastvm-\u0026#39;  ## fast-vm notes directory - here fast-vm stores VM notes and other VM stateful details # required by: config_fastvm_conf fastvm_notes_dir: \u0026#39;/var/tmp\u0026#39;  ## Allow only owners of VMs and \u0026#39;root\u0026#39; to delete them # required by: config_fastvm_conf fastvm_owner_only_delete: \u0026#39;yes\u0026#39;  ## Multicast address of fence_virt daemon # required by: install_fence_virt fence_virtd_address: \u0026#39;225.0.0.12\u0026#39; [root@fastvm-host defaults]# /etc/ansible/roles/ondrejhome.fast-vm-server/defaults 위 경로의 main.yml 을 설치 환경에 맞게 설정합니다. (현재는 firewalld 설정 관련 Option 을 추가 하였습니다. 상세 내용은 하단에 첨부하도록 하겠습니다.)\n/etc/ansible/roles/ondrejhome.fast-vm-server/defaults/main.yml 에서 아래 Option 을 시스템 환경에 맞게 설정하였습니다.\nfastvm_vg: centos fastvm_lv_size: 100G [root@fastvm-host ~]# cat install.yaml - hosts: localhost  roles:  - { role: ondrejhome.fast-vm-server } 위와 같이 localhost 에 설치하도록 ansible-playbook 을 작성합니다.\n[root@fastvm-host ~]# ansible-playbook install.yaml ... 중략 ... TASK [ondrejhome.fast-vm-server : make fence_xvm.key readable by everyone] ************************************************************************************** ok: [localhost] TASK [ondrejhome.fast-vm-server : generate /etc/fence_virt.conf configuration] ************************************************************************************** ok: [localhost] TASK [ondrejhome.fast-vm-server : start and enable fence_virtd service] ************************************************************************************** changed: [localhost] TASK [ondrejhome.fast-vm-server : install custom qemu-kvm, qemu-img and seabios-bin packages] **************************************************************************************  [WARNING]: Consider using yum module rather than running yum changed: [localhost] PLAY RECAP *************************************************************************** localhost : ok=28 changed=2 unreachable=0 failed=0 [root@fastvm-host ~]# 위와 같이 설치가 손쉽게 되는 것을 볼 수 있습니다.\nFast-VM 운영  Show VM List [root@fastvm-host ~]# fast-vm list VM# Image name Status Profile_name Size( %used ) Activity Notes === Space used: 0.00% of 100.00g [root@fastvm-host ~]# Import OS Images fast-vm Document Page 를 참조하면 다양한 Linux 배포판 Template 가 생성 되어 있고, 사용이 가능합니다.\n Image List : https://www.famera.cz/blog/fast-vm/image_list.html  해당 Page 를 통해 Os Image 를 Import 하도록 하겠습니다.\n[root@test-vm-host ~]# fast-vm import_image centos-6.9 http://ftp.linux.cz/pub/linux/people/ondrej_famera/fastvm-images/generated/6g__centos-6.9.img.xz \\ \u0026gt; https://raw.githubusercontent.com/OndrejHome/fast-vm-public-images/master/centos/xml/centos-6.3-current.xml \\ \u0026gt; https://raw.githubusercontent.com/OndrejHome/fast-vm-public-images/master/centos/hacks/6g_centos-6-hacks.sh [__][inf] provided empty file path [__][inf] Detected remote file with size 199916708 [__][inf] provided empty file path [__][inf] Detected remote file with size 1596 [__][inf] downloading https://raw.githubusercontent.com/OndrejHome/fast-vm-public-images/master/centos/xml/centos-6.3-current.xml [__]into /tmp/tmp.Ca4rz7xD9y [__][inf] provided empty file path [__][inf] Detected remote file with size 1334 [__][inf] downloading https://raw.githubusercontent.com/OndrejHome/fast-vm-public-images/master/centos/hacks/6g_centos-6-hacks.sh [__]into /tmp/tmp.6QO7k5K5zK [__][inf] Size of image was determined from the filename to be 6G. [__][inf] creating LV fastvm-centos-6.9 ... [__][inf] importing image http://ftp.linux.cz/pub/linux/people/ondrej_famera/fastvm-images/generated/6g__centos-6.9.img.xz into /dev/c7vg/fastvm-centos-6.9 [__][inf] please wait while importing image (to show image write progress, install \u0026#39;pv\u0026#39;)  % Total % Received % Xferd Average Speed Time Time Time Current  Dload Upload Total Spent Left Speed 100 190M 100 190M 0 0 3180k 0 0:01:01 0:01:01 --:--:-- 55347 0+640923 records in 0+640923 records out 6442450944 bytes (6.4 GB) copied, 62.6419 s, 103 MB/s [__][ok] Image centos-6.9 imported [root@test-vm-host ~]# 위와 같이 Image 를 Download 하고 fast-vm 에 Import 하였습니다.\n[root@test-vm-host ~]# fast-vm list_images IMAGE |SYSTEM |USER | Image name Size |XML Hack file for|XML Hack file for| centos-6.10 6g |ok create |missing no hack files| centos-6.9 6g |ok create |missing no hack files| centos-7.3 6g |ok create |missing no hack files| centos-7.6 6g |ok create |missing no hack files| centos-7.6-ext 6g |ok create |missing no hack files| NOTE: if image is missing XML it wouldn\u0026#39;t be possible to create a VM from it!  Image with missing hack file(s) can work if it\u0026#39;s not needed.  USER XML and hack file(s) takes precendense before SYSTEM ones. [root@test-vm-host ~]# CentOS 6.9 가 Import 되었습니다.\nhack file 수정 기본적으로 제공된 OS Image 는 유럽 Timezone 이 설정 되어 있습니다.\n그리하여 hack file 을 수정하여 아시아 Timezone 으로 변경을 할 수 있고, 이를 응용하여 나만의 Custom Image 를 생성 할 수 있습니다.\n$ vi /etc/fast-vm/hacks-centos-6.9.sh  #!/bin/bash ## test if guestfish command is present which guestfish 2\u0026gt;\u0026amp;1 \u0026gt; /dev/null if [ \u0026#34;$?\u0026#34; != 0 ]; then  echo \u0026#34;[!!!] Command \u0026#39;guestfish\u0026#39; not found (Install it!). Making changes to VM FAILED.\u0026#34;  exit 1 fi ## using direct backend to avoid selinux issues on fedora for now export LIBGUESTFS_BACKEND=direct ## hostname VM_HOSTNAME=$(echo $VM_NAME|sed -e \u0026#39;s/\\./-/g; s/_/-/g\u0026#39;)  guestfish -a \u0026#34;/dev/$THINPOOL_VG/$VM_NAME\u0026#34; -m /dev/c6vg/root_lv -m /dev/sda1:/boot \u0026lt;\u0026lt;EOF # configure correct MAC address for eth0 network adapter sh \u0026#39;sed -i \u0026#34;s/HWADDR=.*$/HWADDR=\\\u0026#34;${VM_MAC}\\\u0026#34;/; s/^ONBOOT=.*$/ONBOOT=\\\u0026#34;yes\\\u0026#34;/\u0026#34; /etc/sysconfig/network-scripts/ifcfg-eth0\u0026#39; # change the hostname of machine sh \u0026#39;sed -i \u0026#34;s/HOSTNAME=.*$/HOSTNAME=$VM_HOSTNAME/\u0026#34; /etc/sysconfig/network\u0026#39;  ++ # Change Timezone ++ sh \u0026#39;rm -f /etc/localtime\u0026#39; ++ sh \u0026#39;ln -s /usr/share/zoneinfo/Asia/Seoul /etc/localtime\u0026#39;  # CentOS 6.4, 6.4, 6.5 contained broken line in \u0026#39;file_contexts\u0026#39; file, this removes it so we can apply other selinux labels sh \u0026#39;sed -i \u0026#34;/\\\\pid/d\u0026#34; /etc/selinux/targeted/contexts/files/file_contexts\u0026#39; selinux-relabel /etc/selinux/targeted/contexts/files/file_contexts /etc/selinux/targeted/contexts/files/file_contexts # relabel files that we were touching with correct SELinux labels selinux-relabel /etc/selinux/targeted/contexts/files/file_contexts /etc/sysconfig/network-scripts/ifcfg-eth0 selinux-relabel /etc/selinux/targeted/contexts/files/file_contexts /etc/sysconfig/network EOF 위와 같이 guestfish 명령을 통해 Image 변경 작업을 할 수 있도록 설정합니다.\nStart fast-vm 수정된 Image 를 이용하여 VM 을 생성 하도록 하겠습니다.\n[root@test-vm-host ~]# fast-vm create centos-6.9 30 [30][inf] using file /etc/fast-vm/config-centos-6.9.xml as libvirt XML [30][inf] using file /etc/fast-vm/hacks-centos-6.9.sh as hack file [30][inf] defining virtual machine \u0026#39;fastvm-centos-6.9-30\u0026#39; in libvirt Domain fastvm-centos-6.9-30 defined from /tmp/tmp.AUMMkEdrbo.xml Domain title updated successfully [30][inf] creating disk \u0026#39;fastvm-centos-6.9-30\u0026#39; [30][inf] adding static lease for 192.168.200.30 into libvirts DHCP [30][inf] applying hacks from /etc/fast-vm/hacks-centos-6.9.sh [30][inf] applying hacks finished [30][ok] VM \u0026#39;fastvm-centos-6.9-30\u0026#39; created VM ID 30 으로 VM 이 생성 되었습니다.\n[root@test-vm-host ~]# fast-vm start 30 [30][inf] starting VM fastvm-centos-6.9-30 (192.168.200.30) Domain fastvm-centos-6.9-30 started VM ID 30 의 VM 을 시작 하였습니다.\n[root@test-vm-host ~]# fast-vm ssh 30 [30][inf] checking the 192.168.200.30 for active SSH connection (ctrl+c to interrupt) ................[30][inf] [30]SSH ready Warning: Permanently added \u0026#39;192.168.200.30\u0026#39; (RSA) to the list of known hosts. root@192.168.200.30\u0026#39;s password: fast-vm ssh 명령을 통해 해당 VM 에 ssh 로 접속을 합니다.\n시스템이 부팅중일 경우, ssh 서비스가 시작 될 때까지 접속 대기합니다.\n 참고 : 기본적으로 사용되는 이미지의 root 비밀번호는 testtest 입니다.  [root@fastvm-centos-6-9-30 ~]# cat /etc/redhat-release CentOS release 6.9 (Final) [root@fastvm-centos-6-9-30 ~]# date Fri Aug 16 11:02:43 KST 2019 위와 같이 VM 이 생성 및 시작, hack file 이 적용된 것을 확인 할 수 있습니다.\nDelete fast-vm 아래 명령을 통해 사용을 다한 fast-vm 을 삭제 할 수 있습니다.\n[root@test-vm-host fast-vm]# fast-vm delete 30 [30][wrn] VM fastvm-centos-6.9-30 is active, forcefully stopping it Domain fastvm-centos-6.9-30 destroyed [30][inf] removing DHCP reservation 192.168.200.30 for 52:54:00:96:5c:80 Updated network fast-vm-nat persistent config and live state [30][inf] removing VM drive [30][inf] undefining VM fastvm-centos-6.9-30 from libvirt Domain fastvm-centos-6.9-30 has been undefined [30][ok] VM \u0026#39;fastvm-centos-6.9-30\u0026#39; deleted [root@test-vm-host fast-vm]# 해당 VM 이 삭제 되었습니다.\nfast-vm 에 PR 을 해보자!  fast-vm 은 Open Source Solution 으로 언제나 Github를 통해 PR 을 할 수 있습니다.\n저의 경우, 아래와 같은 이슈로 인해 PR 진행 하였습니다.\n firewalld deamon 의 disable 로 인한 배포 실패 지원하는 ansible 최소버전에 대한 이슈. ( yum Module update_only option 관련 )  위와 같이 PR 을 생성하고 현재는 Source 에 Merge 되었습니다.\n마치며 위에서 소개한 fast-vm 의 부분은 극히 필수적인 요소들입니다.\nlibvirtd 에서 제공하는 다양한 부분을 지원하고 있으며, 해당 내용은 아래 참고 문서 를 확인 바람니다.\n***사용 중 발생된 이슈/추가 기능 구현 관련에 대해 github 를 통해 PR 할 수 있습니다. ***\n참고 문서  github : https://github.com/OndrejHome/ansible.fast-vm-server Fast-VM User Guide : https://www.famera.cz/blog/fast-vm/user_guide.html Fast-VM Image List : https://www.famera.cz/blog/fast-vm/image_list.html  ","permalink":"https://chhanz88.github.io/post/2019-08-16-linux-fast-vm/","summary":"Fast-VM  Fast-VM 이란?  Fast-VM 라는 Open Source Solution 을 알게된 것은 Ondrej Faměra 라는 친구를 만나면서 입니다.\nThank You. Ondrej ^o^\n Fast-VM 은 Ondrej Faměra 가 만든 libvirtd 기반의 가상화 Provisioning Solution 입니다.\n기존의 libvirtd 기반의 가상화는 virt-manager 혹은 virsh 을 통해 VM 생성 및 운영을 하였습니다.\nFast-VM 을 이용하면 fast-vm 이라는 명령어 하나로 VM을 생성하고 관리 할 수 있습니다.\n다양한 Linux 배포판을 설치하고 테스트를 해야되는 저는 fast-vm 을 통해 여러가지 스트레스들이 사라졌습니다.","title":"[Linux] Fast-VM 설치 및 활용"},{"content":"[참고] Install sosreport Package  [root@fastvm-centos-7-6-ext-50 ~]# yum -y install sos ... 중략 ... Installed:  sos.noarch 0:3.6-19.el7.centos 위와 같이 sos Package 를 설치하면 sosreport 를 수집 할 수 있습니다.\nsosreport 생성 경로 변경  sosreport 는 기본적으로 아래와 같이 /var/tmp/sosreport-XXXX.tar.xz 로 생셩이 됩니다.\n/var 경로에 용량이 부족하거나 sosreport 로 수집된 파일의 크기가 큰 경우, 생성 위치를 변경 할 수 있습니다.\nsosreport 기본 경로 생성 로그 [root@fastvm-centos-7-6-ext-50 ~]# sosreport sosreport (version 3.6) ... 중략 ... Creating compressed archive... Your sosreport has been generated and saved in:  /var/tmp/sosreport-fastvm-centos-7-6-ext-50-2019-08-02-uhcfevv.tar.xz The checksum is: a4c9207647dd2ad74fc74d944329dca7 Please send this file to your support representative. sosreport 변경 아래와 같이 --tmp-dir 옵션을 사용합니다.\n[root@fastvm-centos-7-6-ext-50 ~]# sosreport --tmp-dir /tmp sosreport (version 3.6) This command will collect diagnostic and configuration information from this CentOS Linux system and installed applications. An archive containing the collected information will be generated in /tmp/sos.O8jeMy and may be provided to a CentOS support representative. Any information provided to CentOS will be treated in accordance with the published support policies at:  https://wiki.centos.org/ The generated archive may contain data considered sensitive and its content should be reviewed by the originating organization before being passed to any third party. No changes will be made to system configuration. Press ENTER to continue, or CTRL-C to quit. Please enter the case id that you are generating this report for []:  Setting up archive ...  Setting up plugins ...  Running plugins. Please wait ...  Finishing plugins [Running: yum]  Finished running plugins Creating compressed archive... Your sosreport has been generated and saved in:  /tmp/sosreport-fastvm-centos-7-6-ext-50-2019-08-02-gdjxiur.tar.xz The checksum is: 91c2374e9d1a48b901241a6f9786df5f Please send this file to your support representative. 위와 같이 sosreport 생성 경로를 변경하였습니다.\n해당 수집 방법은 sos Package Version 및 RHEL/CentOS Version 이 낮은 경우 다른 방법으로 수집을 해야 될 수 있습니다.\n위와 같은 경우 아래 첨부된 참고 자료 확인 바람니다.\n참고 자료   https://access.redhat.com/solutions/1847  ","permalink":"https://chhanz88.github.io/post/2019-08-01-linux-sosreport/","summary":"[참고] Install sosreport Package  [root@fastvm-centos-7-6-ext-50 ~]# yum -y install sos ... 중략 ... Installed:  sos.noarch 0:3.6-19.el7.centos 위와 같이 sos Package 를 설치하면 sosreport 를 수집 할 수 있습니다.\nsosreport 생성 경로 변경  sosreport 는 기본적으로 아래와 같이 /var/tmp/sosreport-XXXX.tar.xz 로 생셩이 됩니다.\n/var 경로에 용량이 부족하거나 sosreport 로 수집된 파일의 크기가 큰 경우, 생성 위치를 변경 할 수 있습니다.\nsosreport 기본 경로 생성 로그 [root@fastvm-centos-7-6-ext-50 ~]# sosreport sosreport (version 3.","title":"[Linux] sosreport 생성 경로 변경"},{"content":"Jekyll을 이용하여 Github Page를 만들어보자!  구글 검색을 하시다보면 많은 기술 자료들이 블로그를 통해 올라오는 것을 볼 수 있습니다.\n그 블로그들의 공통점은 github.io 의 도메인을 가지고 있다는 것입니다.\ngithub.io 와 Jekyll 을 이용하면 누구든 쉽게 개인 블로그를 만들고 많은 정보를 공유 할 수 있습니다.\n한번 직접 만들어 보겠습니다.\nRepository 생성  github 를 가입하고 이메일 인증까지 완료하면 다음과 같이 Repository 생성을 하도록 페이지가 나옵니다.\nRepository 의 이름을 chhanz-test.github.io 로 생성을 합니다.\nRepository 를 생성하고 위와 같이 Repository 초기화를 하는 단계가 나옵니다.\ngithub 를 잘 활용하시는 분들은 command line 을 이용하여 초기화를 진행해도 됩니다.\n간단한 페이지만 생성을 할 것이니, Web 에서 만들어 보겠습니다.\nREADME.md 파일에 MarkDown 문법을 이용하여 페이지를 작성합니다.\nMarkDown 문법에 대해서는 MarkDown - Basic-syntax 참조 바람니다.\nCommit 을 합니다.\n위와 같이 README.md 파일이 생성된 것을 볼 수 있습니다.\nhttps://chhanz-test.github.io 로 접속을 해보겠습니다.\n위와 같이 README.md 에 작성한 내용이 웹서비스가 되는 것을 볼 수 있습니다.\n하지만 우린 이런 웹페이지를 원한 것은 아니죠\u0026hellip;\nGibhub 테마  다른 사람들의 번쩍번쩍한 웹페이지와 같이 우리의 웹페이지도 반짝이게 만들어보겠습니다.\nRepository 에서 Setting 메뉴로 들어갑니다.\n해당 메뉴의 하단 부분에 Github Pages 라는 항목이 있습니다.\n여기서 테마를 선택 할 수 있습니다. Choose a Theme 를 선택합니다.\nGithub 에서 기본적으로 제공되는 테마들을 볼 수 있습니다.\n예제로 테마 하나를 선택해보겠습니다.\n다크한 테마로 Select Theme 를 합니다.\n_config.yml 파일이 생성이 되고 특정 테마가 적용된 것을 볼 수 있습니다.\n다크 다크 하네요.\n이후에는 Jekyll 구문을 이용하여 웹페이지를 풍성하게 만들 수 있습니다.\n포스팅, 메뉴, Sitemap 등등을 말이죠.\nhttp://jekyllrb-ko.github.io/docs/usage/\n위 Jekyll 문서를 참고하여 작업을 하면 됩니다.\n일일이 문서를 찾아서 페이지를 만드는 것은 많은 공부가 필요합니다.\ngithub 에는 능력자 분들이 많습니다.\n그 분들의 힘을 빌려봅시다 :)\nFork Jekyll Theme  능력자 분들은 이런 사이트를 제공해주셨습니다.\nhttp://jekyllthemes.org/\n해당 사이트에서 마음에 드는 테마를 선택하면 해당 테마의 Github Repository 로 연결됩니다.\n마음에 드는 테마를 Fork 하도록 하겠습니다.\nFork 가 완료되면 아래와 같이 나의 Github Repository 에 Forked 된 Repository 가 생성이 됩니다.\nForked 된 Repository 를 Clone 합니다.\n(Jekyll 을 이용하여 Build 하는 과정은 해당 테마에서 제공한 README 를 참조합니다.)\n간단하게 웹페이지를 수정해보겠습니다.\n페이지의 제목, 설명등등을 수정했습니다.\nJekyll 을 이용해서 Build 합니다.\n위와 같이 MarkDown 문법을 사용해서 글을 작성하였습니다.\nJekyll 에서 신규 파일 생성을 확인하고 웹페이지에 반영합니다.\n새로 작성한 글이 잘 나오는지 확인합니다.\n게시글의 내용까지 완벽합니다!!\n이제 이렇게 수정된 소스를 chhanz-test.github.io 의 Repository 로 넣고 git push 를 합니다.\n실제 https://chhanz-test.github.io/ 에 접속하면 !!!\nJekyll 을 이용하여 테스트한 웹페이지가 Published 되는 것을 볼 수 있습니다.\n","permalink":"https://chhanz88.github.io/post/2019-07-24-github-io/","summary":"Jekyll을 이용하여 Github Page를 만들어보자!  구글 검색을 하시다보면 많은 기술 자료들이 블로그를 통해 올라오는 것을 볼 수 있습니다.\n그 블로그들의 공통점은 github.io 의 도메인을 가지고 있다는 것입니다.\ngithub.io 와 Jekyll 을 이용하면 누구든 쉽게 개인 블로그를 만들고 많은 정보를 공유 할 수 있습니다.\n한번 직접 만들어 보겠습니다.\nRepository 생성  github 를 가입하고 이메일 인증까지 완료하면 다음과 같이 Repository 생성을 하도록 페이지가 나옵니다.\nRepository 의 이름을 chhanz-test.github.io 로 생성을 합니다.","title":"[Github] Jekyll을 이용하여 Github Page를 만들어보자!"},{"content":"이전 포스팅 다시 보기    [Openshift] Openshift Origin v3.11 설치, App 배포 [Openshift] Openshift Web Console 을 이용한 배포   Openshift 의 HPA 를 이용한 Auto-Scaling 구현  Openshift 에서 Horizontal Pod Autoscaler(이하 HPA) 를 이용하여 설정한 CPU 사용률을 기반으로 Replicaset, Deployment 의 Pod 수를 자동으로 Scaling 할 수 있습니다.\nHPA 를 하기 위해서는 Pod 의 부하에 대해 모니터링 및 수집을 하는 Metrics-Server 가 필요합니다.\n아래에서 Openshift 에 Metrics-Server 를 배포하고 성능 수집을 해보도록 하겠습니다.\nMetrics-Server 배포  기본적으로 Openshift 설치 과정에서는 metric-server 는 기본 배포로 설정이 안되어 있습니다.\n추가로 설치를 진행 해야됩니다.\n 참고 문서: okd v3.11 Documentation - Pod Autoscaling  아래와 같이 이전 포스팅에서 사용한 Openshift Playbook 을 이용합니다.\n# ansible-playbook -i inventory/hosts.localhost /usr/share/ansible/openshift-ansible/playbooks/metrics-server/config.yml -e openshift_metrics_server_install=true ... \u0026lt; 중략 \u0026gt; ...  PLAY RECAP ******************************************************************************************************************************************************************************************************************************************** localhost : ok=119 changed=13 unreachable=0 failed=0   INSTALLER STATUS ************************************************************************************************************************************************************************************************************************************** Initialization : Complete (0:00:59) metrics-server Install : Complete (0:00:56) Thursday 06 June 2019 13:28:05 +0200 (0:00:00.105) 0:01:56.691 ********* =============================================================================== Run variable sanity checks -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 23.51s Gathering Facts -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 4.03s Gather Cluster facts --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 3.00s metrics_server : Ensure metrics-server namespace is present ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 2.80s metrics_server : slurp ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 2.44s Initialize openshift.node.sdn_mtu -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 2.28s get openshift_current_version ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 2.09s metrics_server : generate metrics-server keys -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1.64s metrics_server : generate metrics-server secret template --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1.26s metrics_server : Applying /tmp/openshift-metrics-server-ansible-56a8WR/templates/metrics-server-certs.yaml ------------------------------------------------------------------------------------------------------------------------------------- 1.17s metrics_server : Applying /tmp/openshift-metrics-server-ansible-56a8WR/templates/metrics-server-sa.yaml ---------------------------------------------------------------------------------------------------------------------------------------- 1.14s metrics_server : Applying /tmp/openshift-metrics-server-ansible-56a8WR/templates/metrics-server-resource-reader-rolebinding.yaml --------------------------------------------------------------------------------------------------------------- 1.14s metrics_server : Applying /tmp/openshift-metrics-server-ansible-56a8WR/templates/metrics-server-service.yaml ----------------------------------------------------------------------------------------------------------------------------------- 1.13s metrics_server : generate ca certificate chain ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1.13s metrics_server : Applying /tmp/openshift-metrics-server-ansible-56a8WR/templates/metrics-server-auth-delegator-rolebinding.yaml ---------------------------------------------------------------------------------------------------------------- 1.12s metrics_server : Applying /tmp/openshift-metrics-server-ansible-56a8WR/templates/extension-apiserver-authentication-reader-metrics-server-rolebinding.yaml ------------------------------------------------------------------------------------- 1.09s metrics_server : Applying /tmp/openshift-metrics-server-ansible-56a8WR/templates/metrics-server-apiservice.yaml -------------------------------------------------------------------------------------------------------------------------------- 1.08s metrics_server : Applying /tmp/openshift-metrics-server-ansible-56a8WR/templates/metrics-server-deployment.yaml -------------------------------------------------------------------------------------------------------------------------------- 1.06s metrics_server : Checking generation of Service metrics-server --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1.03s metrics_server : Determine change status of ClusterRoleBinding system:metrics-server ----------------------------------------------------------------------------------------------------------------------------------------------------------- 1.03s [root@master openshift-ansible]# 위와 같이 배포가 완료되면, oc 명령을 통해 metrics-server 가 정상 작동하는지 확인 할 수 있습니다.\n[root@master ~]# oc adm top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% master.example.com 907m 11% 6146Mi 38% //openshift-metrics-server namespaces 에 metrics-server 가 배포 된 것을 확인 할 수 있습니다. [root@master ~]# oc adm top pod --all-namespaces NAMESPACE NAME CPU(cores) MEMORY(bytes) default docker-registry-1-9t2zp 2m 32Mi default registry-console-1-n4v9t 0m 1Mi default router-1-58cpz 4m 45Mi kube-service-catalog apiserver-pnsnf 1m 51Mi kube-service-catalog controller-manager-qsq5m 10m 22Mi kube-system master-api-master.example.com 96m 1061Mi kube-system master-controllers-master.example.com 99m 532Mi kube-system master-etcd-master.example.com 36m 408Mi openshift-ansible-service-broker asb-1-k9nww 2m 19Mi openshift-console console-5677c7c58d-n5z5h 2m 9Mi openshift-metrics-server metrics-server-7bf4cf7dd4-svfs7 1m 30Mi openshift-monitoring alertmanager-main-0 4m 25Mi openshift-monitoring alertmanager-main-1 7m 25Mi openshift-monitoring alertmanager-main-2 6m 26Mi openshift-monitoring cluster-monitoring-operator-6465f8fbc7-f4w4t 0m 35Mi openshift-monitoring grafana-6b9f85786f-z9gpj 6m 42Mi openshift-monitoring kube-state-metrics-7449d589bc-qsrvh 7m 67Mi openshift-monitoring node-exporter-hjh46 2m 21Mi openshift-monitoring prometheus-k8s-0 64m 495Mi openshift-monitoring prometheus-k8s-1 47m 485Mi openshift-monitoring prometheus-operator-6644b8cd54-cv2hf 0m 25Mi openshift-node sync-fnd6m 0m 7Mi openshift-sdn ovs-ssckw 5m 80Mi openshift-sdn sdn-77shb 12m 37Mi openshift-template-service-broker apiserver-2mpk8 4m 30Mi openshift-web-console webconsole-7df4f9f689-mxfkj 15m 25Mi sample-project chhanz-hello-example 0m 0Mi sample-project http-example-1-5dhj5 3m 9Mi sample-project sampleapp-2-hbmzs 7m 222Mi test-project load-generator-779c5f458c-g4kff 0m 0Mi test-project mysample-3-cr8cp 1m 215Mi test-project mysample-3-dvrlp 1m 206Mi test-project mysample-3-npgq7 1m 228Mi test-project mysample-3-t9dwg 1m 214Mi test-project mysample-3-vtm5j 1m 217Mi test-project nginx-1-rjd86 0m 1Mi test-project php-apache-b5b7bd9c8-hpcw8 0m 9Mi test-project ruby-ex-1-nkfxn 1m 83Mi [root@master ~]# 참고로 $ oc top nodes 명령을 통해 성능 수집이 정상적으로 동작하려면 배포 후, 일정 시간이 지나야 됩니다.\nHPA 구현  HPA 를 이용한 Auto-Scaling 을 구현하기 위해 아래와 같이 신규 Project 를 생성합니다.\n[root@master ~]# oc new-project hpa-project Now using project \u0026#34;hpa-project\u0026#34; on server \u0026#34;https://master.example.com:8443\u0026#34;.  You can add applications to this project with the \u0026#39;new-app\u0026#39; command. For example, try:   oc new-app centos/ruby-25-centos7~https://github.com/sclorg/ruby-ex.git  to build a new example application in Ruby. [root@master ~]# [root@master ~]# oc get po No resources found. [root@master ~]# [root@master ~]# 부하(load)를 감지하고 응답 할 수 있는 App 을 생성합니다.\n 참고 문서 - Horizontal Pod Autoscaler 연습  Dockerfile 생성 및 Build # ls Dockerfile index.php # cat Dockerfile FROM php:5-apache ADD index.php /var/www/html/index.php RUN chmod a+rx index.php # cat index.php \u0026lt;?php  $x = 0.0001;  for ($i = 0; $i \u0026lt;= 1000000; $i++) {  $x += sqrt($x);  }  echo \u0026#34;OK!\u0026#34;; ?\u0026gt; # docker build -t hpa-example . Sending build context to Docker daemon 3.072kB Step 1/3 : FROM php:5-apache 5-apache: Pulling from library/php 5e6ec7f28fb7: Pull complete cf165947b5b7: Pull complete 7bd37682846d: Pull complete 99daf8e838e1: Pull complete ae320713efba: Pull complete ebcb99c48d8c: Pull complete 9867e71b4ab6: Pull complete 936eb418164a: Pull complete bc298e7adaf7: Pull complete ccd61b587bcd: Pull complete b2d4b347f67c: Pull complete 56e9dde34152: Pull complete 9ad99b17eb78: Pull complete Digest: sha256:0a40fd273961b99d8afe69a61a68c73c04bc0caa9de384d3b2dd9e7986eec86d Status: Downloaded newer image for php:5-apache  ---\u0026gt; 24c791995c1e Step 2/3 : ADD index.php /var/www/html/index.php  ---\u0026gt; bcc3ff35ceb8 Step 3/3 : RUN chmod a+rx index.php  ---\u0026gt; Running in d1e173fb27a3 Removing intermediate container d1e173fb27a3  ---\u0026gt; f4bb43246866 Successfully built f4bb43246866 Successfully tagged hpa-example:latest  # docker push han0495/hpa-example The push refers to repository [docker.io/han0495/hpa-example] b42dc42a2d18: Pushed 056e15bb815c: Pushed 1aab22401f12: Mounted from library/php 13ab94c9aa15: Mounted from library/php 588ee8a7eeec: Mounted from library/php bebcda512a6d: Mounted from library/php 5ce59bfe8a3a: Mounted from library/php d89c229e40ae: Mounted from library/php 9311481e1bdc: Mounted from library/php 4dd88f8a7689: Mounted from library/php b1841504f6c8: Mounted from library/php 6eb3cfd4ad9e: Mounted from library/php 82bded2c3a7c: Mounted from library/php b87a266e6a9c: Mounted from library/php 3c816b4ead84: Mounted from library/php latest: digest: sha256:07c8ebfbe5b8084b878d0ed4ebafe5baad124b0b932e4f56d81480fbf715f03f size: 3449 App 배포 php 로 만들어진 App 을 Openshift 에 배포합니다.\n[root@master ~]# oc login -u admin Logged into \u0026#34;https://master.example.com:8443\u0026#34; as \u0026#34;admin\u0026#34; using existing credentials.  You have access to the following projects and can switch between them with \u0026#39;oc project \u0026lt;projectname\u0026gt;\u0026#39;:   default  * hpa-project  kube-public  kube-service-catalog  kube-system  management-infra  openshift  openshift-ansible-service-broker  openshift-console  openshift-infra  openshift-logging  openshift-metrics-server  openshift-monitoring  openshift-node  openshift-sdn  openshift-template-service-broker  openshift-web-console  sample-project  test-project  Using project \u0026#34;hpa-project\u0026#34;.  [root@master ~]# oc run hpa-example-pod --image=han0495/hpa-example --expose --port 80 --requests=cpu=200m service/hpa-example-pod created deploymentconfig.apps.openshift.io/hpa-example-pod created [root@master ~]# oc get all NAME READY STATUS RESTARTS AGE pod/hpa-example-pod-1-deploy 1/1 Running 0 9s pod/hpa-example-pod-1-ngj64 0/1 ContainerCreating 0 6s  NAME DESIRED CURRENT READY AGE replicationcontroller/hpa-example-pod-1 1 1 0 9s  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/hpa-example-pod ClusterIP 172.30.193.122 \u0026lt;none\u0026gt; 80/TCP 9s  NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/hpa-example-pod 1 1 1 config 위와 같이 배포가 완료 되었습니다.\n해당 Pod 은 CPU 리소스를 200m 까지 요청 수 있도록 설정 하였습니다.\nHPA 생성 생성된 Pod 에 HPA 를 만들고 연결합니다.\n[root@master ~]# oc autoscale deploymentconfig.apps.openshift.io/hpa-example-pod --min 1 --max 8 --cpu-percent=70 horizontalpodautoscaler.autoscaling/hpa-example-pod autoscaled   [root@master ~]# oc describe hpa Name: hpa-example-pod Namespace: hpa-project Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; CreationTimestamp: Mon, 24 Jun 2019 08:20:40 +0200 Reference: DeploymentConfig/hpa-example-pod Metrics: ( current / target )  resource cpu on pods (as a percentage of request): 0% (0) / 70% Min replicas: 1 Max replicas: 8 DeploymentConfig pods: 1 current / 1 desired Conditions:  Type Status Reason Message  ---- ------ ------ -------  AbleToScale True ReadyForNewScale the last scale time was sufficiently old as to warrant a new scale  ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)  ScalingLimited True TooFewReplicas the desired replica count is increasing faster than the maximum scale rate Events: \u0026lt;none\u0026gt; [root@master ~]# 생성된 HPA 의 상세 정보를 보도록 하겠습니다.\n부하(load)가 발생되면 해당 Pod 의 평균 CPU 사용량이 70% 을 유지하도록 설정 되어 있습니다. 최소 1개의 Pod 을 만들 수 있고, 최대 8개까지 Pod 을 증가 시킬 수 있습니다.\n부하 테스트  Openshift 에서 제공하는 Grafana 를 이용하여 실제 Pod 에 발생되는 부하(load)를 모니터링 하도록 하겠습니다.\nCPU 의 사용량은 0% 로 거의 부하(load)가 없습니다.\n해당 Pod 에 부하(load)를 발생 시키도록 하겠습니다.\n[root@master ~]# oc run -it load-generator --image=busybox [root@master ~]# oc get all NAME READY STATUS RESTARTS AGE pod/hpa-example-pod-1-ngj64 1/1 Running 0 16m pod/load-generator-1-tl2s4 1/1 Running 0 29s  NAME DESIRED CURRENT READY AGE replicationcontroller/hpa-example-pod-1 1 1 1 16m replicationcontroller/load-generator-1 1 1 1 32s  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/hpa-example-pod ClusterIP 172.30.193.122 \u0026lt;none\u0026gt; 80/TCP 16m  NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE horizontalpodautoscaler.autoscaling/hpa-example-pod DeploymentConfig/hpa-example-pod 0%/70% 1 8 1 13m  NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/hpa-example-pod 1 1 1 config deploymentconfig.apps.openshift.io/load-generator 1 1 1 config  [root@master ~]# oc exec load-generator-1-f9xrd -ti /bin/sh / # / # while true; do wget -q -O- http://hpa-example-pod.hpa-project.svc.cluster.local; done 위와 같이 busybox를 이용하여 load-generator 를 만들었습니다.\n# while true; do wget -q -O- http://hpa-example-pod.hpa-project.svc.cluster.local; done 명령을 통해 App 에 부하(load)를 유발합니다.\n점점 HPA 에서 부하(load)를 감지합니다.\n본격적으로 HPA 에서 부하(load)를 감지하고 CPU 사용률 평균 70%를 유지하기 위해 Pod 를 Auto-Scaling 합니다.\n지속된 부하(load)로 인해 Pod이 7개까지 증가 되었습니다.\nGrafana에서 Pod 상태를 보도록 하겠습니다.\n다량의 부하(load)로 인해 설정된 최대 Pod 수 8개까지 증가 된 것을 확인 할 수 있습니다.\nGrafana 에서도 Pod 8개가 Auto-Scaling 되어 작동하는 것을 확인 하였습니다.\n이처럼 HPA에서 부하(load)를 감지하고 Auto-Scaling 을 하여 정상적인 서비스를 유지 할 수 있습니다.\n부하 테스트 종료 부하(load) 발생을 중지하면 HPA에서 감지하고 다시 Pod 의 수를 조절합니다.\n이번 포스팅에서는 HPA 를 이용하여 Auto-Scaling 을 구현 하였습니다.\nOpenshift의 HPA 을 이용하면 유연하고 지속적인 서비스를 구현하고 운영 할 수 있습니다.\n참고 자료   okd Documentation - pod_autoscaling k8s Documentation - Horizontal Pod Autoscaler 연습  ","permalink":"https://chhanz88.github.io/post/2019-07-15-okd-hpa/","summary":"이전 포스팅 다시 보기    [Openshift] Openshift Origin v3.11 설치, App 배포 [Openshift] Openshift Web Console 을 이용한 배포   Openshift 의 HPA 를 이용한 Auto-Scaling 구현  Openshift 에서 Horizontal Pod Autoscaler(이하 HPA) 를 이용하여 설정한 CPU 사용률을 기반으로 Replicaset, Deployment 의 Pod 수를 자동으로 Scaling 할 수 있습니다.\nHPA 를 하기 위해서는 Pod 의 부하에 대해 모니터링 및 수집을 하는 Metrics-Server 가 필요합니다.\n아래에서 Openshift 에 Metrics-Server 를 배포하고 성능 수집을 해보도록 하겠습니다.","title":"[Openshift] Openshift 의 HPA 를 이용한 Auto-Scaling 구현"},{"content":"[Openshift] Openshift Web Console 을 이용한 배포  안녕하세요. chhanz 입니다.\n이전 포스팅인 Openshift Origin 설치에 이어서 Openshift Web Console 을 살펴보고 Web Console 을 이용하여 APP 배포도 해보겠습니다.\nOpenshift Web Console  기본적으로 설치가 완료된 Openshift는 Web Console 이 expose 되어 있습니다.\n생성되어 있는 route 를 확인하고 접속 해보도록 하겠습니다.\n위와 같이 모든 namespace 의 route 를 확인 할 수 있습니다.\n현재 테스트 시스템에서는 http://console.apps.example.com 로 route 가 생성이 되어 있습니다.\n해당 Domain 으로 접속 해보겠습니다.\nOpenshfit Web Console 에 접근하였습니다.\nOpenshift Web Console 에서는 기본적으로 3가지의 Web Console 을 제공합니다.\n  Service Catalog Application Console Cluster Console   지금부터 각각의 Web Console 을 확인해보고 사용해 보겠습니다.\nService Catalog  Service Catalog 는 Openshift 에서 제공되는 기본적인 APP 을 보여주며, 개발자는 Service Catalog 를 통해 손쉽게 APP 을 배포 할 수 있습니다.\n이처럼 제공되는 Service 가 많습니다.\n한번 Service Catalog 를 통해 APP 를 배포 해보도록 하겠습니다.\nApache HTTPD APP 을 선택합니다.\nAPP 이 배포될 Project 및 사용될 Container Image, 사용될 Source 를 입력합니다.\nAPP 에 설정된 결과값이 표시되며, APP 배포가 시작 되었습니다.\nAPP 이 배포중에 있습니다.\n배포가 완료된 APP 의 상세 로그를 보면 Container Image를 Pull 하고 Source를 Clone 하여 해당 APP 에 맞게 자동으로 S2I(Source To Image) 를 진행합니다.\n정상적으로 APP 가 배포되고, 서비스 되는 것을 볼 수 있습니다.\nApplication Console  Application Console 는 현재 Openshift 에서 서비스 중인 APP 에 대해 상세한 관리가 가능한 Web Console 입니다.\n예제를 보면서 기능들을 확인하도록 하겠습니다.\nOpenshift Cluster 에 만들어진 Project 목록이 표기되며, 이전 포스팅에서 sample APP 를 배포한 sample-project 를 선택합니다.\nDetail APP \u0026amp; Scale Out APP Pods 옆 ^ 모양의 버튼을 누르면 바로 내가 원하는 만큼 쉽게 Scale Out 할 수 있습니다.\nBuild APP New Version 의 Build 도 Web Console 을 통해 쉽게 Build 하고 배포 할 수 있습니다.\n#1 로 Build 되어 있습니다. 아래와 같이 #1의 Build 로그 또한 확인이 가능합니다.\n#2 신규 Source 를 Git 에서 Clone 하여 New Version 으로 Build 하겠습니다.\n오른쪽 상단의 Start Build 버튼을 누르면 바로 Build 가 시작됩니다.\n그럼 아래와 같이 Build 가 시작되고 완료가 되는 것을 볼 수 있습니다.\nMonitoring 위와 같이 많은 작업을 Web Console로 할 수 있습니다. 이런 많은 APP 들을 한번에 Monitoring 메뉴를 통해 한번에 모니터링 할 수 있습니다.\nProject 내에 모든 리소스들이 생성되고 추가되고 Build 되고 삭제되는 것 등등 많은 Event 들이 기록되며 각각의 상세 내역도 확인이 가능합니다.\nCluster Console  마지막으로 Cluster Console 이 있습니다.\nCluster Console 은 Openshift Cluster 의 인프라 영역까지 관리가 가능한 Web Console 입니다.\n지금까지 설명한 Service Catalog, Application Console 은 개발자를 타겟으로한 Console 이라면, Cluster Console 은 운영자를 타겟으로 만들어진 Console 이라 보면 좋습니다.\nProject \u0026amp;\u0026amp; Status Cluster Console 에 접근하면 Openshift 에 만들어진 Project 들을 확인 할 수 있으며, 해당 Project 의 자세한 현황도 파악 할 수 있습니다.\nCheck Resource Project 내에 생성된 Pod, Network 등을 쉽게 점검하고 확인 할 수 있습니다.\nCreate Pod Web Console 을 통해 직접 Yaml 파일 내용을 입력하여 바로 Pod 을 생성 할 수 있습니다.\n위와 같이 직접 수정해서 Pod 을 생성하도록 하겠습니다.\n수정한 내역이 반영되어 바로 Pod 이 생성 된 것을 볼 수 있습니다.\n마치며 제가 사용해본 Openshift 는 Cli 환경에서 충분이 많은 기능을 구현했다고 생각했습니다.\n하지만 Cli 환경이 미숙한 사용자들의 입장에서 본다면 Openshift 의 Web Console 은 정말 매력적인 기능일 것 같습니다.\n쉽게 Cluster 의 상태를 확인 할 수 있고, 서비스를 배포하고 운영 할 수 있었습니다.\n다음 포스팅에서는 Openshfit 의 HPA 를 이용하여 Auto Scaling 을 구현해보도록 하겠습니다.\n감사합니다.\n","permalink":"https://chhanz88.github.io/post/2019-06-24-overview-okd-gui/","summary":"[Openshift] Openshift Web Console 을 이용한 배포  안녕하세요. chhanz 입니다.\n이전 포스팅인 Openshift Origin 설치에 이어서 Openshift Web Console 을 살펴보고 Web Console 을 이용하여 APP 배포도 해보겠습니다.\nOpenshift Web Console  기본적으로 설치가 완료된 Openshift는 Web Console 이 expose 되어 있습니다.\n생성되어 있는 route 를 확인하고 접속 해보도록 하겠습니다.\n위와 같이 모든 namespace 의 route 를 확인 할 수 있습니다.\n현재 테스트 시스템에서는 http://console.apps.example.com 로 route 가 생성이 되어 있습니다.","title":"[Openshift] Openshift Web Console 을 이용한 배포"},{"content":"VMware ESXi에서 Linux SCSI ID 확인 방법  VMware ESXi 상에서 운영중인 Linux 의 경우, 아래와 같이 DISK 의 SCSI ID 가 확인이 불가능합니다.\nGuestOS 의 DISK 를 다른 GuestOS 로 이관을 하거나, GuestOS 의 VMX 를 재생성 하였을때, VMDK 의 순서가 확인이 안될 경우\u0026hellip;\n이러한 경우로 인해 명확하게 어떤 VMDK 가 실제로 운영체제에서 어떤 DISK 로 사용 되었는지 확인이 필요합니다.\n실제로 VMware 시스템에서는 아래와 같이 SCSI ID 가 확인이 기본적으로 안됩니다.\n이처럼 SCSI ID 가 확인이 불가능합니다.\nVMware ESXi 설정 변경  ESXi 에서 GuestOS를 수정합니다.( 해당 설정을 변경하기 위해 GuestOS 는 중지해야합니다.)\n[VM 선택] - [작업] - [설정 편집] - [VM 옵션] - [고급] - [구성 매개 변수] - [구성 편집] 을 선택합니다.\n위와 같이 disk.enableuuid=true 항목을 추가합니다.\n설정 확인  VM 설정을 하면 아래와 같이 SCSI ID를 정상적으로 확인 할 수 있습니다.\n참고 자료   https://blogs.vmware.com/kb/2013/03/setting-disk-enableuuidtrue-in-vmware-data-protection.html  ","permalink":"https://chhanz88.github.io/post/2019-06-16-vmware-scsi-id-check/","summary":"VMware ESXi에서 Linux SCSI ID 확인 방법  VMware ESXi 상에서 운영중인 Linux 의 경우, 아래와 같이 DISK 의 SCSI ID 가 확인이 불가능합니다.\nGuestOS 의 DISK 를 다른 GuestOS 로 이관을 하거나, GuestOS 의 VMX 를 재생성 하였을때, VMDK 의 순서가 확인이 안될 경우\u0026hellip;\n이러한 경우로 인해 명확하게 어떤 VMDK 가 실제로 운영체제에서 어떤 DISK 로 사용 되었는지 확인이 필요합니다.\n실제로 VMware 시스템에서는 아래와 같이 SCSI ID 가 확인이 기본적으로 안됩니다.","title":"[VMware] Linux SCSI ID 확인 방법"},{"content":"안녕하세요 chhanz 입니다.\n이번 포스팅에서는 Kubernetes에 기반을 둔 Developer-Oriented PaaS 인 Openshift를 살펴볼 것입니다.\nOpenshift 란?  OpenShift 는 개발자 및 IT 운영팀을 단일 플랫폼에서 통합하여, 하이브리드 클라우드 및 멀티 클라우드 인프라 전반에서 애플리케이션을 일관되게 구축, 배포 및 관리하도록 지원하는 플랫폼입니다.\nOpenshift 이점  주요 이점  애플리케이션 라이프사이클 전반에 걸쳐 운영 및 개발팀에서 더 큰 가치 실현 애플리케이션 개발 주기 단축 및 소프트웨어 배포 빈도 증가 하이브리드 클라우드 및 멀티 클라우드 전반에서 IT 운영 비용 절감 및 애플리케이션 이식성 실현  개발팀을 위한 이점   Openshift는 개발자에게 셀프 서비스 방식으로 애플리케이션과 컴포넌트를 프로비저닝, 빌드 및 배포하도록 지원하는 최적의 플랫폼입니다. Source To Image(S2I) 프로세스 같은 자동화된 워크플로우 덕분에 소스 형상 관리의 소스 코드를 즉시 실행 가능한 도커 포맷 컨테이너 이미지로 간단하게 생성할 수 있습니다. Openshift는 CI(Continuous Integration)와 CD(Continuous Delivery)의 통합 툴을 제공하기 때문에 모든 조직에게 이상적인 솔루션이 될 수 있습니다.   운영팀을 위한 이점   Openshift는 IT 운영팀에 애플리케이션 빌드 배포 자동화와 정책 기반 권한 관리를 지원하는 안전한 엔터프라이즈급 Kubernetes를 제공합니다. 클러스터 서비스, 스케줄링 그리고 오케스트레이션을 통해 부하 분산과 자동 스케일링 기능을 제공합니다. 보안 기능을 통해 테넌트가 다른 애플리케이션이나 기본 호스트에 지장을 주지 않도록 방지합니다. Openshift는 Persistent Storage를 Linux 컨테이너에 직접 연결할 수 있기 때문에 IT 조직은 하나의 플랫폼에서 스테이트풀(Stateful) 및 스테이트리스(Stateless) 애플리케이션을 모두 실행할 수 있습니다.   Openshift 종류  Openshift 에는 다양한 버전이 존재합니다.\n OpenShift Origin  OpenShift Online, OpenShift Dedicated 및 OpenShift Container Platform에서 사용되는 업스트림 커뮤니티 프로젝트입니다.   OpenShift Container Platform  OpenShift Container Platform은 Red Hat이 제공하고 지원하는 엔터프라이즈 버전입니다.   OpenShift Online  OpenShift Online은 Red Hat의 호스팅형 퍼블릭 PaaS 로서 클라우드에서 애플리케이션 개발, 구축, 배포, 호스팅 솔루션을 제공합니다.   OpenShift Dedicated  OpenShift Dedicated는 퍼블릭 클라우드에서 관리형 싱글 테넌트 OpenShift 환경을 제공합니다. 전체 OpenShift Cluster를 기업 전용 솔루션으로 구축하고 Red Hat을 통해 종합적으로 관리합니다.    Openshift Origin 설치  Openshift 설치 테스트 환경은 아래와 같습니다.\n CentOS Linux release 7.6.1810 (Core)\nOpenshift Origin v3.11\nDocker v1.13.1\n Openshift 를 설치하기 위해서는 기본적으로 필요한 OS 설정 및 Infra 구성이 필요합니다.\n  OS 설정\n SELinux : Enforcing Firewalld Disable / iptables Enable (설치간 자동으로 설정 진행됨) NetworkManager Enable    추가 필요 Infra 요소\n Internal DNS 서버 (이번 포스팅에서는 DNS 시스템 구성 내용은 제외되어 있습니다.)    위와 같이 Openshift 를 배포하기 위해서는 기본 설정이 필요합니다.\n주요 OS 설정 부분  /etc/sysconfig/network-script/ 의 내용은 아래와 같은 내부 Internal DNS 및 Upstream DNS 설정 해야됩니다.\n# cat /etc/sysconfig/network-scripts/ifcfg-eth1 TYPE=Ethernet BOOTPROTO=static DEFROUTE=yes NAME=eth1 UUID=XXXXXXXX-XXXXX-XXXX-XXXX-XXXXXXXXXXXXX DEVICE=eth1 ONBOOT=yes IPADDR=192.168.XX.50 NETMASK=255.255.255.0 GATEWAY=192.168.XX.XX DNS1=192.168.XX.111 # Internal DNS DNS2=1.1.1.1 # Upstream SELinux 설정 확인\n# cat /etc/selinux/config ... SELINUX=enforcing # Enforcing 설정 ... DNS 설정 부분  Internal DNS 는 아래와 같이 Zone File 을 설정합니다.\n# cat /var/named/example.com.zone $TTL 3H @\tIN SOA\tns.example.com. root.example.com. ( \t0\t; serial \t1D\t; refresh \t1H\t; retry \t1W\t; expire \t3H )\t; minimum  \tIN NS ns.example.com. ns\tIN\tA\t192.168.XX.111 *\tIN\tA\t192.168.XX.50 # wildcard DNS 설정 master\tIN\tA\t192.168.XX.50 ; wildcard DNS 설정을 합니다.\n참고자료 : https://docs.openshift.com/container-platform/3.11/install/prerequisites.html#wildcard-dns-prereq\nRequirements Package 설치  Openshift 설치를 하기 위해서 centos-openshift-origin311 Repository 를 Enable 해야됩니다.\n[root@master ~]# yum install centos-release-openshift-origin311.noarch Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile  * base: ftp.nara.wide.ad.jp  * extras: data.aonenetworks.kr  * updates: ftp.nara.wide.ad.jp Resolving Dependencies --\u0026gt; Running transaction check ---\u0026gt; Package centos-release-openshift-origin311.noarch 0:1-2.el7.centos will be installed --\u0026gt; Processing Dependency: centos-release-paas-common for package: centos-release-openshift-origin311-1-2.el7.centos.noarch --\u0026gt; Processing Dependency: centos-release-ansible26 for package: centos-release-openshift-origin311-1-2.el7.centos.noarch --\u0026gt; Running transaction check ---\u0026gt; Package centos-release-ansible26.noarch 0:1-3.el7.centos will be installed --\u0026gt; Processing Dependency: centos-release-configmanagement for package: centos-release-ansible26-1-3.el7.centos.noarch ---\u0026gt; Package centos-release-paas-common.noarch 0:1-1.el7.centos will be installed --\u0026gt; Running transaction check ---\u0026gt; Package centos-release-configmanagement.noarch 0:1-1.el7.centos will be installed --\u0026gt; Finished Dependency Resolution  Dependencies Resolved  ===================================================================================================================================================================================================================================================================================================  Package Arch Version Repository Size =================================================================================================================================================================================================================================================================================================== Installing:  centos-release-openshift-origin311 noarch 1-2.el7.centos extras 11 k Installing for dependencies:  centos-release-ansible26 noarch 1-3.el7.centos extras 4.1 k  centos-release-configmanagement noarch 1-1.el7.centos extras 4.3 k  centos-release-paas-common noarch 1-1.el7.centos extras 11 k  Transaction Summary =================================================================================================================================================================================================================================================================================================== Install 1 Package (+3 Dependent packages)  Total download size: 31 k Installed size: 39 k Is this ok [y/d/N]:y  ... Running transaction  Installing : centos-release-configmanagement-1-1.el7.centos.noarch 1/4  Installing : centos-release-ansible26-1-3.el7.centos.noarch 2/4  Installing : centos-release-paas-common-1-1.el7.centos.noarch 3/4  Installing : centos-release-openshift-origin311-1-2.el7.centos.noarch 4/4  Verifying : centos-release-paas-common-1-1.el7.centos.noarch 1/4  Verifying : centos-release-configmanagement-1-1.el7.centos.noarch 2/4  Verifying : centos-release-openshift-origin311-1-2.el7.centos.noarch 3/4  Verifying : centos-release-ansible26-1-3.el7.centos.noarch 4/4  Installed:  centos-release-openshift-origin311.noarch 0:1-2.el7.centos  Dependency Installed:  centos-release-ansible26.noarch 0:1-3.el7.centos centos-release-configmanagement.noarch 0:1-1.el7.centos centos-release-paas-common.noarch 0:1-1.el7.centos  Complete! [root@master ~]# 위와 같이 설치가 완료가 되면 추가로 Openshift Origin v3.11 설치를 위한 Repogitory 가 Enable 됩니다.\n[root@master ~]# yum repolist Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile  * base: ftp.nara.wide.ad.jp  * centos-ansible26: data.aonenetworks.kr  * extras: data.aonenetworks.kr  * updates: ftp.nara.wide.ad.jp centos-ansible26 | 2.9 kB 00:00:00 centos-openshift-origin311 | 2.9 kB 00:00:00 (1/2): centos-ansible26/7/x86_64/primary_db | 6.5 kB 00:00:00 (2/2): centos-openshift-origin311/primary_db | 19 kB 00:00:00 repo id repo name status base/7/x86_64 CentOS-7 - Base 10,019 centos-ansible26/7/x86_64 CentOS-7 - Ansible26 8 centos-openshift-origin311 CentOS OpenShift Origin 31 extras/7/x86_64 CentOS-7 - Extras 409 updates/7/x86_64 CentOS-7 - Updates 1,982 repolist: 12,449 [root@master ~]# Openshift Origin v3.11 은 ansible Version 에 종속성이 있어서 추가로 centos-ansible26 Repository 가 Enable 됩니다.\n추가로 Openshift ansible Playbook 및 ansible 을 설치합니다.\n[root@master ~]# yum install openshift-ansible ansible 설치가 완료되면 아래와 같이 /usr/share/ansible/openshift-ansible/ 경로에 Playbook 이 생성됩니다.\n[root@master ~]# cd /usr/share/ansible/openshift-ansible/ [root@master openshift-ansible]# ls -la 합계 12 drwxr-xr-x. 5 root root 72 6월 5 07:14 . drwxr-xr-x. 3 root root 31 6월 5 07:14 .. -rw-r--r--. 1 root root 1303 11월 2 2018 ansible.cfg drwxr-xr-x. 3 root root 34 6월 5 07:15 inventory drwxr-xr-x. 38 root root 4096 6월 5 07:14 playbooks drwxr-xr-x. 93 root root 4096 6월 5 07:14 roles [root@master openshift-ansible]# All-in-one Inventory 작성  노드 하나에 master, infra, compute 의 기능을 전부 설치하는 All-in-one Inventory 를 작성합니다.\n[root@master openshift-ansible]# cat inventory/hosts.localhost #bare minimum hostfile  [OSEv3:children] masters nodes etcd  [OSEv3:vars] # if your target hosts are Fedora uncomment this #ansible_python_interpreter=/usr/bin/python3 openshift_deployment_type=origin openshift_portal_net=172.30.0.0/16 # localhost likely doesn\u0026#39;t meet the minimum requirements openshift_disable_check=disk_availability,memory_availability  openshift_node_groups=[{\u0026#39;name\u0026#39;: \u0026#39;node-config-all-in-one\u0026#39;, \u0026#39;labels\u0026#39;: [\u0026#39;node-role.kubernetes.io/master=true\u0026#39;, \u0026#39;node-role.kubernetes.io/infra=true\u0026#39;, \u0026#39;node-role.kubernetes.io/compute=true\u0026#39;]}] openshift_release=\u0026#34;3.11\u0026#34; openshift_master_default_subdomain=apps.example.com  # uncomment the following to enable htpasswd authentication; defaults to AllowAllPasswordIdentityProvider openshift_master_identity_providers=[{\u0026#39;name\u0026#39;: \u0026#39;htpasswd_auth\u0026#39;, \u0026#39;login\u0026#39;: \u0026#39;true\u0026#39;, \u0026#39;challenge\u0026#39;: \u0026#39;true\u0026#39;, \u0026#39;kind\u0026#39;: \u0026#39;HTPasswdPasswordIdentityProvider\u0026#39;}]   [masters] localhost ansible_connection=local  [etcd] localhost ansible_connection=local  [nodes] # openshift_node_group_name should refer to a dictionary with matching key of name in list openshift_node_groups. localhost ansible_connection=local openshift_node_group_name=\u0026#34;node-config-all-in-one\u0026#34; openshift_master_identity_providers 항목을 추가하여 htpasswd 기능을 활성화 하였습니다.\nprerequisites.yml 배포  Openshift Cluster 배포하기 전에 prerequisites.yml 를 실행하여 Cluster 구성간 필요한 설정 작업 및 추가 패키지 설치를 합니다.\n# ansible-playbook -i inventory/hosts.localhost playbooks/prerequisites.yml ...  PLAY RECAP **************************************************************************************************************************************************************************************************************************************************************************************** localhost : ok=83 changed=23 unreachable=0 failed=0   INSTALLER STATUS ********************************************************************************************************************************************************************************************************************************************************************************** Initialization : Complete (0:00:35) Tuesday 04 June 2019 10:38:27 +0900 (0:00:00.032) 0:01:35.916 *********** =============================================================================== Ensure openshift-ansible installer package deps are installed ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 19.40s container_runtime : Install Docker -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 10.40s os_firewall : need to pause here, otherwise the iptables service starting can sometimes cause ssh to fail --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 10.13s os_firewall : Wait 10 seconds after disabling firewalld ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 10.10s Gathering Facts ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 4.49s openshift_excluder : Install docker excluder - yum ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 4.01s os_firewall : Install iptables packages ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 3.08s container_runtime : restart container runtime ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 2.88s container_runtime : Fixup SELinux permissions for docker ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1.54s openshift_repos : refresh cache ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 0.76s openshift_repos : Ensure libselinux-python is installed ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 0.74s Gather Cluster facts ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0.66s os_firewall : Ensure firewalld service is not enabled -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0.60s openshift_repos : Configure origin gpg keys ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 0.60s os_firewall : Start and enable iptables service -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0.55s container_runtime : Get current installed Docker version ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0.52s container_runtime : Place additional/blocked/insecure registries in /etc/containers/registries.conf ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0.50s openshift_repos : Configure correct origin release repository ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 0.46s container_runtime : Configure Docker service unit file ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0.45s Detecting Operating System from ostree_booted ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0.44s [root@master openshift-ansible]# deploy_cluster.yml 배포  prerequisites.yml 가 위와 같이 정상적으로 배포가 되면 Cluster 구성 준비가 완료 된 것입니다.\ndeploy_cluster.yml 을 이용하여 Cluster 배포를 시작합니다.\n# ansible-playbook -i inventory/hosts.localhost playbooks/deploy_cluster.yml ...  PLAY RECAP **************************************************************************************************************************************************************************************************************************************************************************************** localhost : ok=566 changed=138 unreachable=0 failed=0   INSTALLER STATUS ********************************************************************************************************************************************************************************************************************************************************************************** Initialization : Complete (0:00:23) Health Check : Complete (0:00:38) Node Bootstrap Preparation : Complete (0:01:14) etcd Install : Complete (0:00:32) Master Install : Complete (0:04:37) Master Additional Install : Complete (0:00:36) Node Join : Complete (0:00:18) Hosted Install : Complete (0:00:40) Cluster Monitoring Operator : Complete (0:00:11) Web Console Install : Complete (0:00:40) Console Install : Complete (0:00:18) metrics-server Install : Complete (0:00:01) Service Catalog Install : Complete (0:03:39) Tuesday 04 June 2019 11:23:04 +0900 (0:00:00.096) 0:14:35.316 ********** =============================================================================== openshift_control_plane : Wait for all control plane pods to become ready ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 97.17s /root/openshift-ansible/roles/openshift_control_plane/tasks/main.yml:272 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- template_service_broker : Verify that TSB is running -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 53.92s /root/openshift-ansible/roles/template_service_broker/tasks/deploy.yml:52 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ openshift_service_catalog : Wait for Controller Manager rollout success ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 43.92s /root/openshift-ansible/roles/openshift_service_catalog/tasks/start.yml:14 ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- openshift_control_plane : Wait for control plane pods to appear --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 42.90s /root/openshift-ansible/roles/openshift_control_plane/tasks/main.yml:220 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Run health checks (install) - EL ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 37.39s /root/openshift-ansible/playbooks/openshift-checks/private/install.yml:24 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ openshift_service_catalog : Wait for API Server rollout success --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 32.19s /root/openshift-ansible/roles/openshift_service_catalog/tasks/start.yml:2 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ openshift_web_console : Pause for the web console deployment to start --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 30.16s /root/openshift-ansible/roles/openshift_web_console/tasks/install.yml:158 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ openshift_service_catalog : oc_process ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 16.35s /root/openshift-ansible/roles/openshift_service_catalog/tasks/install.yml:44 --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- openshift_console : Waiting for console rollout to complete -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 9.06s /root/openshift-ansible/roles/openshift_console/tasks/start.yml:2 -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- openshift_manageiq : Configure role/user permissions --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 5.73s /root/openshift-ansible/roles/openshift_manageiq/tasks/main.yaml:45 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ tuned : Restart tuned service -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 5.41s /root/openshift-ansible/roles/tuned/tasks/main.yml:38 -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- openshift_control_plane : Wait for APIs to become available -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 5.29s /root/openshift-ansible/roles/openshift_control_plane/tasks/check_master_api_is_ready.yml:2 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ openshift_node : Install node, clients, and conntrack packages ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 4.55s /root/openshift-ansible/roles/openshift_node/tasks/install.yml:2 --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- tuned : Restart tuned service -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 4.05s /root/openshift-ansible/roles/tuned/tasks/main.yml:38 -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Gathering Facts ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 3.86s /root/openshift-ansible/playbooks/init/basic_facts.yml:2 ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- openshift_excluder : Install docker excluder - yum ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 3.62s /root/openshift-ansible/roles/openshift_excluder/tasks/install.yml:9 ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- openshift_cli : Install clients ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 3.57s /root/openshift-ansible/roles/openshift_cli/tasks/main.yml:2 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ansible_service_broker : Create custom resource definitions for asb ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 3.29s /root/openshift-ansible/roles/ansible_service_broker/tasks/install.yml:128 ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- openshift_hosted : Create OpenShift router ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 3.22s /root/openshift-ansible/roles/openshift_hosted/tasks/router.yml:85 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- openshift_control_plane : Start and enable self-hosting node ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 3.04s /root/openshift-ansible/roles/openshift_control_plane/tasks/main.yml:201 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- [root@master openshift-ansible]# 배포가 완료되면 아래와 같이 # oc get all 명령을 통해 Openshift 가 정상적으로 설치가 된 것을 확인 할 수 있습니다.\n[root@master ~]# oc get all NAME READY STATUS RESTARTS AGE pod/docker-registry-1-9t2zp 1/1 Running 0 10m pod/registry-console-1-n4v9t 1/1 Running 0 10m pod/router-1-58cpz 1/1 Running 3 10m  NAME DESIRED CURRENT READY AGE replicationcontroller/docker-registry-1 1 1 1 10m replicationcontroller/registry-console-1 1 1 1 10m replicationcontroller/router-1 1 1 1 10m  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/docker-registry ClusterIP 172.30.205.4 \u0026lt;none\u0026gt; 5000/TCP 10m service/kubernetes ClusterIP 172.30.0.1 \u0026lt;none\u0026gt; 443/TCP,53/UDP,53/TCP 10m service/registry-console ClusterIP 172.30.204.108 \u0026lt;none\u0026gt; 9000/TCP 10m service/router ClusterIP 172.30.237.203 \u0026lt;none\u0026gt; 80/TCP,443/TCP,1936/TCP 10m  NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/docker-registry 1 1 1 config deploymentconfig.apps.openshift.io/registry-console 1 1 1 config deploymentconfig.apps.openshift.io/router 1 1 1 config  NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/docker-registry docker-registry-default.apps.example.com docker-registry \u0026lt;all\u0026gt; passthrough None route.route.openshift.io/registry-console registry-console-default.apps.example.com registry-console \u0026lt;all\u0026gt; passthrough None [root@master ~]# Htpasswd 설정 \u0026amp;\u0026amp; USER 권한 설정  htpasswd 설정을 동해 Openshift Cluster를 관리할 admin 계정을 생성합니다.\n# cat /etc/origin/master/master-config.yaml | grep htpasswd  name: htpasswd_auth  file: /etc/origin/master/htpasswd Master 설정을 확인하여 htpasswd 가 저장되는 경로를 확인합니다.\n[root@master master]# htpasswd /etc/origin/master/htpasswd admin New password: Re-type new password: Updating password for user admin  [root@master master]# cat /etc/origin/master/htpasswd admin:$apr1$vfGx9xr2$G9/SgRxXUupu/mUVSuwmR/ [root@master master]# htpasswd 명령을 이용하여 신규 사용자 admin 을 생성합니다.\n[root@master master]# oc login -u admin Authentication required for https://master.example.com:8443 (openshift) Username: admin Password: Login successful.  You have access to the following projects and can switch between them with \u0026#39;oc project \u0026lt;projectname\u0026gt;\u0026#39;: ...  * sample-project ... Using project \u0026#34;\u0026#34;. [root@master master]# oc new-project sample-project Now using project \u0026#34;sample-project\u0026#34; on server \u0026#34;https://master.example.com:8443\u0026#34;.  You can add applications to this project with the \u0026#39;new-app\u0026#39; command. For example, try:   oc new-app centos/ruby-25-centos7~https://github.com/sclorg/ruby-ex.git  to build a new example application in Ruby. [root@master master]# 신규로 생성된 admin 으로 login을 하고 사용할 Project 를 생성합니다.\n[root@master master]# oc login -u system:admin Logged into \u0026#34;https://master.example.com:8443\u0026#34; as \u0026#34;system:admin\u0026#34; using existing credentials.  You have access to the following projects and can switch between them with \u0026#39;oc project \u0026lt;projectname\u0026gt;\u0026#39;: ...  * sample-project ... Using project \u0026#34;sample-project\u0026#34;.  [root@master master]# oc adm policy add-scc-to-user anyuid -z default scc \u0026#34;anyuid\u0026#34; added to: [\u0026#34;system:serviceaccount:sample-project:default\u0026#34;]  [root@master master]# oc login -u admin Logged into \u0026#34;https://master.example.com:8443\u0026#34; as \u0026#34;admin\u0026#34; using existing credentials.  You have access to the following projects and can switch between them with \u0026#39;oc project \u0026lt;projectname\u0026gt;\u0026#39;: ...  * sample-project ... Using project \u0026#34;sample-project\u0026#34;. [root@master master]# default serviceaccount 에 anyuid scc 를 추가하여 pod 를 제어 할 수 있는 권한을 부여합니다.\nSample APP 배포  Sample APP 를 배포하여 실제로 Openshift 를 이용하여 서비스를 시작해보겠습니다.\nS2I 기능을 이용한 APP 배포  Openshift 의 S2I 기능을 이용하여 Sample APP 를 배포하겠습니다.\n APP : wildfly:latest\nSource : https://github.com/chhanz/openshift-deploy.git\n [root@master ~]# oc new-app wildfly~https://github.com/chhanz/openshift-deploy --name sampleapp --\u0026gt; Found image 05e5cf6 (2 weeks old) in image stream \u0026#34;openshift/wildfly\u0026#34; under tag \u0026#34;13.0\u0026#34; for \u0026#34;wildfly\u0026#34;   WildFly 13.0.0.Final  --------------------  Platform for building and running JEE applications on WildFly 13.0.0.Final   Tags: builder, wildfly, wildfly13   * A source build using source code from https://github.com/chhanz/openshift-deploy will be created  * The resulting image will be pushed to image stream tag \u0026#34;sampleapp:latest\u0026#34;  * Use \u0026#39;start-build\u0026#39; to trigger a new build  * This image will be deployed in deployment config \u0026#34;sampleapp\u0026#34;  * Port 8080/tcp will be load balanced by service \u0026#34;sampleapp\u0026#34;  * Other containers can access this service through the hostname \u0026#34;sampleapp\u0026#34;  --\u0026gt; Creating resources ...  imagestream.image.openshift.io \u0026#34;sampleapp\u0026#34; created  buildconfig.build.openshift.io \u0026#34;sampleapp\u0026#34; created  deploymentconfig.apps.openshift.io \u0026#34;sampleapp\u0026#34; created  service \u0026#34;sampleapp\u0026#34; created --\u0026gt; Success  Build scheduled, use \u0026#39;oc logs -f bc/sampleapp\u0026#39; to track its progress.  Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:  \u0026#39;oc expose svc/sampleapp\u0026#39;  Run \u0026#39;oc status\u0026#39; to view your app. 신규 APP 가 생성이 되고 sample.war 소스를 기반으로\n[root@master ~]# oc logs -f bc/sampleapp Cloning \u0026#34;https://github.com/chhanz/openshift-deploy\u0026#34; ... \tCommit:\t7c4656f9d88fe9ffbf846ba0dff5b74742f34c67 (Change Location) \tAuthor:\tchhanz \u0026lt;han0495@gmail.com\u0026gt; \tDate:\tFri Jun 7 22:00:34 2019 +0900 Using docker-registry.default.svc:5000/openshift/wildfly@sha256:895e0a6c732f8244ce75b376651155fdef12311df0d940ce111756ef43aa2bfc as the s2i builder image Moving binaries in source directory into /wildfly/standalone/deployments for later deployment... Moving all war artifacts from /opt/app-root/src/. directory into /wildfly/standalone/deployments for later deployment... \u0026#39;/opt/app-root/src/./sample.war\u0026#39; -\u0026gt; \u0026#39;/wildfly/standalone/deployments/sample.war\u0026#39; Moving all ear artifacts from /opt/app-root/src/. directory into /wildfly/standalone/deployments for later deployment... Moving all rar artifacts from /opt/app-root/src/. directory into /wildfly/standalone/deployments for later deployment... Moving all jar artifacts from /opt/app-root/src/. directory into /wildfly/standalone/deployments for later deployment... ...done  Pushing image docker-registry.default.svc:5000/sample-project/sampleapp:latest ... Pushed 2/13 layers, 15% complete Pushed 3/13 layers, 23% complete Pushed 4/13 layers, 31% complete Pushed 5/13 layers, 39% complete Pushed 6/13 layers, 46% complete Pushed 7/13 layers, 54% complete Pushed 8/13 layers, 62% complete Pushed 9/13 layers, 69% complete Pushed 10/13 layers, 77% complete Pushed 11/13 layers, 85% complete Pushed 12/13 layers, 92% complete Pushed 13/13 layers, 100% complete Push successful 위와 같이 git 에서 소스를 가져와서 이미지로 생성하고 Openshift registory 에 Push 합니다.\n[root@master ~]# oc get po NAME READY STATUS RESTARTS AGE sampleapp-1-build 0/1 Completed 0 4m sampleapp-1-rsklv 1/1 Running 0 3m [root@master ~]#  [root@master ~]# oc get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE sampleapp ClusterIP 172.30.37.118 \u0026lt;none\u0026gt; 8080/TCP 4m [root@master ~]# 생성된 service 를 이용하여 route 를 생성합니다.\n[root@master ~]# oc expose service/sampleapp route.route.openshift.io/sampleapp exposed [root@master ~]# oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD sampleapp sampleapp-sample-project.apps.example.com sampleapp 8080-tcp None [root@master ~]# sampleapp 에 대한 route 가 생성이 되었습니다.\n실제로 서비스가 되는지 확인하도록 하겠습니다.\nWeb Console  Openshift 는 Web Console 을 제공하여 좀더 편리하게 서비스를 배포하고 운영 할 수 있도록 하고 있습니다.\nadmin 계정으로 로그인을 하겠습니다.\n위와 같이 배포된 어플리케이션들에 대해 상태를 확인하고 Scale Up/Down 등의 기능을 수행이 가능합니다.\nOpenshift Origin v3.11에는 기본적으로 Prometheus + Grafana Dashboard 를 제공하고 있습니다.\n(openshift-monitoring Project 에서 실행중)\n이번 포스팅에서는 Openshift Origin 설치 및 APP 를 배포하여 서비스가 동작되는 것을 확인 하였습니다.\n이후 포스팅에서는 Web Console 를 통한 배포/관리, HPA 를 이용한 Auto-Scaling 구현을 작성해보도록 하겠습니다.\n감사합니다.\n참고자료   설치 관련 참고 자료  DNS 구성 관련 : https://www.unixmen.com/setting-dns-server-centos-7/ DNS 설정 관련 : https://docs.openshift.com/container-platform/3.11/install/prerequisites.html#wildcard-dns-prereq ansible-playbook inventory 관련 : https://docs.okd.io/3.11/install/configuring_inventory_file.html   scc 관련  https://blog.openshift.com/understanding-service-accounts-sccs/   git source  chhanz github : https://github.com/chhanz/openshift-deploy    ","permalink":"https://chhanz88.github.io/post/2019-06-07-install-okd-all-in-one/","summary":"안녕하세요 chhanz 입니다.\n이번 포스팅에서는 Kubernetes에 기반을 둔 Developer-Oriented PaaS 인 Openshift를 살펴볼 것입니다.\nOpenshift 란?  OpenShift 는 개발자 및 IT 운영팀을 단일 플랫폼에서 통합하여, 하이브리드 클라우드 및 멀티 클라우드 인프라 전반에서 애플리케이션을 일관되게 구축, 배포 및 관리하도록 지원하는 플랫폼입니다.\nOpenshift 이점  주요 이점  애플리케이션 라이프사이클 전반에 걸쳐 운영 및 개발팀에서 더 큰 가치 실현 애플리케이션 개발 주기 단축 및 소프트웨어 배포 빈도 증가 하이브리드 클라우드 및 멀티 클라우드 전반에서 IT 운영 비용 절감 및 애플리케이션 이식성 실현  개발팀을 위한 이점   Openshift는 개발자에게 셀프 서비스 방식으로 애플리케이션과 컴포넌트를 프로비저닝, 빌드 및 배포하도록 지원하는 최적의 플랫폼입니다.","title":"[Openshift] Openshift Origin v3.11 설치, App 배포"},{"content":"CentOS 7 PPC 설치 on PowerVM  설치 환경   IBM Power7 P750\nIBM PowerVM 2.4\nCentOS7(1804)\n VM 설정  VIOS 에서 VM LPAR 를 생성하고 vCD-ROM 을 이용하여 ISO 를 Mount 하여 OS 설치 준비를 합니다.\n$ loadopt -disk CentOS-7-ppc64-Everything-1804.iso -vtd vtopt0 lsrep $ lsrep Size(mb) Free(mb) Parent Pool Parent Size Parent Free  10198 2958 rootvg 279552 220416  Name File Size Optical Access CentOS-7-ppc64-Everything-1804.iso 7240 vtopt0 rw $ lsmap -vadapter vhost25 SVSA Physloc Client Partition ID --------------- -------------------------------------------- ------------------ vhost25 U8233.E8B.0637D5P-V1-C181 0x0000001a  VTD linuxos_1 Status Available LUN 0x8200000000000000 Backing device linuxos_lv Physloc Mirrored N/A  VTD vtopt0 Status Available LUN 0x8100000000000000 Backing device /var/vio/VMLibrary/CentOS-7-ppc64-Everything-1804.iso Physloc Mirrored N/A CD 부팅  LPAR를 키고 SMS 모드로 진입합니다. 이후 vCD-ROM 을 통해 ISO 를 부팅합니다.\nPowerPC Firmware  Version AL730_149  SMS 1.7 (c) Copyright IBM Corp. 2000,2008 All rights reserved. -------------------------------------------------------------------------------  Main Menu  1. Select Language  2. Setup Remote IPL (Initial Program Load)  3. Change SCSI Settings  4. Select Console  5. Select Boot Options           -------------------------------------------------------------------------------  Navigation Keys:   X = eXit System Management Services  -------------------------------------------------------------------------------  Type menu item number and press Enter or select Navigation key:5  # SMS 모드에서 Select Boot Options 선택   PowerPC Firmware  Version AL730_149  SMS 1.7 (c) Copyright IBM Corp. 2000,2008 All rights reserved. -------------------------------------------------------------------------------  Multiboot  1. Select Install/Boot Device  2. Configure Boot Device Order  3. Multiboot Startup \u0026lt;OFF\u0026gt;  4. SAN Zoning Support            -------------------------------------------------------------------------------  Navigation keys:  M = return to Main Menu  ESC key = return to previous screen X = eXit System Management Services  -------------------------------------------------------------------------------  Type menu item number and press Enter or select Navigation key:1  # SMS 모드에서 Select Install/Boot Device 선택    PowerPC Firmware  Version AL730_149  SMS 1.7 (c) Copyright IBM Corp. 2000,2008 All rights reserved. -------------------------------------------------------------------------------  Select Device Type  1. Diskette  2. Tape  3. CD/DVD  4. IDE  5. Hard Drive  6. Network  7. List all Devices         -------------------------------------------------------------------------------  Navigation keys:  M = return to Main Menu  ESC key = return to previous screen X = eXit System Management Services  -------------------------------------------------------------------------------  Type menu item number and press Enter or select Navigation key:3  # SMS 모드에서 CD/DVD 선택   PowerPC Firmware  Version AL730_149  SMS 1.7 (c) Copyright IBM Corp. 2000,2008 All rights reserved. -------------------------------------------------------------------------------  Select Media Type  1. SCSI  2. SSA  3. SAN  4. SAS  5. SATA  6. USB  7. IDE  8. ISA  9. List All Devices       -------------------------------------------------------------------------------  Navigation keys:  M = return to Main Menu  ESC key = return to previous screen X = eXit System Management Services  -------------------------------------------------------------------------------  Type menu item number and press Enter or select Navigation key:1  # SMS 모드에서 SCSI 선택   PowerPC Firmware  Version AL730_149  SMS 1.7 (c) Copyright IBM Corp. 2000,2008 All rights reserved. -------------------------------------------------------------------------------  Select Media Adapter  1. U8233.E8B.0637D5P-V26-C181-T1 /vdevice/v-scsi@300000b5  2. List all devices              -------------------------------------------------------------------------------  Navigation keys:  M = return to Main Menu  ESC key = return to previous screen X = eXit System Management Services  -------------------------------------------------------------------------------  Type menu item number and press Enter or select Navigation key:1  # SMS 모드에서 vCD-ROM가 연결된 v-SCSI Adapter 선택 선택   PowerPC Firmware  Version AL730_149  SMS 1.7 (c) Copyright IBM Corp. 2000,2008 All rights reserved. -------------------------------------------------------------------------------  Select Device  Device Current Device  Number Position Name  1. 2 SCSI CD-ROM  ( loc=U8233.E8B.0637D5P-V26-C181-T1-L8100000000000000 )            -------------------------------------------------------------------------------  Navigation keys:  M = return to Main Menu  ESC key = return to previous screen X = eXit System Management Services  -------------------------------------------------------------------------------  Type menu item number and press Enter or select Navigation key:1  # SMS 모드에서 SCSI CD-ROM 선택 선택   PowerPC Firmware  Version AL730_149  SMS 1.7 (c) Copyright IBM Corp. 2000,2008 All rights reserved. -------------------------------------------------------------------------------  Select Task  SCSI CD-ROM  ( loc=U8233.E8B.0637D5P-V26-C181-T1-L8100000000000000 )   1. Information  2. Normal Mode Boot  3. Service Mode Boot          -------------------------------------------------------------------------------  Navigation keys:  M = return to Main Menu  ESC key = return to previous screen X = eXit System Management Services  -------------------------------------------------------------------------------  Type menu item number and press Enter or select Navigation key:2  # 부팅 모드 설정   PowerPC Firmware  Version AL730_149  SMS 1.7 (c) Copyright IBM Corp. 2000,2008 All rights reserved. -------------------------------------------------------------------------------  Are you sure you want to exit System Management Services?  1. Yes  2. No              -------------------------------------------------------------------------------  Navigation Keys:   X = eXit System Management Services  -------------------------------------------------------------------------------  Type menu item number and press Enter or select Navigation key:1   # 재부팅 시작 CentOS 설치  선택한 ISO를 통해 x86에서 많이 보던 설치 화면을 볼 수 있었습니다.\n### 설치 화면   Install CentOS 7 (64-bit kernel)  Test this media \u0026amp; install CentOS 7 (64-bit kernel)  Rescue a CentOS system (64-bit kernel)  Other options...              Use the ^ and v keys to change the selection.  Press \u0026#39;e\u0026#39; to edit the selected item, or \u0026#39;c\u0026#39; for a command prompt.  The selected entry will be started automatically in 0s.  ### ### Text 설치 화면 ###  * installation log files are stored in /tmp during the installation  * shell is available on TTY2  * if the graphical installation interface fails to start, try again with the  inst.text bootoption to start text installation  * when reporting a bug add logs from /tmp as separate text/plain attachments 11:44:29 Not asking for VNC because we don`t have a network 11:44:30 X startup failed, falling back to text mode ================================================================================ ================================================================================ Installation   1) [x] Language settings 2) [!] Time settings  (English (United States)) (Timezone is not set.)  3) [!] Installation source 4) [!] Software selection  (Processing...) (Processing...)  5) [!] Installation Destination 6) [x] Kdump  (No disks selected) (Kdump is enabled)  7) [ ] Network configuration 8) [!] Root password  (Not connected) (Password is not set.)  9) [!] User creation  (No user will be created)  Please make your choice from above [\u0026#39;q\u0026#39; to quit | \u0026#39;b\u0026#39; to begin installation |  \u0026#39;r\u0026#39; to refresh]: [anaconda] 1:main* 2:shell 3:log 4:storage-lo\u0026gt; Switch tab: Alt+Tab | Help: F1  ### 설치 시작 Progress Setting up the installation environment . Creating disklabel on /dev/sda . Creating xfs on /dev/sda2 . Creating lvmpv on /dev/sda3 . Creating swap on /dev/mapper/centos-swap . Creating xfs on /dev/mapper/centos-root . Creating prepboot on /dev/sda1 . Running pre-installation scripts . Starting package installation process  \u0026lt; 중략 \u0026gt;  Installing iwl5150-firmware (298/298) Performing post-installation setup tasks Installing boot loader . Performing post-installation setup tasks .  Configuring installed system . Writing network configuration . Creating users . Configuring addons . Generating initramfs . Running post-installation scripts .  Use of this product is subject to the license agreement found at /usr/share/centos-release/EULA   Installation complete. Press return to quit  ### 설치 완료  ### 부팅 완료 CentOS Linux 7 (AltArch) Kernel 3.10.0-862.el7.ppc64 on an ppc64  localhost login: 위와 같이 TEXT 모드로 설치가 완료되었습니다.\n설치 이후 기능 테스트  OS Information\n### OS Information [root@powerlinux ~]# cat /etc/redhat-release CentOS Linux release 7.5.1804 (AltArch)  [root@powerlinux ~]# uname -a Linux powerlinux.centos.com 3.10.0-862.el7.ppc64 #1 SMP Tue Apr 10 15:05:38 GMT 2018 ppc64 ppc64 ppc64 GNU/Linux [root@powerlinux ~]# YUM TEST\n## YUM TEST  [root@powerlinux ~]# yum repolist Loaded plugins: fastestmirror Determining fastest mirrors base | 3.6 kB 00:00:00 extras | 2.9 kB 00:00:00 updates | 2.9 kB 00:00:00 (1/4): extras/7/ppc64/primary_db | 31 kB 00:00:00 (2/4): base/7/ppc64/group_gz | 166 kB 00:00:00 (3/4): base/7/ppc64/primary_db | 5.2 MB 00:00:06 (4/4): updates/7/ppc64/primary_db | 5.3 MB 00:00:10 repo id repo name status base/7/ppc64 CentOS-7 - Base 8,788 extras/7/ppc64 CentOS-7 - Extras 63 updates/7/ppc64 CentOS-7 - Updates 1,391 repolist: 10,242 [root@powerlinux ~]# H/W Information\n### H/W Information  [root@powerlinux ~]# cat /proc/cpuinfo processor : 0 cpu : POWER7 (architected), altivec supported clock : 3300.000000MHz revision : 2.1 (pvr 003f 0201)  processor : 1 cpu : POWER7 (architected), altivec supported clock : 3300.000000MHz revision : 2.1 (pvr 003f 0201)  processor : 2 cpu : POWER7 (architected), altivec supported clock : 3300.000000MHz revision : 2.1 (pvr 003f 0201)  processor : 3 cpu : POWER7 (architected), altivec supported clock : 3300.000000MHz revision : 2.1 (pvr 003f 0201)  processor : 4 cpu : POWER7 (architected), altivec supported clock : 3300.000000MHz revision : 2.1 (pvr 003f 0201)  processor : 5 cpu : POWER7 (architected), altivec supported clock : 3300.000000MHz revision : 2.1 (pvr 003f 0201)  processor : 6 cpu : POWER7 (architected), altivec supported clock : 3300.000000MHz revision : 2.1 (pvr 003f 0201)  processor : 7 cpu : POWER7 (architected), altivec supported clock : 3300.000000MHz revision : 2.1 (pvr 003f 0201)  timebase : 512000000 platform : pSeries model : IBM,8233-E8B machine : CHRP IBM,8233-E8B  ### Memroy [root@powerlinux ~]# cat /proc/meminfo MemTotal: 7824576 kB MemFree: 6970496 kB MemAvailable: 7008000 kB  ## lshw Information [root@powerlinux ~]# lshw powerlinux.centos.com  description: pSeries LPAR  product: Power 750 Express  vendor: IBM  serial: IBM,020637D5P  width: 64 bits  capabilities: smp  configuration: chassis=rackmount  *-core  description: Motherboard  physical id: 0  *-firmware  product: IBM,AL730_149  physical id: 1  logical name: /proc/device-tree  *-cpu:0  description: POWER7 (architected), altivec supported  product: PowerPC,POWER7  physical id: 0  bus info: cpu@0  version: 2.1 (pvr 003f 0201)  size: 3300MHz  capabilities: performance-monitor  configuration: threads=4  *-cache:0  description: L1 Cache (instruction)  physical id: 0  size: 32KiB  *-cache:1  description: L1 Cache (data)  physical id: 1  size: 32KiB  *-cpu:1  description: POWER7 (architected), altivec supported  product: PowerPC,POWER7  physical id: 4  bus info: cpu@1  version: 2.1 (pvr 003f 0201)  size: 3300MHz  capabilities: performance-monitor  configuration: threads=4  *-cache:0  description: L1 Cache (instruction)  physical id: 0  size: 32KiB  *-cache:1  description: L1 Cache (data)  physical id: 1  size: 32KiB  *-memory  description: System memory  physical id: 2  size: 8GiB  *-vty  description: Virtual I/O device (vty)  physical id: 1  bus info: vio@30000000  logical name: /proc/device-tree/vdevice/vty@30000000  configuration: driver=hvc_console  *-l-lan  description: Ethernet interface  physical id: 2  bus info: vio@3000005b  logical name: /proc/device-tree/vdevice/l-lan@3000005b  serial: 92:e7:e9:5e:74:5b  size: 1Gbit/s  capacity: 1Gbit/s  capabilities: ethernet physical fibre 1000bt-fd autonegotiation  configuration: autonegotiation=on broadcast=yes driver=ibmveth driverversion=1.06 duplex=full ip=192.168.13.121 link=yes multicast=yes port=fibre speed=1Gbit/s  *-v-scsi  description: Virtual I/O device (v-scsi)  physical id: 3  bus info: vio@300000b5  logical name: /proc/device-tree/vdevice/v-scsi@300000b5  logical name: scsi0  configuration: driver=ibmvscsi  *-cdrom  description: SCSI CD-ROM  product: VOPTA  vendor: AIX  physical id: 0.1.0  bus info: scsi@0:0.1.0  logical name: /dev/cdrom  logical name: /dev/sr0  capabilities: removable audio  configuration: ansiversion=4 status=ready  *-medium  physical id: 0  logical name: /dev/cdrom  capabilities: partitioned partitioned:mac  *-volume:0 UNCLAIMED  description: Apple partition map  physical id: 1  capacity: 1KiB  *-volume:1 UNCLAIMED  description: Apple HFS  physical id: 2  size: 7292MiB  capabilities: ro hfs initialized  configuration: created=2018-05-07 15:00:59 filesystem=hfs label=7 modified=2018-05-07 15:01:01 state=clean  *-disk  description: SCSI Disk  product: VDASD  vendor: AIX  physical id: 0.2.0  bus info: scsi@0:0.2.0  logical name: /dev/sda  version: 0001  serial: 00f637d500004c0000000166d1f4daed.15  size: 50GiB (53GB)  capabilities: partitioned partitioned:dos  configuration: ansiversion=3 logicalsectorsize=512 sectorsize=512 signature=c4b50700  *-volume:0  description: PPC PReP Boot partition  physical id: 1  bus info: scsi@0:0.2.0,1  logical name: /dev/sda1  capacity: 4MiB  capabilities: primary bootable boot  *-volume:1  description: Linux filesystem partition  physical id: 2  bus info: scsi@0:0.2.0,2  logical name: /dev/sda2  logical name: /boot  capacity: 1GiB  capabilities: primary  configuration: mount.fstype=xfs mount.options=rw,relatime,attr2,inode64,noquota state=mounted  *-volume:2  description: Linux LVM Physical Volume partition  physical id: 3  bus info: scsi@0:0.2.0,3  logical name: /dev/sda3  serial: MjzP9l-erSw-jIrI-8idl-6eUT-y3TI-Zpm0fc  size: 48GiB  capacity: 48GiB  capabilities: primary multi lvm2  *-ibm_sp  description: Virtual I/O device (IBM,sp)  physical id: 4  bus info: vio@4000  logical name: /proc/device-tree/vdevice/IBM,sp@4000  *-rtc  description: Virtual I/O device (rtc)  physical id: 5  bus info: vio@4001  logical name: /proc/device-tree/vdevice/rtc@4001  *-nvram  description: Virtual I/O device (nvram)  physical id: 6  bus info: vio@4002  logical name: /proc/device-tree/vdevice/nvram@4002  *-gscsi  description: Virtual I/O device (gscsi)  physical id: 7  bus info: vio@4004  logical name: /proc/device-tree/vdevice/gscsi@4004 [root@powerlinux ~]# 참고 자료   https://developer.ibm.com/linuxonpower/2018/08/17/announcing-centos-linux-7-power9/  ","permalink":"https://chhanz88.github.io/post/2019-05-10-install-centos7-ppc-on-vios/","summary":"CentOS 7 PPC 설치 on PowerVM  설치 환경   IBM Power7 P750\nIBM PowerVM 2.4\nCentOS7(1804)\n VM 설정  VIOS 에서 VM LPAR 를 생성하고 vCD-ROM 을 이용하여 ISO 를 Mount 하여 OS 설치 준비를 합니다.\n$ loadopt -disk CentOS-7-ppc64-Everything-1804.iso -vtd vtopt0 lsrep $ lsrep Size(mb) Free(mb) Parent Pool Parent Size Parent Free  10198 2958 rootvg 279552 220416  Name File Size Optical Access CentOS-7-ppc64-Everything-1804.iso 7240 vtopt0 rw $ lsmap -vadapter vhost25 SVSA Physloc Client Partition ID --------------- -------------------------------------------- ------------------ vhost25 U8233.","title":"[Linux] CentOS7 PPC 설치 on PowerVM"},{"content":"GRUBBY COMMAND 활용   grubby 라는 명령을 통해 GRUB2 부트로더를 손쉽게 수정 할 수 있습니다.\ngrubby \u0026ndash;help  [root@fastvm-centos-7-6-21 ~]# grubby --help Usage: grubby [OPTION...]  --add-kernel=kernel-path add an entry for the specified kernel  --add-multiboot=STRING add an entry for the specified multiboot  kernel  --args=args default arguments for the new kernel or  new arguments for kernel being updated  --mbargs=STRING default arguments for the new multiboot  kernel or new arguments for multiboot  kernel being updated  --bad-image-okay don`t sanity check images in boot  entries (for testing only)  --boot-filesystem=bootfs filesystem which contains /boot  directory (for testing only)  --bootloader-probe check which bootloader is installed on  boot sector  -c, --config-file=path path to grub config file to update (\u0026#34;-\u0026#34;  for stdin)  --copy-default use the default boot entry as a template  for the new entry being added; if the  default is not a linux image, or if the  kernel referenced by the default image  does not exist, the first linux entry  whose kernel does exist is used as the  template  --debug print debugging information for failures  --default-kernel display the path of the default kernel  --default-index display the index of the default kernel  --default-title display the title of the default kernel  --elilo configure elilo bootloader  --efi force grub2 stanzas to use efi  --env=path path for environment data  --extlinux configure extlinux bootloader (from  syslinux)  --grub configure grub bootloader  --grub2 configure grub2 bootloader  --info=kernel-path display boot information for specified  kernel  --initrd=initrd-path initrd image for the new kernel  -i, --extra-initrd=initrd-path auxiliary initrd image for things other  than the new kernel  --lilo configure lilo bootloader  --make-default make the newly added entry the default  boot entry  -o, --output-file=path path to output updated config file (\u0026#34;-\u0026#34;  for stdout)  --remove-args=STRING remove kernel arguments  --remove-mbargs=STRING remove multiboot kernel arguments  --remove-kernel=kernel-path remove all entries for the specified  kernel  --remove-multiboot=STRING remove all entries for the specified  multiboot kernel  --set-default=kernel-path make the first entry referencing the  specified kernel the default  --set-default-index=entry-index make the given entry index the default  entry  --set-index=entry-index use the given index when creating a new  entry  --silo configure silo bootloader  --title=entry-title title to use for the new kernel entry  --update-kernel=kernel-path updated information for the specified  kernel  -v, --version print the version of this program and  exit  --yaboot configure yaboot bootloader  --zipl configure zipl bootloader  Help options:  -?, --help Show this help message  --usage Display brief usage message grubby 를 이용해서 현재 부트로더 설정 확인  # 파라미터 확인  [root@fastvm-centos-7-6-21 ~]# grubby --info=ALL index=0 kernel=/boot/vmlinuz-3.10.0-957.el7.x86_64 args=\u0026#34;ro crashkernel=128M rd.lvm.lv=c7vg/root_lv rd.lvm.lv=c7vg/swap_lv console=ttyS0,115200n8 LANG=en_US.UTF-8\u0026#34; root=/dev/mapper/c7vg-root_lv initrd=/boot/initramfs-3.10.0-957.el7.x86_64.img title=CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core) index=1 kernel=/boot/vmlinuz-0-rescue-59c8a0b7323f456ab9d1194e09abca71 args=\u0026#34;ro crashkernel=128M rd.lvm.lv=c7vg/root_lv rd.lvm.lv=c7vg/swap_lv console=ttyS0,115200n8\u0026#34; root=/dev/mapper/c7vg-root_lv initrd=/boot/initramfs-0-rescue-59c8a0b7323f456ab9d1194e09abca71.img title=CentOS Linux (0-rescue-59c8a0b7323f456ab9d1194e09abca71) 7 (Core) index=2 non linux entry grubby 를 이용해서 부트로더에 파라미터 추가  # 파라미터 추가  [root@fastvm-centos-7-6-21 ~]# grubby --update-kernel=ALL --args=\u0026#34;spectre_v2=off nopti\u0026#34;  # 추가된 내용 확인  [root@fastvm-centos-7-6-21 ~]# grubby --info=ALL index=0 kernel=/boot/vmlinuz-3.10.0-957.el7.x86_64 args=\u0026#34;ro crashkernel=128M rd.lvm.lv=c7vg/root_lv rd.lvm.lv=c7vg/swap_lv console=ttyS0,115200n8 LANG=en_US.UTF-8 spectre_v2=off nopti\u0026#34; root=/dev/mapper/c7vg-root_lv initrd=/boot/initramfs-3.10.0-957.el7.x86_64.img title=CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core) index=1 kernel=/boot/vmlinuz-0-rescue-59c8a0b7323f456ab9d1194e09abca71 args=\u0026#34;ro crashkernel=128M rd.lvm.lv=c7vg/root_lv rd.lvm.lv=c7vg/swap_lv console=ttyS0,115200n8 spectre_v2=off nopti\u0026#34; root=/dev/mapper/c7vg-root_lv initrd=/boot/initramfs-0-rescue-59c8a0b7323f456ab9d1194e09abca71.img title=CentOS Linux (0-rescue-59c8a0b7323f456ab9d1194e09abca71) 7 (Core) index=2 non linux entry [root@fastvm-centos-7-6-21 ~]# grubby 를 이용해서 부트로더의 파라미터 삭제  # 파라미터 제거  [root@fastvm-centos-7-6-21 ~]# grubby --update-kernel=ALL --remove-args=\u0026#34;spectre_v2=off nopti\u0026#34;  # 제거된 내용 확인  [root@fastvm-centos-7-6-21 ~]# grubby --info=ALL index=0 kernel=/boot/vmlinuz-3.10.0-957.el7.x86_64 args=\u0026#34;ro crashkernel=128M rd.lvm.lv=c7vg/root_lv rd.lvm.lv=c7vg/swap_lv console=ttyS0,115200n8 LANG=en_US.UTF-8\u0026#34; root=/dev/mapper/c7vg-root_lv initrd=/boot/initramfs-3.10.0-957.el7.x86_64.img title=CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core) index=1 kernel=/boot/vmlinuz-0-rescue-59c8a0b7323f456ab9d1194e09abca71 args=\u0026#34;ro crashkernel=128M rd.lvm.lv=c7vg/root_lv rd.lvm.lv=c7vg/swap_lv console=ttyS0,115200n8\u0026#34; root=/dev/mapper/c7vg-root_lv initrd=/boot/initramfs-0-rescue-59c8a0b7323f456ab9d1194e09abca71.img title=CentOS Linux (0-rescue-59c8a0b7323f456ab9d1194e09abca71) 7 (Core) index=2 non linux entry [root@fastvm-centos-7-6-21 ~]# 상기 --update-kernel= 에 특정 커널을 지정하여 특정 커널에만 부트로더 파라미터를 적용 할 수 있다.\ncat /proc/cmdline  grubby 외에도 현재 부팅된 부트로더 파라미터를 확인 하는 방법이 있습니다.\n확인 방법은 아래와 같습니다.\n[root@fastvm-centos-7-6-21 ~]# cat /proc/cmdline BOOT_IMAGE=/vmlinuz-3.10.0-957.el7.x86_64 root=/dev/mapper/c7vg-root_lv ro crashkernel=128M rd.lvm.lv=c7vg/root_lv rd.lvm.lv=c7vg/swap_lv console=ttyS0,115200n8 LANG=en_US.UTF-8 spectre_v2=off nopti 참고 자료   https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/s2-proc-cmdline  ","permalink":"https://chhanz88.github.io/post/2019-04-17-grubby_and_proc_cmdline/","summary":"GRUBBY COMMAND 활용   grubby 라는 명령을 통해 GRUB2 부트로더를 손쉽게 수정 할 수 있습니다.\ngrubby \u0026ndash;help  [root@fastvm-centos-7-6-21 ~]# grubby --help Usage: grubby [OPTION...]  --add-kernel=kernel-path add an entry for the specified kernel  --add-multiboot=STRING add an entry for the specified multiboot  kernel  --args=args default arguments for the new kernel or  new arguments for kernel being updated  --mbargs=STRING default arguments for the new multiboot  kernel or new arguments for multiboot  kernel being updated  --bad-image-okay don`t sanity check images in boot  entries (for testing only)  --boot-filesystem=bootfs filesystem which contains /boot  directory (for testing only)  --bootloader-probe check which bootloader is installed on  boot sector  -c, --config-file=path path to grub config file to update (\u0026#34;-\u0026#34;  for stdin)  --copy-default use the default boot entry as a template  for the new entry being added; if the  default is not a linux image, or if the  kernel referenced by the default image  does not exist, the first linux entry  whose kernel does exist is used as the  template  --debug print debugging information for failures  --default-kernel display the path of the default kernel  --default-index display the index of the default kernel  --default-title display the title of the default kernel  --elilo configure elilo bootloader  --efi force grub2 stanzas to use efi  --env=path path for environment data  --extlinux configure extlinux bootloader (from  syslinux)  --grub configure grub bootloader  --grub2 configure grub2 bootloader  --info=kernel-path display boot information for specified  kernel  --initrd=initrd-path initrd image for the new kernel  -i, --extra-initrd=initrd-path auxiliary initrd image for things other  than the new kernel  --lilo configure lilo bootloader  --make-default make the newly added entry the default  boot entry  -o, --output-file=path path to output updated config file (\u0026#34;-\u0026#34;  for stdout)  --remove-args=STRING remove kernel arguments  --remove-mbargs=STRING remove multiboot kernel arguments  --remove-kernel=kernel-path remove all entries for the specified  kernel  --remove-multiboot=STRING remove all entries for the specified  multiboot kernel  --set-default=kernel-path make the first entry referencing the  specified kernel the default  --set-default-index=entry-index make the given entry index the default  entry  --set-index=entry-index use the given index when creating a new  entry  --silo configure silo bootloader  --title=entry-title title to use for the new kernel entry  --update-kernel=kernel-path updated information for the specified  kernel  -v, --version print the version of this program and  exit  --yaboot configure yaboot bootloader  --zipl configure zipl bootloader  Help options:  -?","title":"[Linux] grubby 사용법"},{"content":" 이번 포스팅은 Kubernetes Korea Group의 Kubernetes Architecture Study 모임에서 스터디 후, 발표된 내용입니다.\nLink : k8skr-study-architecture Github\n Kubernetes Volume #2  저번 포스팅 Kubernetes Volume #1 에서는 Local Volume 에 관련된 emptyDir / hostPath / gitRepo 에 대해 설명드렸습니다.\n이어서 이번 포스팅에서는 Network Volume 으로 사용될 nfs / cephfs / ceph rbd 를 예제와 함께 알아보도록 하겠습니다.\nPersistent Volume 와 Persistent Volume Claim  Persistent Volume 와 Persistent VolumeClaim 가 있는데,\n Persistent Volume(이하 PV) 는 Kubernetes 에서 관리되는 저장소로 Pod 과는 다른 수명 주기로 관리됩니다.\nPod 이 재실행 되더라도, PV의 데이터는 정책에 따라 유지/삭제가 됩니다.\n  Persistent Volume Claim(이하 PVC) 는 PV를 추상화하여 개발자가 손쉽게 PV를 사용 가능하게 만들어주는 기능입니다.\n개발자는 사용에 필요한 Volume의 크기, Volume의 정책을 선택하고 요청만 하면 됩니다.\n운영자는 개발자의 요청에 맞게 PV 를 생성하게 되고, PVC는 해당 PV를 가져가게 됩니다.\n 이와 같은 방식을 Static Provisioning 이라 합니다.\n예제를 통해 Static Provisioning을 확인 해보겠습니다.\nStatic Provisioning  NFS NFS 서버를 PV로 사용하는 방식입니다.\n예제에 활용될 yaml 파일 내용은 아래와 같습니다.\n[root@m01 pod-example]# cat nfs-pod.yml apiVersion: v1 kind: Pod metadata:  name: nfs-nginx spec:  containers:  - name: nginx  image: nginx  volumeMounts:  - name: nfsvol  mountPath: /usr/share/nginx/html  volumes:  - name : nfsvol  nfs:  path: /data/nfs-ngnix  server: 192.168.13.10 위 yaml 파일을 이용해 Pod 을 생성하면\n[root@m01 pod-example]# kubectl describe po nfs-nginx Name: nfs-nginx Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: w03/192.168.13.16 Start Time: Sun, 14 Apr 2019 13:44:52 +0900 Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.233.89.5 Containers:  nginx:  Container ID: docker://20fa842803535803e1c0c48c204cffe1d464f9f96e3fcf4d7eed11c0bb8aeed0  Image: nginx  Image ID: docker-pullable://nginx@sha256:50174b19828157e94f8273e3991026dc7854ec7dd2bbb33e7d3bd91f0a4b333d  Port: \u0026lt;none\u0026gt;  Host Port: \u0026lt;none\u0026gt;  State: Running  Started: Sun, 14 Apr 2019 13:45:13 +0900  Ready: True  Restart Count: 0  Environment: \u0026lt;none\u0026gt;  Mounts:  /usr/share/nginx/html from nfsvol (rw)  /var/run/secrets/kubernetes.io/serviceaccount from default-token-9vmtn (ro) Conditions:  Type Status  Initialized True  Ready True  ContainersReady True  PodScheduled True Volumes:  nfsvol:  Type: NFS (an NFS mount that lasts the lifetime of a pod)  Server: 192.168.13.10  Path: /data/nfs-ngnix  ReadOnly: false  default-token-9vmtn:  Type: Secret (a volume populated by a Secret)  SecretName: default-token-9vmtn  Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s  node.kubernetes.io/unreachable:NoExecute for 300s Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Normal Scheduled 50s default-scheduler Successfully assigned default/nfs-nginx to w03  Normal Pulling 46s kubelet, w03 pulling image \u0026#34;nginx\u0026#34;  Normal Pulled 28s kubelet, w03 Successfully pulled image \u0026#34;nginx\u0026#34;  Normal Created 28s kubelet, w03 Created container  Normal Started 28s kubelet, w03 Started container nfs-nginx 라는 Pod 이 생성이 되고 위와 같이 nfsvol 이라는 Volume 이 Attach 된 것을 확인 할 수 있습니다.\nnginx 서비스가 연결된 Volume 을 통해 서비스가 되는지 확인해봅니다.\n[root@m01 pod-example]# curl 10.233.89.5 \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;NFS Index-v1\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; [root@m01 pod-example]#  # Pod 내부에 접근해서 확인 [root@m01 pod-example]# kubectl exec -ti nfs-nginx /bin/bash root@nfs-nginx:/# root@nfs-nginx:/# cd /usr/share/nginx/html/ root@nfs-nginx:/usr/share/nginx/html# cat index.html \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;NFS Index-v1\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; root@nfs-nginx:/usr/share/nginx/html# NFS Index-v1 라는 index.html 을 가지고 있는 Volume 입니다.\nNFS 서버에 직접 접근해서 index.html 파일을 수정해보겠습니다.\n[root@kube-depoly nfs-ngnix]# pwd /data/nfs-ngnix [root@kube-depoly nfs-ngnix]# cat index.html \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;NFS Index-v1\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; [root@kube-depoly nfs-ngnix]# vi index.html // index.html 수정 [root@kube-depoly nfs-ngnix]# cat index.html \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;NFS Index-v2\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; [root@kube-depoly nfs-ngnix]#  # 적용 확인 [root@m01 pod-example]# curl 10.233.89.5 \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;NFS Index-v2\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; [root@m01 pod-example]# 이와 같이 Pod 에 NFS 서버가 연결 되어 있는 것을 확인 할 수 있었습니다.\n# NFS 로 Volume Attach 되어 있음 root@nfs-nginx:/usr/share/nginx/html# mount | grep nfs 192.168.13.10:/data/nfs-ngnix on /usr/share/nginx/html type nfs4 (rw,relatime,vers=4.1,rsize=262144,wsize=262144,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.13.15,local_lock=none,addr=192.168.13.10) root@nfs-nginx:/usr/share/nginx/html# cephfs Software Defined Storage 인 ceph 를 이용하는 방식입니다.\n Worker 노드에서 cephfs 를 사용하기 위해 ceph-common 패키지를 설치합니다.\n # yum -y install epel-release # rpm -Uvh https://download.ceph.com/rpm-luminous/el7/noarch/ceph-release-1-0.el7.noarch.rpm # yum -y install ceph-common cephfs 를 사용하기 위해서는 Key 가 필요로 한데, 아래와 같은 방식으로 Key 값을 수집하고 Kubernete Secret 에 등록합니다.\n[root@s01 yum.repos.d]# ceph auth get client.admin exported keyring for client.admin [client.admin]  key = AQC6s6Vc83jwKBAAtckE6yz3eTM9lWwK60QNYw==  caps mds = \u0026#34;allow *\u0026#34;  caps mgr = \u0026#34;allow *\u0026#34;  caps mon = \u0026#34;allow *\u0026#34;  caps osd = \u0026#34;allow *\u0026#34; [root@s01 yum.repos.d]#  [root@s01 ceph]# ceph-authtool -p ceph.client.admin.keyring AQC6s6Vc83jwKBAAtckE6yz3eTM9lWwK60QNYw== [root@s01 ceph]#  # Secret 생성 [root@m01 cephfs]# cat ceph-secret.yaml apiVersion: v1 kind: Secret metadata:  name: ceph-secret data:  key: QVFDNnM2VmM4M2p3S0JBQXRja0U2eXozZVRNOWxXd0s2MFFOWXc9PQ==  [root@m01 cephfs]# kubectl create -f ceph-secret.yaml secret/ceph-secret created  [root@m01 cephfs]# kubectl get secret ceph-secret NAME TYPE DATA AGE ceph-secret Opaque 1 23s [root@m01 cephfs]# kubectl get secret ceph-secret -o yaml apiVersion: v1 data:  key: QVFDNnM2VmM4M2p3S0JBQXRja0U2eXozZVRNOWxXd0s2MFFOWXc9PQ== kind: Secret metadata:  creationTimestamp: \u0026#34;2019-04-14T06:13:44Z\u0026#34;  name: ceph-secret  namespace: default  resourceVersion: \u0026#34;873772\u0026#34;  selfLink: /api/v1/namespaces/default/secrets/ceph-secret  uid: 7515ad43-5e7c-11e9-ba95-001a4a160172 type: Opaque [root@m01 cephfs]# Pod 에 cephfs Volume 을 연결합니다.\n[root@m01 cephfs]# cat cephfs-with-secret.yaml apiVersion: v1 kind: Pod metadata:  name: cephfs-httpd spec:  containers:  - name: cephfs-httpd  image: httpd  volumeMounts:  - mountPath: /usr/local/apache2/htdocs  name: cephfs  volumes:  - name: cephfs  cephfs:  monitors:  - 192.168.13.6:6789  - 192.168.13.7:6789  - 192.168.13.8:6789  user: admin  path: /httpd-index  secretRef:  name: ceph-secret  readOnly: false cephfs 의 /httpd-index 경로에는 index.html 이 존재합니다.\nPod 을 생성합니다.\n[root@m01 cephfs]# kubectl create -f cephfs-with-secret.yaml pod/cephfs-httpd created [root@m01 cephfs]# kubectl get po NAME READY STATUS RESTARTS AGE cephfs-httpd 1/1 Running 0 24s load-generator-557649ddcd-jq987 1/1 Running 1 4d20h php-apache-9bd5c887f-p6lrq 1/1 Running 0 4d20h [root@m01 cephfs]#  [root@m01 cephfs]# kubectl describe po cephfs-httpd Name: cephfs-httpd Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: w03/192.168.13.16 Start Time: Sun, 14 Apr 2019 15:16:48 +0900 Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.233.89.6 Containers:  cephfs-httpd:  Container ID: docker://71e17fb3708a68448fdded4a20c81af63716a1146156dc5a5b4b8145a290f3dc  Image: httpd  Image ID: docker-pullable://httpd@sha256:b4096b744d92d1825a36b3ace61ef4caa2ba57d0307b985cace4621139c285f7  Port: \u0026lt;none\u0026gt;  Host Port: \u0026lt;none\u0026gt;  State: Running  Started: Sun, 14 Apr 2019 15:17:04 +0900  Ready: True  Restart Count: 0  Environment: \u0026lt;none\u0026gt;  Mounts:  /usr/local/apache2/htdocs from cephfs (rw)  /var/run/secrets/kubernetes.io/serviceaccount from default-token-9vmtn (ro) Conditions:  Type Status  Initialized True  Ready True  ContainersReady True  PodScheduled True Volumes:  cephfs:  Type: CephFS (a CephFS mount on the host that shares a pods lifetime)  Monitors: [192.168.13.6:6789 192.168.13.7:6789 192.168.13.8:6789]  Path: /httpd-index  User: admin  SecretFile:  SecretRef: \u0026amp;LocalObjectReference{Name:ceph-secret,}  ReadOnly: false  default-token-9vmtn:  Type: Secret (a volume populated by a Secret)  SecretName: default-token-9vmtn  Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s  node.kubernetes.io/unreachable:NoExecute for 300s Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Normal Scheduled 36s default-scheduler Successfully assigned default/cephfs-httpd to w03  Normal Pulling 33s kubelet, w03 pulling image \u0026#34;httpd\u0026#34;  Normal Pulled 21s kubelet, w03 Successfully pulled image \u0026#34;httpd\u0026#34;  Normal Created 20s kubelet, w03 Created container  Normal Started 20s kubelet, w03 Started container [root@m01 cephfs]#  # Pod 테스트 [root@m01 cephfs]# curl 10.233.89.6 cephfs Index - v1 [root@m01 cephfs]# cephfs는 NFS 와 매우 유사합니다. 그로 인해 NFS와 동일하게 PV에 직접 접근해서 파일 내용을 수정하고, Volume Attach 내용을 확인 할 수 있습니다.\n# Mount 확인 - Pod 이 작동중인 Worker 노드 [root@w03 ~]# mount | grep ceph 192.168.13.6:6789,192.168.13.7:6789,192.168.13.8:6789:/httpd-index on /var/lib/kubelet/pods/e2b5c688-5e7c-11e9-ba95-001a4a160172/volumes/kubernetes.io~cephfs/cephfs type ceph (rw,relatime,name=admin,secret=\u0026lt;hidden\u0026gt;,acl,wsize=16777216) [root@w03 ~]# [root@w03 ~]# cd /var/lib/kubelet/pods/e2b5c688-5e7c-11e9-ba95-001a4a160172/volumes/kubernetes.io~cephfs/cephfs [root@w03 cephfs]# ls -la 합계 1 drwxr-xr-x 1 root root 1 4월 14 15:05 . drwxr-x--- 3 root root 20 4월 14 15:16 .. -rw-r--r-- 1 root root 18 4월 14 15:05 index.html [root@w03 cephfs]# cat index.html cephfs Index - v1 [root@w03 cephfs]#  # Cephfs 에 접근해서 직접 파일 수정 [root@kube-depoly httpd-index]# pwd /cephfs/httpd-index [root@kube-depoly httpd-index]# vi index.html [root@kube-depoly httpd-index]# cat index.html cephfs Index - v2 [root@kube-depoly httpd-index]#  [root@m01 cephfs]# curl 10.233.89.6 cephfs Index - v2 [root@m01 cephfs]# 지금까지 Static Provisioning 관련 해서 확인 해보았습니다.\n개발자가 PVC를 통해 시스템 관리자에게 PV를 요구하는 과정을 통해 PV를 할당 받고 사용이 가능한데 이 과정을 자동화를 하게 되면 Dynamic Provisioning 이라고 합니다.\nDynamic Provisioning  ceph rbd Dynamic Provisioning는 PVC를 통해 요청하는 PV대해 동적으로 생성을 해주는 제공 방식을 말합니다.\n개발자는 StorageClass 를 통해 필요한 Storage Type을 지정하여 동적으로 할당을 받을 수 있습니다.\n# Secret 생성 - ceph Login을 위한 Key [root@m01 ceph-rbd]# kubectl create -f ceph-admin-secret.yml secret/ceph-admin-secret created [root@m01 ceph-rbd]# kubectl create -f ceph-secret.yml secret/ceph-secret created # StorageClass 생성 [root@m01 ceph-rbd]# cat class.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata:  name: rbd provisioner: ceph.com/rbd parameters:  monitors: 192.168.13.6:6789,192.168.13.7:6789,192.168.13.8:6789  pool: kube  adminId: admin  adminSecretNamespace: kube-system  adminSecretName: ceph-admin-secret  userId: kube  userSecretNamespace: kube-system  userSecretName: ceph-secret  imageFormat: \u0026#34;2\u0026#34;  imageFeatures: layering [root@m01 ceph-rbd]# kubectl get sc NAME PROVISIONER AGE rbd ceph.com/rbd 17h [root@m01 ceph-rbd]# StorageClass yaml 파일을 보면 provisioner: ceph.com/rbd 항목이 있습니다.\n 위와 같이 ceph rbd 를 제공해줄 provisioner 가 필요합니다.\nprovisioner 상세 배포 방식은 ceph-rbd-depoly 문서를 참조합니다.\n # Pod  [root@m01 ceph-rbd]# kubectl get po -n kube-system | grep rbd rbd-provisioner-67b4857bcd-7ctlz 1/1 Running 0 17h  # Pod 상세 내역 [root@m01 ceph-rbd]# kubectl describe po rbd-provisioner-67b4857bcd-7ctlz -n kube-system Name: rbd-provisioner-67b4857bcd-7ctlz Namespace: kube-system Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: w02/192.168.13.15 Start Time: Sun, 14 Apr 2019 21:13:49 +0900 Labels: app=rbd-provisioner  pod-template-hash=67b4857bcd Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.233.96.9 Controlled By: ReplicaSet/rbd-provisioner-67b4857bcd Containers:  rbd-provisioner:  Container ID: docker://8f25dca0c870685dc0140294787124e288793243ed6120921d278c701b6c7039  Image: quay.io/external_storage/rbd-provisioner:latest  Image ID: docker-pullable://quay.io/external_storage/rbd-provisioner@sha256:94fd36b8625141b62ff1addfa914d45f7b39619e55891bad0294263ecd2ce09a  Port: \u0026lt;none\u0026gt;  Host Port: \u0026lt;none\u0026gt;  State: Running  Started: Sun, 14 Apr 2019 21:13:54 +0900  Ready: True  Restart Count: 0  Environment:  PROVISIONER_NAME: ceph.com/rbd  Mounts:  /var/run/secrets/kubernetes.io/serviceaccount from rbd-provisioner-token-79f4c (ro) Conditions:  Type Status  Initialized True  Ready True  ContainersReady True  PodScheduled True Volumes:  rbd-provisioner-token-79f4c:  Type: Secret (a volume populated by a Secret)  SecretName: rbd-provisioner-token-79f4c  Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s  node.kubernetes.io/unreachable:NoExecute for 300s Events: \u0026lt;none\u0026gt; PVC 를 통해 rbd PV를 요청합니다.\n[root@m01 ceph-rbd]# cat claim.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata:  name: rbd-pvc spec:  accessModes:  - ReadWriteOnce  storageClassName: rbd  resources:  requests:  storage: 2Gi  # PVC 생성 [root@m01 ceph-rbd]# kubectl create -f claim.yaml persistentvolumeclaim/rbd-pvc created  # PVC 확인 [root@m01 ceph-rbd]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim1 Bound pvc-fe9d8199-5eae-11e9-ba95-001a4a160172 1Gi RWO rbd 17h nginx-vol-pvc Bound nginx-pv 3Gi RWX 17h rbd-pvc Bound pvc-5b571f95-5f43-11e9-ba95-001a4a160172 2Gi RWO rbd 3s  # PV 연결 확인 [root@m01 ceph-rbd]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nginx-pv 3Gi RWX Retain Bound default/nginx-vol-pvc 17h pvc-5b571f95-5f43-11e9-ba95-001a4a160172 2Gi RWO Delete Bound default/rbd-pvc rbd 5s pvc-fe9d8199-5eae-11e9-ba95-001a4a160172 1Gi RWO Delete Bound default/claim1 rbd 17h [root@m01 ceph-rbd]# 위와 같이 PVC 를 요청하자 바로 PV가 ceph rbd 형식으로 생성이 되고 Attach 된 것을 확인 할 수 있었습니다.\n[root@m01 ceph-rbd]# kubectl logs -f rbd-provisioner-67b4857bcd-7ctlz -n kube-system I0414 12:13:54.944458 1 main.go:85] Creating RBD provisioner ceph.com/rbd with identity: ceph.com/rbd I0414 12:13:54.949989 1 leaderelection.go:185] attempting to acquire leader lease kube-system/ceph.com-rbd... I0414 12:13:55.001529 1 leaderelection.go:194] successfully acquired lease kube-system/ceph.com-rbd I0414 12:13:55.001754 1 event.go:221] Event(v1.ObjectReference{Kind:\u0026#34;Endpoints\u0026#34;, Namespace:\u0026#34;kube-system\u0026#34;, Name:\u0026#34;ceph.com-rbd\u0026#34;, UID:\u0026#34;c5bb0c91-5eae-11e9-b387-001a4a160174\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;919145\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;LeaderElection\u0026#39; rbd-provisioner-67b4857bcd-7ctlz_c5ecdcd7-5eae-11e9-a9ca-6e9439dbce0f became leader I0414 12:13:55.001901 1 controller.go:631] Starting provisioner controller ceph.com/rbd_rbd-provisioner-67b4857bcd-7ctlz_c5ecdcd7-5eae-11e9-a9ca-6e9439dbce0f! I0414 12:13:55.102448 1 controller.go:680] Started provisioner controller ceph.com/rbd_rbd-provisioner-67b4857bcd-7ctlz_c5ecdcd7-5eae-11e9-a9ca-6e9439dbce0f! I0414 12:15:33.439001 1 controller.go:987] provision \u0026#34;default/claim1\u0026#34; class \u0026#34;rbd\u0026#34;: started I0414 12:15:33.455112 1 event.go:221] Event(v1.ObjectReference{Kind:\u0026#34;PersistentVolumeClaim\u0026#34;, Namespace:\u0026#34;default\u0026#34;, Name:\u0026#34;claim1\u0026#34;, UID:\u0026#34;fe9d8199-5eae-11e9-ba95-001a4a160172\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;919428\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;Provisioning\u0026#39; External provisioner is provisioning volume for claim \u0026#34;default/claim1\u0026#34; I0414 12:15:35.895596 1 provision.go:132] successfully created rbd image \u0026#34;kubernetes-dynamic-pvc-00ab7b91-5eaf-11e9-a9ca-6e9439dbce0f\u0026#34; I0414 12:15:35.895798 1 controller.go:1087] provision \u0026#34;default/claim1\u0026#34; class \u0026#34;rbd\u0026#34;: volume \u0026#34;pvc-fe9d8199-5eae-11e9-ba95-001a4a160172\u0026#34; provisioned I0414 12:15:35.895924 1 controller.go:1101] provision \u0026#34;default/claim1\u0026#34; class \u0026#34;rbd\u0026#34;: trying to save persistentvvolume \u0026#34;pvc-fe9d8199-5eae-11e9-ba95-001a4a160172\u0026#34; I0414 12:15:35.934205 1 controller.go:1108] provision \u0026#34;default/claim1\u0026#34; class \u0026#34;rbd\u0026#34;: persistentvolume \u0026#34;pvc-fe9d8199-5eae-11e9-ba95-001a4a160172\u0026#34; saved I0414 12:15:35.934420 1 controller.go:1149] provision \u0026#34;default/claim1\u0026#34; class \u0026#34;rbd\u0026#34;: succeeded I0414 12:15:35.935026 1 event.go:221] Event(v1.ObjectReference{Kind:\u0026#34;PersistentVolumeClaim\u0026#34;, Namespace:\u0026#34;default\u0026#34;, Name:\u0026#34;claim1\u0026#34;, UID:\u0026#34;fe9d8199-5eae-11e9-ba95-001a4a160172\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;919428\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;ProvisioningSucceeded\u0026#39; Successfully provisioned volume pvc-fe9d8199-5eae-11e9-ba95-001a4a160172 I0415 05:57:32.016818 1 controller.go:987] provision \u0026#34;default/rbd-pvc\u0026#34; class \u0026#34;rbd\u0026#34;: started I0415 05:57:32.037901 1 event.go:221] Event(v1.ObjectReference{Kind:\u0026#34;PersistentVolumeClaim\u0026#34;, Namespace:\u0026#34;default\u0026#34;, Name:\u0026#34;rbd-pvc\u0026#34;, UID:\u0026#34;5b571f95-5f43-11e9-ba95-001a4a160172\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;1081791\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;Provisioning\u0026#39; External provisioner is provisioning volume for claim \u0026#34;default/rbd-pvc\u0026#34; I0415 05:57:33.772819 1 provision.go:132] successfully created rbd image \u0026#34;kubernetes-dynamic-pvc-5be57bb6-5f43-11e9-a9ca-6e9439dbce0f\u0026#34; I0415 05:57:33.773007 1 controller.go:1087] provision \u0026#34;default/rbd-pvc\u0026#34; class \u0026#34;rbd\u0026#34;: volume \u0026#34;pvc-5b571f95-5f43-11e9-ba95-001a4a160172\u0026#34; provisioned I0415 05:57:33.773112 1 controller.go:1101] provision \u0026#34;default/rbd-pvc\u0026#34; class \u0026#34;rbd\u0026#34;: trying to save persistentvvolume \u0026#34;pvc-5b571f95-5f43-11e9-ba95-001a4a160172\u0026#34; I0415 05:57:33.793499 1 controller.go:1108] provision \u0026#34;default/rbd-pvc\u0026#34; class \u0026#34;rbd\u0026#34;: persistentvolume \u0026#34;pvc-5b571f95-5f43-11e9-ba95-001a4a160172\u0026#34; saved I0415 05:57:33.793633 1 controller.go:1149] provision \u0026#34;default/rbd-pvc\u0026#34; class \u0026#34;rbd\u0026#34;: succeeded I0415 05:57:33.793801 1 controller.go:987] provision \u0026#34;default/rbd-pvc\u0026#34; class \u0026#34;rbd\u0026#34;: started I0415 05:57:33.794971 1 event.go:221] Event(v1.ObjectReference{Kind:\u0026#34;PersistentVolumeClaim\u0026#34;, Namespace:\u0026#34;default\u0026#34;, Name:\u0026#34;rbd-pvc\u0026#34;, UID:\u0026#34;5b571f95-5f43-11e9-ba95-001a4a160172\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;1081791\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;ProvisioningSucceeded\u0026#39; Successfully provisioned volume pvc-5b571f95-5f43-11e9-ba95-001a4a160172 I0415 05:57:33.826515 1 controller.go:996] provision \u0026#34;default/rbd-pvc\u0026#34; class \u0026#34;rbd\u0026#34;: persistentvolume \u0026#34;pvc-5b571f95-5f43-11e9-ba95-001a4a160172\u0026#34; already exists, skipping  참조: rbd-provisioner docker log  Pod 에 PVC를 yaml 추가하여 Pod 생성 할때, PV를 요청하고 StorageClass 를 이용해서 동적으로 Volume을 할당 받았습니다.\n아래는 Pod 에 PVC를 추가한 yaml 구문 Template 입니다.\n[root@m01 ceph-rbd]# cat test-pod.yaml kind: Pod apiVersion: v1 metadata:  name: test-pod spec:  containers:  - name: test-pod  image: gcr.io/google_containers/busybox:1.24  command:  - \u0026#34;/bin/sh\u0026#34;  args:  - \u0026#34;-c\u0026#34;  - \u0026#34;touch /mnt/SUCCESS \u0026amp;\u0026amp; exit 0 || exit 1\u0026#34;  volumeMounts:  - name: pvc  mountPath: \u0026#34;/mnt\u0026#34;  restartPolicy: \u0026#34;Never\u0026#34;  volumes:  - name: pvc  persistentVolumeClaim:  claimName: claim1 지금까지 포스팅에서 소개된 Volume 은 Kubernetes 에서 제공되는 일부 입니다.\n다양한 Volume 을 제공하고 있으며, 상세 내용은 첨부된 문서 참고하시고 운영하시는 환경에 맞게 사용하면 됩니다.\n감사합니다.\n참고 문서  - ceph rbd 관련\n RBD Volume Provisioner for Kubernetes 1.5+ RBD Volume Provisioner On Kubernetes Depolyment Example Ceph rbd  - Kubernetes 문서\n PV 관련 Types of Volumes PV Access Mode 관련 PV Reclaim Policy Example PVC  - Example yaml\n chhanz Github  ","permalink":"https://chhanz88.github.io/post/2019-04-15-kubernetes-chapter-2-network-volume/","summary":"이번 포스팅은 Kubernetes Korea Group의 Kubernetes Architecture Study 모임에서 스터디 후, 발표된 내용입니다.\nLink : k8skr-study-architecture Github\n Kubernetes Volume #2  저번 포스팅 Kubernetes Volume #1 에서는 Local Volume 에 관련된 emptyDir / hostPath / gitRepo 에 대해 설명드렸습니다.\n이어서 이번 포스팅에서는 Network Volume 으로 사용될 nfs / cephfs / ceph rbd 를 예제와 함께 알아보도록 하겠습니다.\nPersistent Volume 와 Persistent Volume Claim  Persistent Volume 와 Persistent VolumeClaim 가 있는데,","title":"[Kubernetes] Kubernetes Volume #2"},{"content":" 이번 포스팅은 Kubernetes Korea Group의 Kubernetes Architecture Study 모임에서 스터디 후, 발표된 내용입니다.\nLink : k8skr-study-architecture Github\n Kubernetes Volume  Kubernetes 에서 Volume 으로 사용 가능한 유형은 아래와 같습니다.\n  emptyDir hostPath gitRepo Openstack Cinder cephfs iscsi rbd 그 외 Public Cloud Storage   이처럼 Kubernetes 에서는 다양한 Volume 을 지원합니다.\n책에 소개된 emptyDir / hostPath / gitRepo 에 대해 예제와 함께 어떤식으로 사용이 되는지 확인 해보겠습니다.\n추가로 책에는 없는 nfs / cephfs / rbd 를 Kubernetes Volume 으로 사용 해보겠습니다.\nemptyDir  emptyDir 은 Pod 과 함께 생성되고, 삭제되는 임시 Volume 입니다.\n컨테이너 단위로 관리되는 것이 아니고 Pod 단위로 관리가 되기 때문에 Pod 내 컨테이너가 Error 로 인해 삭제 혹은 재시작이 되더라도 emptyDir 은 삭제가 되지 않고 계속 사용이 가능합니다.\nemptyDir 예제 # cat fortuneloop.sh #!/bin/bash trap \u0026#34;exit\u0026#34; SIGINT mkdir /var/htdocs while : do  echo $(date) Writing fortune to /var/htdocs/index.html  /usr/games/fortune \u0026gt; /var/htdocs/index.html  sleep 10 done 위 스크립트를 이용해서 Docker 이미지를 Build 합니다.\nFROMubuntu:latestRUN apt-get update ; apt-get -y install fortuneADD fortuneloop.sh /bin/fortuneloop.shENTRYPOINT /bin/fortuneloop.shDocker Image 를 Build 합니다.\n# docker build -t han0495/fortune . Sending build context to Docker daemon 3.072kB Step 1/4 : FROM ubuntu:latest  ---\u0026gt; 94e814e2efa8 Step 2/4 : RUN apt-get update ; apt-get -y install fortune  ---\u0026gt; Running in 3c8f694a68af  \u0026lt; 중 략 \u0026gt;  Removing intermediate container 3c8f694a68af  ---\u0026gt; 1e8b262e7bdf Step 3/4 : ADD fortuneloop.sh /bin/fortuneloop.sh  ---\u0026gt; 3eee41108b5b Step 4/4 : ENTRYPOINT /bin/fortuneloop.sh  ---\u0026gt; Running in 082ee707cdf1 Removing intermediate container 082ee707cdf1  ---\u0026gt; 58d2d430a7b4 Successfully built 58d2d430a7b4 Successfully tagged han0495/fortune:latest [root@m01 inside]# 아래와 같은 yaml 파일을 작성합니다.\n# cat fortune.yml apiVersion: v1 kind: Pod metadata:  name: fortune spec:  containers:  - image: han0495/fortune  name: html-generator  volumeMounts:  - name: html  mountPath: /var/htdocs  - image: nginx:alpine  name: web-server  volumeMounts:  - name: html  mountPath: /usr/share/nginx/html  readOnly: true  ports:  - containerPort: 80  protocol: TCP  volumes:  - name: html  emptyDir: {} 작성한 yaml 파일을 이용하여, Pod을 생성합니다.\n[root@m01 fortune]# kubectl get po NAME READY STATUS RESTARTS AGE fortune 2/2 Running 0 5m10s load-generator-557649ddcd-nl6js 1/1 Running 1 6d18h php-apache-9bd5c887f-nm4h5 1/1 Running 0 6d18h tomcat-f94554bb9-gkhpz 1/1 Running 0 7d web-7d77974d4c-gd76n 1/1 Running 0 7d2h  [root@m01 fortune]# kubectl describe po fortune Name: fortune Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: w03/192.168.13.16 Start Time: Mon, 08 Apr 2019 17:41:40 +0900 Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.233.89.8 Containers:  html-generator:  Container ID: docker://e25bc8c3b94a2a02edc8b983eb77214a4644a99a3931c1f96f131819060cc676  Image: han0495/fortune  Image ID: docker-pullable://han0495/fortune@sha256:63d5786a84e67dcd5eec70d516d5788c8e3c3a90d23f23bec1825f7a4526bb00  Port: \u0026lt;none\u0026gt;  Host Port: \u0026lt;none\u0026gt;  State: Running  Started: Mon, 08 Apr 2019 17:42:01 +0900  Ready: True  Restart Count: 0  Environment: \u0026lt;none\u0026gt;  Mounts:  /var/htdocs from html (rw)  /var/run/secrets/kubernetes.io/serviceaccount from default-token-vt6hm (ro)  web-server:  Container ID: docker://ed7c2fd5adbb919fde6ed01d1a80fa74df689d1aa99bc8d883b1b68ed918dd09  Image: nginx:alpine  Image ID: docker-pullable://nginx@sha256:d5e177fed5e4f264e55b19b84bdc494078a06775612a4f60963f296756ea83aa  Port: 80/TCP  Host Port: 0/TCP  State: Running  Started: Mon, 08 Apr 2019 17:42:09 +0900  Ready: True  Restart Count: 0  Environment: \u0026lt;none\u0026gt;  Mounts:  /usr/share/nginx/html from html (ro)  /var/run/secrets/kubernetes.io/serviceaccount from default-token-vt6hm (ro) Conditions:  Type Status  Initialized True  Ready True  ContainersReady True  PodScheduled True Volumes:  html:  Type: EmptyDir (a temporary directory that shares a pod\u0026#39;s lifetime)  Medium:  default-token-vt6hm:  Type: Secret (a volume populated by a Secret)  SecretName: default-token-vt6hm  Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s  node.kubernetes.io/unreachable:NoExecute for 300s Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Normal Scheduled 5m26s default-scheduler Successfully assigned default/fortune to w03  Normal Pulling 5m23s kubelet, w03 pulling image \u0026#34;han0495/fortune\u0026#34;  Normal Pulled 5m5s kubelet, w03 Successfully pulled image \u0026#34;han0495/fortune\u0026#34;  Normal Created 5m5s kubelet, w03 Created container  Normal Started 5m4s kubelet, w03 Started container  Normal Pulling 5m4s kubelet, w03 pulling image \u0026#34;nginx:alpine\u0026#34;  Normal Pulled 4m56s kubelet, w03 Successfully pulled image \u0026#34;nginx:alpine\u0026#34;  Normal Created 4m56s kubelet, w03 Created container  Normal Started 4m56s kubelet, w03 Started container [root@m01 fortune]# 실제로 파일이 Docker 이미지에 넣은 스크립트가 동작하는지 확인합니다.\n[root@m01 fortune]# kubectl exec -ti fortune /bin/bash Defaulting container name to html-generator. Use \u0026#39;kubectl describe pod/fortune -n default\u0026#39; to see all of the containers in this pod. root@fortune:/# root@fortune:/# cd /var root@fortune:/var# cd htdocs/ root@fortune:/var/htdocs# ls index.html root@fortune:/var/htdocs# root@fortune:/var/htdocs# cat index.html You are standing on my toes. root@fortune:/var/htdocs# while true \u0026gt; do \u0026gt; date \u0026gt; cat index.html \u0026gt; sleep 10 \u0026gt; done Mon Apr 8 08:48:43 UTC 2019 Artistic ventures highlighted. Rob a museum. Mon Apr 8 08:48:53 UTC 2019 Never reveal your best argument. Mon Apr 8 08:49:03 UTC 2019 You can create your own opportunities this week. Blackmail a senior executive. Mon Apr 8 08:49:13 UTC 2019 You have a strong appeal for members of the opposite sex. Mon Apr 8 08:49:23 UTC 2019 You will be reincarnated as a toad; and you will be much happier. ^C root@fortune:/var/htdocs# hostPath  hostPath는 로컬 디스크의 경로를 Pod 에 Mount 해서 사용하는 Volume 방식입니다.\nDocker 에서 -v 옵션으로 Volume 을 연결하는 것과 동일하다고 생각하면 됩니다.\nhostPath 예제 # cat hostpath-pod.yml apiVersion: v1 kind: Pod metadata:  name: hostpath spec:  containers:  - name: nginx  image: nginx  volumeMounts:  - name: volumepath  mountPath: /usr/share/nginx/html  volumes:  - name : volumepath  hostPath:  path: /imsi  type: Directory nginx 의 index Directory 에 /imsi 라는 로컬 디스크 경로를 Mount 합니다.\n이후 Pod 을 해당 yaml 을 이용해서 구동합니다.\n[root@m01 pod-example]# kubectl exec -ti hostpath -- /bin/bash root@hostpath:/# root@hostpath:/# ls bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var root@hostpath:/# root@hostpath:/# cd /usr root@hostpath:/usr# cd share/ root@hostpath:/usr/share# cd nginx/ root@hostpath:/usr/share/nginx# cd html/ root@hostpath:/usr/share/nginx/html# ls root@hostpath:/usr/share/nginx/html# root@hostpath:/usr/share/nginx/html# touch test root@hostpath:/usr/share/nginx/html# exit exit [root@m01 pod-example]# Pod 이 구동된 Worker의 로컬 디스크 /imsi 경로를 확인해보면 아래와 같이 파일이 생성된 것을 확인 할 수 있습니다.\n[root@w01 ~]# ls -la /imsi 합계 0 drwxrwxrwx 2 root root 18 4월 9 13:15 . dr-xr-xr-x. 18 root root 256 4월 9 09:45 .. -rw-r--r-- 1 root root 0 4월 9 13:15 test [root@w01 ~]# gitRepo  gitRepo 는 github에 있는 Repository 에서 Source 를 Clone 하고 해당 Clone 된 데이터를 Pod 의 Volume으로 할당합니다.\ngitRepo 예제 # cat git-http.yml apiVersion: v1 kind: Pod metadata:  name: gitrepo-httpd spec:  containers:  - image: httpd  name: web-server  volumeMounts:  - name: html  mountPath: /usr/local/apache2/htdocs  readOnly: true  ports:  - containerPort: 80  protocol: TCP  volumes:  - name: html  gitRepo:  repository: https://github.com/chhanz/docker_training.git  revision: master  directory: . 위와 같이 Github에서 docker_training.git Source를 Clone 합니다.\n[root@m01 tmp]# kubectl describe po gitrepo-httpd Name: gitrepo-httpd Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: w02/192.168.13.15 Start Time: Mon, 08 Apr 2019 21:51:43 +0900 Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Status: Pending IP: Containers:  web-server:  Container ID:  Image: nginx:alpine  Image ID:  Port: 80/TCP  Host Port: 0/TCP  State: Waiting  Reason: ContainerCreating  Ready: False  Restart Count: 0  Environment: \u0026lt;none\u0026gt;  Mounts:  /usr/share/nginx/html from html (ro)  /var/run/secrets/kubernetes.io/serviceaccount from default-token-vt6hm (ro) Conditions:  Type Status  Initialized True  Ready False  ContainersReady False  PodScheduled True Volumes:  html:  Type: GitRepo (a volume that is pulled from git when the pod is created)  Repository: https://github.com/chhanz/docker_training.git  Revision: master  default-token-vt6hm:  Type: Secret (a volume populated by a Secret)  SecretName: default-token-vt6hm  Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s  node.kubernetes.io/unreachable:NoExecute for 300s Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Normal Scheduled 6s default-scheduler Successfully assigned default/gitrepo-httpd to w02  Warning FailedMount 1s (x4 over 5s) kubelet, w02 MountVolume.SetUp failed for volume \u0026#34;html\u0026#34; : failed to exec \u0026#39;git clone -- https://github.com/chhanz/docker_training.git .\u0026#39;: : executable file not found in $PATH 위와 같이 에러가 발생하며, Volume 이 Mount 가 안되었습니다.\n이유는 Pod 이 실행될 Worker 노드에 git 명령어가 없어서 발생된 에러였습니다.\n[root@m01 gitrepo-pv]# kubectl describe po gitrepo-httpd Name: gitrepo-httpd Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: w01/192.168.13.14 Start Time: Mon, 08 Apr 2019 21:58:23 +0900 Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.233.118.8 Containers:  web-server:  Container ID: docker://38158075431bab4e7cfe22e34b615e66ac04e37ad7832a531d811cd67b27962a  Image: httpd  Image ID: docker-pullable://httpd@sha256:b4096b744d92d1825a36b3ace61ef4caa2ba57d0307b985cace4621139c285f7  Port: 80/TCP  Host Port: 0/TCP  State: Running  Started: Mon, 08 Apr 2019 21:58:49 +0900  Ready: True  Restart Count: 0  Environment: \u0026lt;none\u0026gt;  Mounts:  /usr/local/apache2/htdocs from html (ro)  /var/run/secrets/kubernetes.io/serviceaccount from default-token-vt6hm (ro) Conditions:  Type Status  Initialized True  Ready True  ContainersReady True  PodScheduled True Volumes:  html:  Type: GitRepo (a volume that is pulled from git when the pod is created)  Repository: https://github.com/chhanz/docker_training.git  Revision: master  default-token-vt6hm:  Type: Secret (a volume populated by a Secret)  SecretName: default-token-vt6hm  Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s  node.kubernetes.io/unreachable:NoExecute for 300s Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Normal Scheduled 28s default-scheduler Successfully assigned default/gitrepo-httpd to w01  Normal Pulling 21s kubelet, w01 pulling image \u0026#34;httpd\u0026#34;  Normal Pulled 0s kubelet, w01 Successfully pulled image \u0026#34;httpd\u0026#34;  Normal Created 0s kubelet, w01 Created container  Normal Started 0s kubelet, w01 Started container [root@m01 gitrepo-pv]# kubectl get po NAME READY STATUS RESTARTS AGE fortune 2/2 Running 0 4h17m gitrepo-httpd 1/1 Running 0 33s load-generator-557649ddcd-nl6js 1/1 Running 1 6d22h php-apache-9bd5c887f-nm4h5 1/1 Running 0 6d22h tomcat-f94554bb9-gkhpz 1/1 Running 0 7d4h web-7d77974d4c-gd76n 1/1 Running 0 7d6h [root@m01 gitrepo-pv]# 위와 같이 Worker 노드에 git 명령을 설치한 이후, 정상적으로 Volume 이 Mount 되는 것을 확인 할 수 있었습니다.\n[root@w01 /]# find * | grep index.html  var/lib/kubelet/pods/fcd69917-59fd-11e9-a751-001a4a160172/volumes/kubernetes.io~git-repo/html/index.html  [root@w01 /]# cd var/lib/kubelet/pods/fcd69917-59fd-11e9-a751-001a4a160172/volumes/kubernetes.io~git-repo/html/ [root@w01 html]# ls copy.html index.html README.md util [root@w01 html]# ls -la total 20 drwxrwxrwx 4 root root 82 Apr 8 21:58 . drwxr-xr-x 3 root root 18 Apr 8 21:58 .. -rw-r--r-- 1 root root 2070 Apr 8 21:58 copy.html drwxr-xr-x 8 root root 180 Apr 8 21:58 .git -rw-r--r-- 1 root root 9594 Apr 8 21:58 index.html -rw-r--r-- 1 root root 108 Apr 8 21:58 README.md drwxr-xr-x 2 root root 23 Apr 8 21:58 util [root@w01 html]# [root@w01 html]# [root@w01 html]# cat README.md # Docker Training ReadMe Custumer training page   - index.html  - copy.html // Copy\u0026amp;Paste  - putty.exe 위와 같이 Worker 노드에 해당 git Source 를 Clone 하고 Pod 에 Mount 된 것을 확인 할 수 있었습니다.\n## Master Node [root@m01 gitrepo-pv]# kubectl delete -f git-http.yml pod \u0026#34;gitrepo-httpd\u0026#34; deleted  ## Worker Node [root@w01 kubernetes.io~git-repo]# cd .. cd: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [root@w01 ..]# ls [root@w01 ..]# [root@w01 ..]# ls -la total 0 [root@w01 ..]# pwd /var/lib/kubelet/pods/fcd69917-59fd-11e9-a751-001a4a160172/volumes/kubernetes.io~git-repo/.. [root@w01 ..]# [root@w01 ..]# [root@w01 ..]# cd .. cd: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [root@w01 ..]# ls [root@w01 ..]# [root@w01 ..]# ls -la /var/lib/kubelet/pods/fcd69917-59fd-11e9-a751-001a4a160172/ ls: cannot access /var/lib/kubelet/pods/fcd69917-59fd-11e9-a751-001a4a160172/: No such file or directory [root@w01 ..]# 이처럼 해당 Pod이 삭제되면 Clone 된 Git Source는 삭제가 되는 것을 확인 하였습니다.\n위 테스트를 진행해보니 Git Source 를 Clone 만하고 Github Repository를 계속 Sync하는 것은 아닌 것으로 확인 하였습니다.\n지금까지 emptyDir / hostPath / gitRepo 에 대해 예제를 통해 확인 하였습니다.\nnfs / cephfs / rbd 은 다음 포스팅에서 이어서 설명하도록 하겠습니다.\n","permalink":"https://chhanz88.github.io/post/2019-04-12-kubernetes-chapter-1-volume/","summary":"이번 포스팅은 Kubernetes Korea Group의 Kubernetes Architecture Study 모임에서 스터디 후, 발표된 내용입니다.\nLink : k8skr-study-architecture Github\n Kubernetes Volume  Kubernetes 에서 Volume 으로 사용 가능한 유형은 아래와 같습니다.\n  emptyDir hostPath gitRepo Openstack Cinder cephfs iscsi rbd 그 외 Public Cloud Storage   이처럼 Kubernetes 에서는 다양한 Volume 을 지원합니다.\n책에 소개된 emptyDir / hostPath / gitRepo 에 대해 예제와 함께 어떤식으로 사용이 되는지 확인 해보겠습니다.","title":"[Kubernetes] Kubernetes Volume #1"},{"content":"[Kubernetes] Kubernetes Horizontal Pod Autoscaler 테스트  Kubernetes 의 Horizontal Pod Autoscaler(이하 HPA) 를 테스트 해보겠습니다.\n부하 테스트 Image 생성  Dockerfile 을 생성합니다.\nFROMphp:5-apacheADD index.php /var/www/html/index.phpRUN chmod a+rx index.phpDocker image에 같이 추가 될 index.php 를 생성합니다.\n?php  $x = 0.0001;  for ($i = 0; $i \u0026lt;= 1000000; $i++) {  $x += sqrt($x);  }  echo \u0026#34;OK!\u0026#34;; ?\u0026gt;Image 를 Build 합니다.\n# docker build -t hpa-example . Sending build context to Docker daemon 3.072kB Step 1/3 : FROM php:5-apache 5-apache: Pulling from library/php 5e6ec7f28fb7: Pull complete cf165947b5b7: Pull complete 7bd37682846d: Pull complete 99daf8e838e1: Pull complete ae320713efba: Pull complete ebcb99c48d8c: Pull complete 9867e71b4ab6: Pull complete 936eb418164a: Pull complete bc298e7adaf7: Pull complete ccd61b587bcd: Pull complete b2d4b347f67c: Pull complete 56e9dde34152: Pull complete 9ad99b17eb78: Pull complete Digest: sha256:0a40fd273961b99d8afe69a61a68c73c04bc0caa9de384d3b2dd9e7986eec86d Status: Downloaded newer image for php:5-apache  ---\u0026gt; 24c791995c1e Step 2/3 : ADD index.php /var/www/html/index.php  ---\u0026gt; bcc3ff35ceb8 Step 3/3 : RUN chmod a+rx index.php  ---\u0026gt; Running in d1e173fb27a3 Removing intermediate container d1e173fb27a3  ---\u0026gt; f4bb43246866 Successfully built f4bb43246866 Successfully tagged hpa-example:latest 만들어진 이미지를 DockerHub 에 PUSH 합니다.\n[root@m01 hpa-example]# docker push han0495/hpa-example The push refers to repository [docker.io/han0495/hpa-example] b42dc42a2d18: Pushed 056e15bb815c: Pushed 1aab22401f12: Mounted from library/php 13ab94c9aa15: Mounted from library/php 588ee8a7eeec: Mounted from library/php bebcda512a6d: Mounted from library/php 5ce59bfe8a3a: Mounted from library/php d89c229e40ae: Mounted from library/php 9311481e1bdc: Mounted from library/php 4dd88f8a7689: Mounted from library/php b1841504f6c8: Mounted from library/php 6eb3cfd4ad9e: Mounted from library/php 82bded2c3a7c: Mounted from library/php b87a266e6a9c: Mounted from library/php 3c816b4ead84: Mounted from library/php latest: digest: sha256:07c8ebfbe5b8084b878d0ed4ebafe5baad124b0b932e4f56d81480fbf715f03f size: 3449 [root@m01 hpa-example]# HPA 생성  POD 및 HPA 를 아래와 같이 생성합니다.\n[root@m01 hpa-example]# kubectl run php-apache --image=han0495/hpa-example --requests=cpu=200m --expose --port=80 kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. service/php-apache created deployment.apps/php-apache created  [root@m01 hpa-example]# kubectl get po NAME READY STATUS RESTARTS AGE php-apache-9bd5c887f-nm4h5 1/1 Running 0 49s tomcat-f94554bb9-gkhpz 1/1 Running 0 6h28m web-7d77974d4c-gd76n 1/1 Running 0 7h52m [root@m01 hpa-example]# curl 10.233.14.29 OK! [root@m01 hpa-example]# curl 10.233.14.29 OK! [root@m01 hpa-example]# kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 horizontalpodautoscaler.autoscaling/php-apache autoscaled [root@m01 hpa-example]# POD 가 생성되고 Build 한 Container Image 가 정상적으로 작동 하는지 확인 하였습니다.\n부하 테스트  busybox POD 을 생성하고 HPA 를 생성한 POD 에 부하를 생성합니다.\n[root@m01 hpa-example]# kubectl run -i --tty load-generator --image=busybox /bin/sh kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. If you don\u0026#39;t see a command prompt, try pressing enter. / # while true; do wget -q -O- http://php-apache.default.svc.cluster.local; done OK! OK! OK! OK! OK! OK! OK! OK! OK! OK! OK! OK! OK! OK! OK! OK! OK! OK! OK! OK! 부하가 생성이 되고 일정 시간이 지나면, HPA 에서 리소스 사용량이 증가되고 HPA 는 지정한 MAXPODS 값에 맞게 POD의 수를 증가시킵니다.\nEvery 2.0s: kubectl get hpa;kubectl get po Tue Apr 9 19:07:57 2019  NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 477%/50% 1 10 4 4m21s NAME READY STATUS RESTARTS AGE load-generator-557649ddcd-jq987 1/1 Running 0 4m9s php-apache-9bd5c887f-8dc4m 0/1 ContainerCreating 0 4s php-apache-9bd5c887f-bdrlz 1/1 Running 0 20s php-apache-9bd5c887f-fthkk 0/1 ContainerCreating 0 4s php-apache-9bd5c887f-g46b8 0/1 ContainerCreating 0 20s php-apache-9bd5c887f-hq88l 0/1 ContainerCreating 0 4s php-apache-9bd5c887f-p6lrq 1/1 Running 0 5m11s php-apache-9bd5c887f-v5bhw 0/1 ContainerCreating 0 4s php-apache-9bd5c887f-wjlrm 0/1 ContainerCreating 0 20s 부하가 477% 이상 발생되면서 POD 이 자동으로 늘어나는 것을 볼 수 있습니다.\nEvery 2.0s: kubectl get hpa;kubectl get po Tue Apr 9 19:08:50 2019  NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 189%/50% 1 10 10 5m15s NAME READY STATUS RESTARTS AGE load-generator-557649ddcd-jq987 1/1 Running 0 5m2s php-apache-9bd5c887f-8dc4m 1/1 Running 0 57s php-apache-9bd5c887f-bdrlz 1/1 Running 0 73s php-apache-9bd5c887f-crrsq 1/1 Running 0 41s php-apache-9bd5c887f-fthkk 1/1 Running 0 57s php-apache-9bd5c887f-g46b8 1/1 Running 0 73s php-apache-9bd5c887f-hq88l 1/1 Running 0 57s php-apache-9bd5c887f-p6lrq 1/1 Running 0 6m4s php-apache-9bd5c887f-v5bhw 1/1 Running 0 57s php-apache-9bd5c887f-vs6sl 1/1 Running 0 42s php-apache-9bd5c887f-wjlrm 1/1 Running 0 HPA 가 POD 을 MAXPODS 수까지 생성하였습니다.\nEvery 2.0s: kubectl get hpa;kubectl get po Tue Apr 9 19:14:41 2019  NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 10 11m NAME READY STATUS RESTARTS AGE load-generator-557649ddcd-jq987 1/1 Running 0 10m php-apache-9bd5c887f-bdrlz 0/1 Terminating 0 7m4s php-apache-9bd5c887f-fthkk 0/1 Terminating 0 6m48s php-apache-9bd5c887f-g46b8 0/1 Terminating 0 7m4s php-apache-9bd5c887f-hq88l 0/1 Terminating 0 6m48s php-apache-9bd5c887f-p6lrq 1/1 Running 0 11m php-apache-9bd5c887f-vs6sl 0/1 Terminating 0 6m33s 부하가 중지되면 HPA는 다시 POD의 수를 줄입니다.\n참고 자료   https://kubernetes.io/ko/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/  ","permalink":"https://chhanz88.github.io/post/2019-04-09-kubernetes-hpa-example/","summary":"[Kubernetes] Kubernetes Horizontal Pod Autoscaler 테스트  Kubernetes 의 Horizontal Pod Autoscaler(이하 HPA) 를 테스트 해보겠습니다.\n부하 테스트 Image 생성  Dockerfile 을 생성합니다.\nFROMphp:5-apacheADD index.php /var/www/html/index.phpRUN chmod a+rx index.phpDocker image에 같이 추가 될 index.php 를 생성합니다.\n?php  $x = 0.0001;  for ($i = 0; $i \u0026lt;= 1000000; $i++) {  $x += sqrt($x);  }  echo \u0026#34;OK!\u0026#34;; ?\u0026gt;Image 를 Build 합니다.\n# docker build -t hpa-example .","title":"[Kubernetes] Kubernetes HPA 테스트"},{"content":"[GPFS] CentOS 7 - IBM GPFS Hands on Tranining 목표 Hands-on 구성도  Hands-on 환경   OS Version : CentOS 7.3\nKernel Version : 3.10.0-514.el7.x86_64\nGPFS Version : gpfs v4.2.2-3\nOS Configuration : SSH Key Copy 완료\n Hands-on Lab Cluster 구성\n[root@gpfs1 desc]# mmcrcluster -N node_desc --ccr-disable -p gpfs1 -s gpfs2 -r /usr/bin/ssh -R /usr/bin/scp -C gpfscluster mmcrcluster: Performing preliminary node verification ... mmcrcluster: Processing quorum and other critical nodes ... mmcrcluster: Finalizing the cluster data structures ... mmcrcluster: Command successfully completed mmcrcluster: Warning: Not all nodes have proper GPFS license designations. Use the mmchlicense command to designate licenses as needed. mmcrcluster: Propagating the cluster configuration data to all affected nodes. This is an asynchronous process. [root@gpfs1 desc]# [root@gpfs1 desc]# mmchlicense server --accept -N gpfs1,gpfs2 The following nodes will be designated as possessing server licenses: gpfs1 gpfs2 mmchlicense: Command successfully completed mmchlicense: Propagating the cluster configuration data to all affected nodes. This is an asynchronous process. [root@gpfs1 desc]# GPFS portability layer 생성\n[root@gpfs1 desc]# mmbuildgpl --build-package -------------------------------------------------------- mmbuildgpl: Building GPL module begins at Thu Apr 4 20:49:28 KST 2019. -------------------------------------------------------- Verifying Kernel Header... kernel version = 31000514 (3.10.0-514.el7.x86_64, 3.10.0-514) module include dir = /lib/modules/3.10.0-514.el7.x86_64/build/include module build dir = /lib/modules/3.10.0-514.el7.x86_64/build kernel source dir = /usr/src/linux-3.10.0-514.el7.x86_64/include Found valid kernel header file under /usr/src/kernels/3.10.0-514.el7.x86_64/include Verifying Compiler... make is present at /bin/make cpp is present at /bin/cpp gcc is present at /bin/gcc g++ is present at /bin/g++ ld is present at /bin/ld Verifying rpmbuild... Verifying Additional System Headers... Verifying kernel-headers is installed ... Command: /bin/rpm -q kernel-headers The required package kernel-headers is installed make World ... make InstallImages ... make rpm ... Wrote: /root/rpmbuild/RPMS/x86_64/gpfs.gplbin-3.10.0-514.el7.x86_64-4.2.2-3.x86_64.rpm -------------------------------------------------------- mmbuildgpl: Building GPL module completed successfully at Thu Apr 4 20:50:08 KST 2019. -------------------------------------------------------- [root@gpfs1 desc]# Node1 build 된 package 설치\n# yum install /root/rpmbuild/RPMS/x86_64/gpfs.gplbin-3.10.0-514.el7.x86_64-4.2.2-3.x86_64.rpm Loaded plugins: fastestmirror Examining /root/rpmbuild/RPMS/x86_64/gpfs.gplbin-3.10.0-514.el7.x86_64-4.2.2-3.x86_64.rpm: gpfs.gplbin-3.10.0-514.el7.x86_64-4.2.2-3.x86_64 Marking /root/rpmbuild/RPMS/x86_64/gpfs.gplbin-3.10.0-514.el7.x86_64-4.2.2-3.x86_64.rpm to be installed Resolving Dependencies --\u0026gt; Running transaction check ---\u0026gt; Package gpfs.gplbin-3.10.0-514.el7.x86_64.x86_64 0:4.2.2-3 will be installed --\u0026gt; Finished Dependency Resolution Dependencies Resolved =================================================================================================================================================================================================================== Package Arch Version Repository Size =================================================================================================================================================================================================================== Installing: gpfs.gplbin-3.10.0-514.el7.x86_64 x86_64 4.2.2-3 /gpfs.gplbin-3.10.0-514.el7.x86_64-4.2.2-3.x86_64 4.1 M Transaction Summary =================================================================================================================================================================================================================== Install 1 Package Total size: 4.1 M Installed size: 4.1 M Is this ok [y/d/N]: y Downloading packages: Running transaction check Running transaction test Transaction test succeeded Running transaction Installing : gpfs.gplbin-3.10.0-514.el7.x86_64-4.2.2-3.x86_64 1/1 Verifying : gpfs.gplbin-3.10.0-514.el7.x86_64-4.2.2-3.x86_64 1/1 Installed: gpfs.gplbin-3.10.0-514.el7.x86_64.x86_64 0:4.2.2-3 Complete! [root@gpfs1 desc]# Node2 또한 위와 같이 설치합니다.\n[root@gpfs1 ]# scp /root/rpmbuild/RPMS/x86_64/gpfs.gplbin-3.10.0-514.el7.x86_64-4.2.2-3.x86_64.rpm gpfs2:/root/ gpfs.gplbin-3.10.0-514.el7.x86_64-4.2.2-3.x86_64.rpm [root@gpfs2 ]# yum -y install /root/gpfs.gplbin-3.10.0-514.el7.x86_64-4.2.2-3.x86_64.rpm License enable\n[root@gpfs1 ~]# mmchlicense server --accept -N gpfs1,gpfs2 The following nodes will be designated as possessing server licenses: gpfs1 gpfs2 mmchlicense: Command successfully completed mmchlicense: Propagating the cluster configuration data to all affected nodes. This is an asynchronous process. [root@gpfs1 ~]# Cluster 구성 확인\n[root@gpfs1 desc]# mmlscluster GPFS cluster information ======================== GPFS cluster name: gpfscluster.gpfs1 GPFS cluster id: 12307248785610108094 GPFS UID domain: gpfscluster.gpfs1 Remote shell command: /usr/bin/ssh Remote file copy command: /usr/bin/scp Repository type: server-based GPFS cluster configuration servers: ----------------------------------- Primary server: gpfs1 Secondary server: gpfs2 Node Daemon node name IP address Admin node name Designation ---------------------------------------------------------------------- 1 gpfs1 192.168.13.200 gpfs1 quorum-manager 2 gpfs2 192.168.13.201 gpfs2 quorum-manager GPFS 서비스 시작\n[root@gpfs1 ~]# mmstartup -a Thu Apr 4 20:30:49 KST 2019: mmstartup: Starting GPFS ... [root@gpfs1 ~]# mmgetstate -a Node number Node name GPFS state ------------------------------------------ 1 gpfs1 active 2 gpfs2 active NSD Stanza File 작성\nStanza files Document : Link\n[root@gpfs1 desc]# cat disk.test.1 %nsd: device=vdb nsd=test1_nsd11 servers=gpfs1 FailureGroup=10 pool=system %nsd: device=vdb nsd=test1_nsd21 servers=gpfs2 FailureGroup=20 pool=system [root@gpfs1 desc]# cat disk.test.2 %nsd: device=vdc nsd=test2_nsd11 servers=gpfs1 FailureGroup=10 pool=system %nsd: device=vdc nsd=test2_nsd21 servers=gpfs2 FailureGroup=20 pool=system [root@gpfs1 desc]# NSD 생성\n# mmcrnsd -F disk.test.1 mmcrnsd: Processing disk vdb mmcrnsd: Processing disk vdb mmcrnsd: Propagating the cluster configuration data to all affected nodes. This is an asynchronous process. # mmcrnsd -F disk.test.2 mmcrnsd: Processing disk vdc mmcrnsd: Processing disk vdc mmcrnsd: Propagating the cluster configuration data to all affected nodes. This is an asynchronous process. NSD 생성 확인\n# mmlsnsd -X Disk name NSD volume ID Device Devtype Node name Remarks --------------------------------------------------------------------------------------------------- test1_nsd11 C0A80DC85CA5F091 /dev/vdb generic gpfs1 server node test1_nsd21 C0A80DC95CA5F093 /dev/vdb generic gpfs2 server node test2_nsd11 C0A80DC85CA5F0C5 /dev/vdc generic gpfs1 server node test2_nsd21 C0A80DC95CA5F0C6 /dev/vdc generic gpfs2 server node [root@gpfs1 desc]# GPFS 파일시스템 생성\n[root@gpfs1 desc]# mmcrfs gpfs.test1 -F disk.test.1 -A yes -B 256k -m 2 -M 2 -r 2 -R 2 -T /test1 The following disks of gpfs.2d1 will be formatted on node gpfs1: test1_nsd11: size 20480 MB test1_nsd21: size 20480 MB Formatting file system ... Disks up to size 192 GB can be added to storage pool system. Creating Inode File 5 % complete on Thu Apr 4 20:57:49 2019 10 % complete on Thu Apr 4 20:57:54 2019 16 % complete on Thu Apr 4 20:57:59 2019 21 % complete on Thu Apr 4 20:58:04 2019 26 % complete on Thu Apr 4 20:58:09 2019 31 % complete on Thu Apr 4 20:58:15 2019 36 % complete on Thu Apr 4 20:58:20 2019 41 % complete on Thu Apr 4 20:58:25 2019 46 % complete on Thu Apr 4 20:58:30 2019 51 % complete on Thu Apr 4 20:58:35 2019 56 % complete on Thu Apr 4 20:58:40 2019 62 % complete on Thu Apr 4 20:58:46 2019 67 % complete on Thu Apr 4 20:58:51 2019 72 % complete on Thu Apr 4 20:58:56 2019 77 % complete on Thu Apr 4 20:59:01 2019 83 % complete on Thu Apr 4 20:59:07 2019 88 % complete on Thu Apr 4 20:59:12 2019 93 % complete on Thu Apr 4 20:59:17 2019 98 % complete on Thu Apr 4 20:59:22 2019 100 % complete on Thu Apr 4 20:59:24 2019 Creating Allocation Maps Creating Log Files 3 % complete on Thu Apr 4 20:59:37 2019 28 % complete on Thu Apr 4 20:59:50 2019 53 % complete on Thu Apr 4 21:00:02 2019 78 % complete on Thu Apr 4 21:00:15 2019 100 % complete on Thu Apr 4 21:00:16 2019 Clearing Inode Allocation Map Clearing Block Allocation Map Formatting Allocation Map for storage pool system Completed creation of file system /dev/gpfs.test1. mmcrfs: Propagating the cluster configuration data to all affected nodes. This is an asynchronous process. [root@gpfs1 desc]# mmcrfs gpfs.test2 -F disk.test.2 -A yes -B 256k -m 2 -M 2 -r 2 -R 2 -T /test2 The following disks of gpfs.2d1 will be formatted on node gpfs1: test2_nsd11: size 20480 MB test2_nsd21: size 20480 MB Formatting file system ... Disks up to size 192 GB can be added to storage pool system. Creating Inode File 3 % complete on Thu Apr 4 21:00:58 2019 9 % complete on Thu Apr 4 21:01:03 2019 14 % complete on Thu Apr 4 21:01:09 2019 19 % complete on Thu Apr 4 21:01:14 2019 24 % complete on Thu Apr 4 21:01:19 2019 29 % complete on Thu Apr 4 21:01:25 2019 34 % complete on Thu Apr 4 21:01:30 2019 39 % complete on Thu Apr 4 21:01:36 2019 45 % complete on Thu Apr 4 21:01:41 2019 50 % complete on Thu Apr 4 21:01:47 2019 56 % complete on Thu Apr 4 21:01:52 2019 61 % complete on Thu Apr 4 21:01:58 2019 66 % complete on Thu Apr 4 21:02:03 2019 71 % complete on Thu Apr 4 21:02:08 2019 76 % complete on Thu Apr 4 21:02:13 2019 81 % complete on Thu Apr 4 21:02:18 2019 86 % complete on Thu Apr 4 21:02:23 2019 91 % complete on Thu Apr 4 21:02:28 2019 95 % complete on Thu Apr 4 21:02:33 2019 100 % complete on Thu Apr 4 21:02:37 2019 Creating Allocation Maps Creating Log Files 3 % complete on Thu Apr 4 21:02:50 2019 28 % complete on Thu Apr 4 21:03:03 2019 53 % complete on Thu Apr 4 21:03:16 2019 78 % complete on Thu Apr 4 21:03:28 2019 100 % complete on Thu Apr 4 21:03:29 2019 Clearing Inode Allocation Map Clearing Block Allocation Map Formatting Allocation Map for storage pool system Completed creation of file system /dev/gpfs.test2. mmcrfs: Propagating the cluster configuration data to all affected nodes. This is an asynchronous process. Fileset 생성\n[root@gpfs1 test1]# mmcrfileset gpfs.test1 dir1 --inode-space new --inode-limit 200000 Fileset dir1 created with id 1 root inode 131075. [root@gpfs1 test1]# mmlsfileset gpfs.test1 -L Filesets in file system \u0026#39;gpfs.test1\u0026#39;: Name Id RootInode ParentId Created InodeSpace MaxInodes AllocInodes Comment root 0 3 -- Thu Apr 4 21:00:16 2019 0 65792 65792 root fileset dir1 1 131075 -- Fri Apr 5 00:14:34 2019 1 200000 65920 [root@gpfs1 test1]# [root@gpfs1 test1]# mmdf gpfs.test1 disk disk size failure holds holds free KB free KB name in KB group metadata data in full blocks in fragments --------------- ------------- -------- -------- ----- -------------------- ------------------- Disks in storage pool: system (Maximum disk size allowed is 192 GB) test1_nsd11 20971520 10 Yes Yes 20235264 ( 96%) 824 ( 0%) test1_nsd21 20971520 20 Yes Yes 20235264 ( 96%) 824 ( 0%) ------------- -------------------- ------------------- (pool total) 41943040 40470528 ( 96%) 1648 ( 0%) ============= ==================== =================== (total) 41943040 40470528 ( 96%) 1648 ( 0%) Inode Information ----------------- Total number of used inodes in all Inode spaces: 4039 Total number of free inodes in all Inode spaces: 127673 Total number of allocated inodes in all Inode spaces: 131712 Total of Maximum number of inodes in all Inode spaces: 265792 [root@gpfs1 test1]# mmcrfileset gpfs.test1 dir2 --inode-space new --inode-limit 200000 Fileset dir2 created with id 2 root inode 262147. [root@gpfs1 test1]# mmlsfileset gpfs.test1 -L Filesets in file system \u0026#39;gpfs.test1\u0026#39;: Name Id RootInode ParentId Created InodeSpace MaxInodes AllocInodes Comment root 0 3 -- Thu Apr 4 21:00:16 2019 0 65792 65792 root fileset dir1 1 131075 -- Fri Apr 5 00:14:34 2019 1 200000 65920 dir2 2 262147 -- Fri Apr 5 00:21:25 2019 2 200000 65920 [root@gpfs1 test1]# Fileset Link 구성\n# mmlinkfileset gpfs.test1 dir1 -J /test1/dir1 Fileset Base linked at /test1/dir1 # mmlinkfileset gpfs.test1 dir2 -J /test2/dir2 Fileset Base linked at /test1/dir2 # mmlsfileset gpfs.test1 Filesets in file system \u0026#39;gpfs.test1\u0026#39;: Name Status Path root Linked /test dir1 Linked /test/dir1 dir2 Linked /test/dir2 참고 자료  IBM GPFS Document :\n https://www.ibm.com/support/knowledgecenter/STXKQY_5.0.0/com.ibm.spectrum.scale.v5r00.doc/bl1adm_stanzafiles.htm\nhttps://www.ibm.com/support/knowledgecenter/en/STXKQY_5.0.2/com.ibm.spectrum.scale.v5r02.doc/bl1adv_gpfsrep.htm\nhttps://www.ibm.com/support/knowledgecenter/en/STXKQY_4.2.1/com.ibm.spectrum.scale.v4r21.doc/bl1adv_filesets.htm\nhttps://www.ibm.com/support/knowledgecenter/en/STXKQY_5.0.2/com.ibm.spectrum.scale.v5r02.doc/bl1adm_mmrestripefs.htm\n ","permalink":"https://chhanz88.github.io/post/2019-04-05-linux-gpfs-hands-on-training/","summary":"[GPFS] CentOS 7 - IBM GPFS Hands on Tranining 목표 Hands-on 구성도  Hands-on 환경   OS Version : CentOS 7.3\nKernel Version : 3.10.0-514.el7.x86_64\nGPFS Version : gpfs v4.2.2-3\nOS Configuration : SSH Key Copy 완료\n Hands-on Lab Cluster 구성\n[root@gpfs1 desc]# mmcrcluster -N node_desc --ccr-disable -p gpfs1 -s gpfs2 -r /usr/bin/ssh -R /usr/bin/scp -C gpfscluster mmcrcluster: Performing preliminary node verification ... mmcrcluster: Processing quorum and other critical nodes .","title":"[GPFS] CentOS 7 - IBM GPFS Hands on Tranining"},{"content":"[Linux] CentOS 7 HP SSACLI Command 사용법  벤더(IBM,HP,Dell 등) 의 x86 하드웨어에서는 Linux 운영체제 상에서 하드웨어 레이드 컨트롤러의 상태를 확인 할 수 있는 도구들을 제공하고 있습니다.\nHP의 경우, Smart Storage Adminstrator 관리 명령어를 제공하고 있습니다.\n(예전에는 hpssacli, hpasucil 와 같은 명령어였습니다.)\n현재는 ssacli 로 변경 되었으며, Linux 운영체제상에서 하드웨어 RAID 구성, 상태 확인, 변경등이 가능하도록 지원하는 HP 에서 제공되는 관리 도구 입니다.\nSSACLI 설치  Download : HP Support Link\n# yum -y install ssacli-3.40-3.0.x86_64.rpm Loaded plugins: fastestmirror Examining ssacli-3.40-3.0.x86_64.rpm: ssacli-3.40-3.0.x86_64 Marking ssacli-3.40-3.0.x86_64.rpm to be installed Resolving Dependencies --\u0026gt; Running transaction check ---\u0026gt; Package ssacli.x86_64 0:3.40-3.0 will be installed --\u0026gt; Finished Dependency Resolution  Dependencies Resolved  ===================================================================================================================================================================================================================================================================================================  Package Arch Version Repository Size =================================================================================================================================================================================================================================================================================================== Installing:  ssacli x86_64 3.40-3.0 /ssacli-3.40-3.0.x86_64 39 M  Transaction Summary =================================================================================================================================================================================================================================================================================================== Install 1 Package  Total size: 39 M Installed size: 39 M Downloading packages: Running transaction check Running transaction test Transaction test succeeded Running transaction  Installing : ssacli-3.40-3.0.x86_64 1/1   Verifying : ssacli-3.40-3.0.x86_64 1/1  Installed:  ssacli.x86_64 0:3.40-3.0  Complete! SSACLI을 이용하여 하드웨어 확인  컨트롤러 상세 상태 확인 # ssacli ctrl all show detail  Smart Array P410i in Slot 0 (Embedded)  Bus Interface: PCI  Slot: 0  Serial Number: 50014380095F6F00  Cache Serial Number: PACCQID11492789  Controller Status: OK  Hardware Revision: C  Firmware Version: 6.40-0  Firmware Supports Online Firmware Activation: False  Rebuild Priority: Medium  Expand Priority: Medium  Surface Scan Delay: 15 secs  Surface Scan Mode: Idle  Parallel Surface Scan Supported: No  Queue Depth: Automatic  Monitor and Performance Delay: 60 min  Elevator Sort: Enabled  Degraded Performance Optimization: Disabled  Wait for Cache Room: Disabled  Surface Analysis Inconsistency Notification: Disabled  Post Prompt Timeout: 0 secs  Cache Board Present: True  Cache Status: OK  Cache Ratio: 100% Read / 0% Write  Drive Write Cache: Disabled  Total Cache Size: 0.2  Total Cache Memory Available: 0.1  No-Battery Write Cache: Disabled  SATA NCQ Supported: True  Number of Ports: 2 Internal only  Encryption: Not Set  Driver Name: hpsa  Driver Version: 3.4.18  Driver Supports SSD Smart Path: True  PCI Address (Domain:Bus:Device.Function): 0000:05:00.0  Port Max Phy Rate Limiting Supported: False  Host Serial Number: SGH033XJT1  Sanitize Erase Supported: False  Primary Boot Volume: None  Secondary Boot Volume: None 전체 Logical Drive 상태 확인 # ssacli ctrl slot=0 ld all show  Smart Array P410i in Slot 0 (Embedded)   Array A   logicaldrive 1 (279.37 GB, RAID 1, OK)   Array B   logicaldrive 2 (558.88 GB, RAID 0, OK)   Array C   logicaldrive 3 (558.88 GB, RAID 0, OK)   Array D   logicaldrive 4 (447.10 GB, RAID 0, Failed) 특정 Logical Drive 상태 확인 아래와 같이 특정 Lofical Drive 의 상태 및 Linux 에서 사용되는 Disk Label, Physical Drive 정보를 확인 할 수 있습니다.\n# ssacli ctrl slot=0 ld 1 show  Smart Array P410i in Slot 0 (Embedded)   Array A   Logical Drive: 1  Size: 279.37 GB  Fault Tolerance: 1  Heads: 255  Sectors Per Track: 32  Cylinders: 65535  Strip Size: 256 KB  Full Stripe Size: 256 KB  Status: OK  Unrecoverable Media Errors: None  Caching: Enabled  Unique Identifier: 600508B1001C5D5CE92263CB4F53A2B0  Disk Name: /dev/sda  Mount Points: /boot 1024 MB Partition Number 1  OS Status: LOCKED  Logical Drive Label: AB1AB56550014380095F6F00F968  Mirror Group 1:  physicaldrive 1I:1:1 (port 1I:box 1:bay 1, SAS HDD, 300 GB, OK)  Mirror Group 2:  physicaldrive 1I:1:2 (port 1I:box 1:bay 2, SAS HDD, 300 GB, OK)  Drive Type: Data  LD Acceleration Method: Controller Cache 전체 Physical Drive 상태 확인 # ssacli ctrl slot=0 pd all show  Smart Array P410i in Slot 0 (Embedded)   Array A   physicaldrive 1I:1:1 (port 1I:box 1:bay 1, SAS HDD, 300 GB, OK)  physicaldrive 1I:1:2 (port 1I:box 1:bay 2, SAS HDD, 300 GB, OK)   Array B   physicaldrive 1I:1:3 (port 1I:box 1:bay 3, SAS HDD, 600 GB, OK)   Array C   physicaldrive 1I:1:4 (port 1I:box 1:bay 4, SAS HDD, 600 GB, OK)   Array D   physicaldrive 2I:1:5 (port 2I:box 1:bay 5, SAS HDD, 146 GB, Failed) Physical Drive 상태 확인 아래와 같이 특정 Physical Drive의 상태를 확인 할 수 있습니다.\n테스트에 사용된 Disk는 장애가 발생되어 Failed 로 상태가 나오고 있습니다.\n# ssacli ctrl slot=0 pd 2I:1:5 show  Smart Array P410i in Slot 0 (Embedded)   Array D   physicaldrive 2I:1:5  Port: 2I  Box: 1  Bay: 5  Status: Failed  Last Failure Reason: Init drive type mix  Drive Type: Data Drive  Interface Type: SAS  Size: 146 GB  Drive exposed to OS: False  Logical/Physical Block Size: 512/512  Rotational Speed: 10000  Firmware Revision: HPDE  Serial Number: 6SD2S3R40000B132J1A9  WWID: 5000C50033CD8419  Model: HP EG0146FAWHU  PHY Count: 2  PHY Transfer Rate: 6.0Gbps, Unknown  Sanitize Erase Supported: False  Shingled Magnetic Recording Support: None 위와 같이 다양하게 하드웨어 RAID를 확인 할 수 있습니다.\n또한 Array(Logical Drive) 생성 및 삭제, 기타 컨트롤러 설정이 Linux 운영체제 안에서 설정이 가능합니다.\n# ssacli help  CLI Syntax  A typical SSACLI command line consists of three parts: a target device,  a command, and a parameter with values if necessary. Using angle brackets to  denote a required variable and plain brackets to denote an optional  variable, the structure of a typical SSACLI command line is as follows:   \u0026lt;target\u0026gt; \u0026lt;command\u0026gt; [parameter=value]   \u0026lt;target\u0026gt; is of format:  [controller all|slot=#|serialnumber=#]  [array all|\u0026lt;id\u0026gt;]  [physicaldrive all|allunassigned|[#:]#:#|[#:]#:#-[#:]#:#]  [ssdphysicaldrive all|allunassigned|[#:]#:#|[#:]#:#-[#:]#:#]  [logicaldrive all|#]  [enclosure all|#:#|serialnumber=#]  [licensekey all|\u0026lt;key\u0026gt;]  [ssdinfo]  [tapedrive all]  Note 1: The #:#:# syntax is only needed for systems that  specify port:box:bay. Other physical drive targeting  schemes are box:bay and port:id.   Example targets:  (\u0026#34;CN0\u0026#34; is a sample port name that may be different depending on the  controller)  controller slot=5  controller serialnumber=P21DA2322S  controller slot=7 array A  controller slot=5 logicaldrive 5  controller slot=5 physicaldrive 1:5  controller slot=5 physicaldrive CN0:2:3  controller slot=5 ssdphysicaldrive all  controller slot=5 tapedrive all  controller slot=5 enclosure CN0:1 show  controller slot=5 licensekey XXXXX-XXXXX-XXXXX-XXXXX-XXXXX  For detailed command information type any of the following:  help add  help create  help delete  help diag  help flash  help heal  help modify  help remove  help shorthand  help show  help target  help rescan  help version  \u0026lt;중략\u0026gt; 참고 자료  HP SSA Document : https://support.hpe.com/hpsc/doc/public/display?docId=c03909334\nHP Support Page : https://support.hpe.com/hpesc/public/home\n추가로 기타 다른 벤더에서도 제공하는 도구는 아래와 같습니다.\n사용 방법도 위와 비슷한 환경으로 되어 있습니다.\nIBM, Lenovo : https://support.lenovo.com/kr/ko/downloads/ds031558\nDell/EMC : https://www.dell.com/support/article/kr/ko/krdhs1/sln283135/how-to-use-the-poweredge-raid-controller-perc-command-line-interface-cli-utility-to-manage-your-raid-controller?lang=en\n","permalink":"https://chhanz88.github.io/post/2019-04-01-linux--use-ssacil/","summary":"[Linux] CentOS 7 HP SSACLI Command 사용법  벤더(IBM,HP,Dell 등) 의 x86 하드웨어에서는 Linux 운영체제 상에서 하드웨어 레이드 컨트롤러의 상태를 확인 할 수 있는 도구들을 제공하고 있습니다.\nHP의 경우, Smart Storage Adminstrator 관리 명령어를 제공하고 있습니다.\n(예전에는 hpssacli, hpasucil 와 같은 명령어였습니다.)\n현재는 ssacli 로 변경 되었으며, Linux 운영체제상에서 하드웨어 RAID 구성, 상태 확인, 변경등이 가능하도록 지원하는 HP 에서 제공되는 관리 도구 입니다.\nSSACLI 설치  Download : HP Support Link","title":"[Linux] CentOS 7 HP SSACLI Command 사용법"},{"content":"[Linux] CentOS 7 raw device 자동 생성 스크립트  고객사에서 Multipath 로 생성된 mpath device 를 raw device 로 생성하는 작업이 있었습니다.\n너무 많은 mpath device 를 raw device 로 생성을 하다보니, 오타도 발생되고 작업 환경을 콘솔에서 하다보니 불편함도 있다보니 해당 스크립트를 만들었습니다.\nDownload Link  GitHubRaw Script 사용 방법 Multipath Device 의 mpath 를 기준으로 해당 DM_UUID 값을 생성하여 60-raw.rules.$DATE 파일로 생성합니다.\n Multipath List 확인  # multipath -ll | grep mpath mpathb (3600c0ff00011e91abe3a475901000000) dm-3 HP ,P2000 G3 FC mpatha (3600c0ff00011e91a3e3b475901000000) dm-2 HP ,P2000 G3 FC 스크립트 Download  # wget https://raw.githubusercontent.com/chhanz/raw-device-generator/master/raw_generator_v1.sh --2019-03-31 20:46:20-- https://raw.githubusercontent.com/chhanz/raw-device-generator/master/raw_generator_v1.sh Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2491 (2.4K) [text/plain] Saving to: ‘raw_generator_v1.sh’ 100%[======================================================================================================================================================================================================================================\u0026gt;] 2,491 --.-K/s in 0s 2019-03-31 20:46:21 (57.6 MB/s) - ‘raw_generator_v1.sh’ saved [2491/2491] 스크립트 구동  # chmod u+x raw_generator_v1.sh 생성된 File 확인  # ls -la total 12 drwxr-xr-x 2 root root 65 Mar 31 20:49 . dr-xr-x---. 16 root root 4096 Mar 31 20:49 .. -rw-r--r-- 1 root root 349 Mar 31 20:49 60-raw.rules.190331_2049 -rwxr--r-- 1 root root 2493 Mar 31 20:49 raw_generator_v1.sh # cat 60-raw.rules.190331_2049 # RawGenerator - 60-raw.rules # Make . chhanz ACTION==\u0026#34;add|change\u0026#34;,ENV{DM_UUID}==\u0026#34;mpath-3600c0ff00011e91a3e3b475901000000\u0026#34;,RUN+=\u0026#34;/usr/bin/raw /dev/raw/raw1 %N\u0026#34; ACTION==\u0026#34;add|change\u0026#34;,ENV{DM_UUID}==\u0026#34;mpath-3600c0ff00011e91abe3a475901000000\u0026#34;,RUN+=\u0026#34;/usr/bin/raw /dev/raw/raw2 %N\u0026#34; ACTION==\u0026#34;add\u0026#34;, KERNEL==\u0026#34;raw*\u0026#34;, OWNER=\u0026#34;grid\u0026#34;, GROUP=\u0026#34;dba\u0026#34;, MODE=\u0026#34;0660\u0026#34; 위와 같이 생성된 파일을 이용하여 /etc/udev/rules.d/ 에 넣고 raw device 생성을 합니다.\n상세 raw device 생성 방법은 이전 Posting 확인합니다.\n참고 자료  GITHUB : https://github.com/chhanz/raw-device-generator\nRaw Device 생성 방법 : https://chhanz.github.io/linux/2018/10/01/linux_rawdevice/\n","permalink":"https://chhanz88.github.io/post/2019-03-31-linux-raw-device-generator-script/","summary":"[Linux] CentOS 7 raw device 자동 생성 스크립트  고객사에서 Multipath 로 생성된 mpath device 를 raw device 로 생성하는 작업이 있었습니다.\n너무 많은 mpath device 를 raw device 로 생성을 하다보니, 오타도 발생되고 작업 환경을 콘솔에서 하다보니 불편함도 있다보니 해당 스크립트를 만들었습니다.\nDownload Link  GitHubRaw Script 사용 방법 Multipath Device 의 mpath 를 기준으로 해당 DM_UUID 값을 생성하여 60-raw.rules.$DATE 파일로 생성합니다.\n Multipath List 확인  # multipath -ll | grep mpath mpathb (3600c0ff00011e91abe3a475901000000) dm-3 HP ,P2000 G3 FC mpatha (3600c0ff00011e91a3e3b475901000000) dm-2 HP ,P2000 G3 FC 스크립트 Download  # wget https://raw.","title":"[Linux] CentOS 7 raw device 자동 생성 스크립트"},{"content":"  회사 기술블로그에 작성한 내용입니다.  오픈소스컨설팅 기술블로그  Author. chhanz \n   안녕하세요? 오픈소스컨설팅 한철희 과장입니다.    이전 \"  Docker 이해하기  \" 를 포스팅에 이어, \"  Docker Swarm 을 이용한 Container Orchestration 환경 만들기  \" 라는 포스팅을 작성하게 되었습니다.   (Review - Docker 이해하기   )    이전 포스팅에서 Docker 를 직접 사용하면서 여러 장점을 확인했습니다.  하지만 과연 실무에 적용하면 안정적으로 서비스를 유지하고 운영할 수 있을지에 대해서는 의문을 가지고 있었습니다.  이러한 의문은 Container 들을 자동으로 관리하게 해주는  Container   Orchestration Tool  을 활용함으로써 해결을 하게 되었습니다.    Container Orchestration       컨테이너 오케스트레이션 이란? 다중 컨테이너 패키지 어플리케이션을 배포하는 동안 사용되는 컨테이너, 리소스의 자동화, 정렬, 조정 및 관리를 하는 것을 말합니다.  위와 같이 많은 오케스트레이션 도구들이 있습니다. 이번 포스팅에서는 쉽게 구성이 가능한 Docker Swarm 을 통해 컨테이너 오케스트레이션을 맛보려고 합니다.   Docker Swarm        Docker Swarm 이란?  수많은 컨테이너 오케스트레이션 도구 중의 하나로, 여러 대의 Docker 호스트들을 마치 하나인 것처럼 만들어주는 Orchestration 도구입니다.  Docker v1.12 이후부터 Docker Swarm Mode 로 별개의 Docker Swarm 엔진에서 Docker 엔진으로 통합되면서 좀 더 간편한 설치가 가능해졌습니다.    쉬워진 Docker Swarm 직접 설치 해보도록 하겠습니다.   Docker Swarm 설치      (Docker Document, How nodes work - https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/)   위와 같이 기본적으로 Docker Swarm 은 Master 노드와 Worker 노드로 시스템을 구성합니다.  Master 노드에서는 클러스터 관리 작업을 하고 클러스터 상태 유지, 스케줄링 서비스, Swarm HTTP API Endpoint 를 제공합니다.  Worker 노드는 컨테이너를 실행하는 역할만 합니다.    이번 Master 노드를 Three-Manager 구성으로 하여 컨테이너 오케스트레이션 및 Docker Swarm 안정성에 대해 확인해보도록 하겠습니다.    System Environment    OS : CentOS Linux release 7.6.1810 (Core)  Docker Version : docker-1.13.1-88.git07f3374.el7.centos.x86_64      위와 같이 시스템 환경으로 Three-Manager 구성을 하도록 하겠습니다.  Docker Swarm 이 Docker 엔진과 통합되면서 설치는 일반적인 Docker 설치와 동일해졌습니다.  설치는 아래와 같습니다.    # yum -y install docker     [root@manager1 ~]# yum -y install docker \u0026lt;중략\u0026gt; Installed: docker.x86_64 2:1.13.1-88.git07f3374.el7.centos Dependency Installed: PyYAML.x86_64 0:3.10-11.el7 atomic-registries.x86_64 1:1.22.1-26.gitb507039.el7.centos audit-libs-python.x86_64 0:2.8.4-4.el7 checkpolicy.x86_64 0:2.5-8.el7 container-selinux.noarch 2:2.74-1.el7 container-storage-setup.noarch 0:0.11.0-2.git5eaf76c.el7 containers-common.x86_64 1:0.1.31-7.gitb0b750d.el7.centos docker-client.x86_64 2:1.13.1-88.git07f3374.el7.centos docker-common.x86_64 2:1.13.1-88.git07f3374.el7.centos libcgroup.x86_64 0:0.41-20.el7 libseccomp.x86_64 0:2.3.1-3.el7 libsemanage-python.x86_64 0:2.5-14.el7 libyaml.x86_64 0:0.1.4-11.el7_0 oci-register-machine.x86_64 1:0-6.git2b44233.el7 oci-systemd-hook.x86_64 1:0.1.18-2.git3efe246.el7 oci-umount.x86_64 2:2.3.4-2.git87f9237.el7 policycoreutils-python.x86_64 0:2.5-29.el7_6.1 python-IPy.noarch 0:0.75-6.el7 python-backports.x86_64 0:1.0-8.el7 python-backports-ssl_match_hostname.noarch 0:3.5.0.1-1.el7 python-ipaddress.noarch 0:1.0.16-2.el7 python-pytoml.noarch 0:0.1.14-1.git7dea353.el7 python-setuptools.noarch 0:0.9.8-7.el7 setools-libs.x86_64 0:3.3.8-4.el7 subscription-manager-rhsm-certificates.x86_64 0:1.21.10-3.el7.centos yajl.x86_64 0:2.0.4-4.el7 Dependency Updated: policycoreutils.x86_64 0:2.5-29.el7_6.1 Complete! [root@manager1 ~]# systemctl enable docker Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service. [root@manager1 ~]# systemctl start docker     위와 같이 모든 Master 노드에 동일하게 설치를 합니다.   [root@manager1 ~]# systemctl enable docker Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service. [root@manager1 ~]# systemctl start docker      # systemctl enable docker      # systemctl start docker    각 노드 별로 Docker 서비스를 시작합니다.   [root@manager1 ~]# systemctl status docker ● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled) Active: active (running) since 월 2019-02-11 14:14:37 KST; 2min 54s ago Docs: http://docs.docker.com Main PID: 14694 (dockerd-current) CGroup: /system.slice/docker.service ├─14694 /usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=docker-runc --exec-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --init-path=/usr/libexec/docker/docker-init-cu... └─14701 /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc --runtime-args --systemd-c...       # systemctl status docker    설치가 완료되면 Docker 서비스를 시작하고 위와 같이 정상적으로 시작되었는지 확인합니다.   Docker Swarm init    Docker Swarm 를 구성하기 위해 아래와 같이 명령 수행을 합니다.    # docker swarm init --advertise-addr [Manager Node IP]     [root@manager1 ~]# docker swarm init --advertise-addr 192.168.13.176 Swarm initialized: current node (y8ul9r3jq0rgt9k3vbvrayeyg) is now a manager. To add a worker to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-2m3tqsm8ly45vpd5i80p4bkor5zaohfmultu4cdnvfpg8yxmuk-bv8adgschaygmg9icehekb9wg \\ 192.168.13.176:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.      위와 같이 초기화를 진행하면 Worker 노드를 추가하는 token 값으로 명령어가 자동 생성됩니다.  해당 명령을 Worker 노드에 입력하면 해당 노드는 Worker 노드가 됩니다.  Master 노드(Manager 노드)를 추가하기 위해서는    # docker swarm join-token manager    명령을 통해 명령어를 생성해야됩니다.   # docker swarm join-token manager To add a manager to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-2m3tqsm8ly45vpd5i80p4bkor5zaohfmultu4cdnvfpg8yxmuk-9ghru6puwdvqms3bn7zqtiyvt \\ 192.168.13.176:2377      Manager 연결을 위해 생성된 명령을 나머지 Manager 노드에 아래와 같이 입력합니다. [root@manager2 ~]# docker swarm join \\ \u0026gt; --token SWMTKN-1-2m3tqsm8ly45vpd5i80p4bkor5zaohfmultu4cdnvfpg8yxmuk-9ghru6puwdvqms3bn7zqtiyvt \\ \u0026gt; 192.168.13.176:2377 This node joined a swarm as a manager. [root@manager3 ~]# docker swarm join \\ \u0026gt; --token SWMTKN-1-2m3tqsm8ly45vpd5i80p4bkor5zaohfmultu4cdnvfpg8yxmuk-9ghru6puwdvqms3bn7zqtiyvt \\ \u0026gt; 192.168.13.176:2377 This node joined a swarm as a manager.      Docker Swarm 구성 확인   [root@manager1 ~]# docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS lrt89xwkugty162qk8c2av5ek manager2.example.com Ready Active Reachable y8ul9r3jq0rgt9k3vbvrayeyg * manager1.example.com Ready Active Leader yqerq5ujds38t0izzlp03dbhd manager3.example.com Ready Active Reachable       # docker node ls    위와 같이 manager 로 노드들이 연결된 것을 확인 할 수 있습니다.   Docker Swarm 기본 사용법    Docker Swarm 에서 사용되는 기본적인 명령을 간단한 Apache 서비스를 기동하면서 확인 해보겠습니다.   Docker Swarm Service 생성    Docker Swarm 명령어는 기존의 docker 명령어와 크게 다른 점은 없습니다.  기본적으로 docker run 에서 사용되는 옵션을 그대로 사용 할 수 있습니다.      아래 명령을 통해 Docker Swarm Mode 로 컨테이너를 실행 할 수 있습니다.       Usage: docker service create [OPTIONS] IMAGE [COMMAND] [ARG...]   [root@manager1 ~]# docker service create --name web httpd xocc6zwdulliijqpypwby764d     Docker Swarm Service 확인    생성된 서비스가 정상적으로 실행이 되었는지는 아래 명령으로 확인이 가능합니다.    Usage: docker service ls   [root@manager1 ~]# docker service ls ID NAME MODE REPLICAS IMAGE xocc6zwdulli web replicated 0/1 httpd:latest [root@manager1 ~]# docker service ls ID NAME MODE REPLICAS IMAGE xocc6zwdulli web replicated 1/1 httpd:latest    처음 REPLICAS 필드가 0/1 로 시작해서 1/1 로 변경이 되면 컨테이너가 정상적으로 실행이 된 것으로 확인 할 수 있습니다.  해당 필드를 통해 컨테이너가 문제가 생겼는지 정상 작동 중인지 확인이 가능합니다.    또한 각 서비스 별로 자세한 정보를 확인하는 명령은 아래와 같습니다.    Usage: docker service ps [SERVICE]   [root@manager1 ~]# docker service ps web ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 9x3qvcl5seif web.1 httpd:latest manager2.example.com Running Running 2 minutes ago   Docker Swarm Service Scale-out    생성된 서비스를 복제하여 분산 서비스를 할 수 있도록 합니다.(Scale-out)    Usage: docker service scale SERVICE=REPLICAS [SERVICE=REPLICAS...]   [root@manager1 ~]# docker service scale web=3 web scaled to 3 [root@manager1 ~]# docker service ls ID NAME MODE REPLICAS IMAGE xocc6zwdulli web replicated 1/3 httpd:latest [root@manager1 ~]# docker service ls ID NAME MODE REPLICAS IMAGE xocc6zwdulli web replicated 3/3 httpd:latest [root@manager1 ~]# for i in $(cat /etc/hosts | grep manager| awk '{print $1}') \u0026gt; do \u0026gt; ssh root@$i \"docker ps -a\" \u0026gt; done CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e517f6af9ebd httpd@sha256:d12c036427f436978f2d4397ad2bd6b5b8f7b03003b7a1da084eb228ef25b7d2 \"httpd-foreground\" 5 minutes ago Up 5 minutes 80/tcp web.2.57p0vzbkqymvak5t5rw6g42k9 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 91dddedf7374 httpd@sha256:d12c036427f436978f2d4397ad2bd6b5b8f7b03003b7a1da084eb228ef25b7d2 \"httpd-foreground\" 9 minutes ago Up 9 minutes 80/tcp web.1.9x3qvcl5seifoidt5jy4fm3oa CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2a3ade0887f9 httpd@sha256:d12c036427f436978f2d4397ad2bd6b5b8f7b03003b7a1da084eb228ef25b7d2 \"httpd-foreground\" 5 minutes ago Up 5 minutes 80/tcp web.3.aqlqfena08g9c50tdl5vgb4ju [root@manager1 ~]# docker service ps web ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 9x3qvcl5seif web.1 httpd:latest manager2.example.com Running Running 9 minutes ago 57p0vzbkqymv web.2 httpd:latest manager1.example.com Running Running 6 minutes ago aqlqfena08g9 web.3 httpd:latest manager3.example.com Running Running 6 minutes ago    Docker Swarm Service 제거    생성한 서비스의 제거 및 종료는 아래 명령을 통해 가능합니다.    Usage: docker service rm SERVICE [SERVICE...]   [root@manager1 ~]# docker service rm web web [root@manager1 ~]# docker service ls ID NAME MODE REPLICAS IMAGE [root@manager1 ~]# docker service ps web Error: No such service: web       지금까지 Docker Swarm 에서 많이 사용되는 명령어들을 보면서 간단한 웹 서비스를 구성하였습니다.  이제부터는 Build 된 PHP Docker Image를 이용해서 Docker Swarm 을 어떻게 실무에 적용 할 수 있는지 확인해 보도록 하겠습니다.   PHP Demo   Docker Image 관리를 위한 사설 Registry 생성    각 Docker Host 노드에 같은 이미지를 배포 하기 위해서는 두가지 방법이 있습니다.  그 방법은 DockerHub 를 활용하는 방법 및 사설 Registry 를 만들어서 사용하는 방법입니다.  어떤 방법을 사용하는 것이 좋을지는 운영 환경에 맞게 선택을 하는 것이 좋습니다.    이번 PHP Demo 에서는 사설 Registry 를 생성하여 Docker Image 를 관리하도록 하겠습니다.  먼저 Docker Registry 를 생성하기 전에 insecure-registries 옵션을 설정하여 인증되지 않은 Registry를 사용 할 수 있도록 해야됩니다.  이미지를 사용해야되는 모든 노드를 아래와 같이 수정을 합니다. 이후 docker 서비스를 재시작합니다. # vi /etc/docker/daemon.json { \"insecure-registries\" : [\"manager1.example.com:5000\"] } # systemctl restart docker   위와 같이 manager1.example.com 만 선택을 하였는데 Load Balancer 가 있다면 해당 IP 혹은 Domain 명을 추가해도 됩니다.    사설 Registry 를 생성하기 위해서는 아래와 같이 진행합니다.    # docker service create --name registry -p 5000:5000 registry   # docker service create --name registry -p 5000:5000 registry z3gl3pie7xm9vjfyetot9zi3q [root@manager1 ~]# docker service ls ID NAME MODE REPLICAS IMAGE z3gl3pie7xm9 registry replicated 1/1 registry:latest [root@manager1 dockerfile]# docker service ps registry ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ct6zhchfla8s registry.1 registry:latest manager3.example.com Running Running 6 minutes ago     위와 같이 현재 manager3.example.com Docker Host 에 컨테이너가 실행중인 것을 확인 할 수 있습니다.   Build The Docker Image    아래와 같이 Dockerfile을 생성하여 PHP 서비스가 가능한 Docker Image 를 Build 합니다. \u0026gt; 참고로 아래 사용된 예제 파일은 github 에서 Clone 할 수 있습니다.  \u0026gt;   https://github.com/chhanz/docker-swarm-demo      PHP Demo 에 사용되는 파일 구성 및 내용은 아래와 같습니다. [root@manager1 docker-swarm-demo]# tree . ├── Dockerfile ├── README.md └── htdocs └── index.php 1 directory, 3 files # vi Dockerfile FROM php:7.2-apache MAINTAINER chhanz \u0026lt;chhan@osci.kr\u0026gt; ADD htdocs/index.php /var/www/html/index.php EXPOSE 80 [root@manager1 docker-swarm-demo]# cat htdocs/index.php \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt; \u0026lt;b\u0026gt; \u0026lt;?php $host=gethostname(); echo \"Container Name : \"; echo $host; ?\u0026gt; \u0026lt;p\u0026gt; Image Version : orignal\u0026lt;/p\u0026gt; \u0026lt;/b\u0026gt; \u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;    제가 포스팅한 내용을 보신 분이라면 Docker Build는 어렵지 않습니다. ㅎㅎ  Image 를 Build 합니다.    # docker build -t phpdemo:v1 .   [root@manager1 docker-swarm-demo]# docker build -t phpdemo:v1 . Sending build context to Docker daemon 4.608 kB Step 1/4 : FROM php:7.2-apache ---\u0026gt; 2424d6c5e6b9 Step 2/4 : MAINTAINER chhanz \u0026lt;chhan@osci.kr\u0026gt; ---\u0026gt; Running in 1257b21144c7 ---\u0026gt; 2beeadfdb912 Removing intermediate container 1257b21144c7 Step 3/4 : ADD htdocs/index.php /var/www/html/index.php ---\u0026gt; 62a4d63e0c2f Removing intermediate container 06a5fe09c5c5 Step 4/4 : EXPOSE 80 ---\u0026gt; Running in 310137303fa5 ---\u0026gt; c62e0ad19807 Removing intermediate container 310137303fa5 Successfully built c62e0ad19807 [root@manager1 docker-swarm-demo]#    Build 가 완료된 Image 를 사설 Registry 에 Push 합니다.    // Docker Image Tag 변경     # docker tag phpdemo:v1 manager1.example.com:5000/phpdemo:v1      // Docker Image Push     # docker push manager1.example.com:5000/phpdemo:v1     [root@manager1 docker-swarm-demo]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE phpdemo v1 c62e0ad19807 18 seconds ago 378 MB docker.io/php 7.2-apache 2424d6c5e6b9 3 days ago 378 MB docker.io/registry \u0026lt;none\u0026gt; d0eed8dad114 12 days ago 25.8 MB [root@manager1 docker-swarm-demo]# docker tag phpdemo:v1 manager1.example.com:5000/phpdemo:v1 [root@manager1 docker-swarm-demo]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE manager1.example.com:5000/phpdemo v1 c62e0ad19807 30 seconds ago 378 MB phpdemo v1 c62e0ad19807 30 seconds ago 378 MB docker.io/php 7.2-apache 2424d6c5e6b9 3 days ago 378 MB docker.io/registry \u0026lt;none\u0026gt; d0eed8dad114 12 days ago 25.8 MB [root@manager1 docker-swarm-demo]# docker push manager1.example.com:5000/phpdemo:v1 The push refers to a repository [manager1.example.com:5000/phpdemo] a0df0b1bee34: Pushed 29f6f251b4d2: Pushed 28255a6692d8: Pushed d9b14cb17d8b: Pushed 725c91d33681: Pushed 005a87a63ac9: Pushed 66fd43b3ea3b: Pushed 20d941ba3638: Pushed eb3e3e0ec224: Pushed 3843f6b0eab9: Pushed 63fc1837f67c: Pushed c68025fbc229: Pushed ec6f4f0a90dc: Pushed 0a07e81f5da3: Pushed v1: digest: sha256:58b33a5f39d60a3f0ba860a1bcbc98f5f767d934c9b1c057cee4b8c1a192fd06 size: 3242 [root@manager1 docker-swarm-demo]#     서비스 배포!    생성된 따끈한 Image 를 이용해서 Docker Swarm Mode 로 서비스를 배포하도록 하겠습니다.    # docker service create --name phpdemo -p 80:80 manager1.example.com:5000/phpdemo:v1   [root@manager1 docker-swarm-demo]# docker service create --name phpdemo -p 80:80 manager1.example.com:5000/phpdemo:v1 mcz67fbul4gmxtjtwc4dvf4n2 // 서비스 시작 중 [root@manager1 docker-swarm-demo]# docker service ls ID NAME MODE REPLICAS IMAGE mcz67fbul4gm phpdemo replicated 0/1 manager1.example.com:5000/phpdemo:v1 z3gl3pie7xm9 registry replicated 1/1 registry:latest [root@manager1 docker-swarm-demo]# docker service ps phpdemo ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS m82bq5rgmk7z phpdemo.1 manager1.example.com:5000/phpdemo:v1 manager2.example.com Running Preparing 7 seconds ago // 서비스 시작 완료 [root@manager1 docker-swarm-demo]# docker service ls ID NAME MODE REPLICAS IMAGE mcz67fbul4gm phpdemo replicated 1/1 manager1.example.com:5000/phpdemo:v1 z3gl3pie7xm9 registry replicated 1/1 registry:latest [root@manager1 docker-swarm-demo]# docker service ps phpdemo ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS m82bq5rgmk7z phpdemo.1 manager1.example.com:5000/phpdemo:v1 manager2.example.com Running Running 2 seconds ago         직접 웹브라우져를 통해 접속해보니 서비스가 정상적으로 작동 되는 것을 확인 할 수 있었습니다.   서비스 복제!    컨터이너 한개로 서비스를 하기에는 안정성이 너무나도 떨어지고 성능 향상을 위해 컨테이너를 복제합니다.    # docker service scale phpdemo=3   [root@manager1 docker-swarm-demo]# docker service ls ID NAME MODE REPLICAS IMAGE mcz67fbul4gm phpdemo replicated 1/1 manager1.example.com:5000/phpdemo:v1 z3gl3pie7xm9 registry replicated 1/1 registry:latest // 서비스 복제 [root@manager1 docker-swarm-demo]# docker service scale phpdemo=3 phpdemo scaled to 3 [root@manager1 docker-swarm-demo]# docker service ls ID NAME MODE REPLICAS IMAGE mcz67fbul4gm phpdemo replicated 3/3 manager1.example.com:5000/phpdemo:v1 z3gl3pie7xm9 registry replicated 1/1 registry:latest [root@manager1 docker-swarm-demo]# [root@manager1 docker-swarm-demo]# docker service ps phpdemo ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS m82bq5rgmk7z phpdemo.1 manager1.example.com:5000/phpdemo:v1 manager2.example.com Running Running 2 minutes ago 09p9qrtxidnw phpdemo.2 manager1.example.com:5000/phpdemo:v1 manager3.example.com Running Running less than a second ago 8x0pcftmzbzw phpdemo.3 manager1.example.com:5000/phpdemo:v1 manager1.example.com Running Running 11 seconds ago [root@manager1 docker-swarm-demo]#    위와 같이 서비스가 복제가 된 것을 확인 할 수 있습니다.    그럼 실제로 어떻게 서비스가 운영되는지 확인해보겠습니다.     3 Replica 서비스     (상기 GIF 파일은 용량이 커서, 출력이 느릴수도 있습니다.)  보시는 것과 같이 각기 다른 컨테이너로 Load Balancing 되는 것을 확인 할 수 있습니다.    위와 같이 Docker Swarm 이 각기 다른 Docker Host를 Load Balancing 를 하는 이유는 아래와 같습니다.    무언가 엄청 복잡해 보이지만 결국은 Ingress Network 를 통해 지정된 포트의 통신은 해당 컨테이너로 자동으로 전달이 될 것입니다.  자세한 Network Architecture 는 아래 Docker Document 를 확인하는 것이 좋습니다.  ( https://success.docker.com/article/networking  )   서비스 Rolling Update    서비스를 운영하다보면 업데이트가 필요로한 시기가 있습니다.  하지만 운영중에 서비스를 중지하고 업데이트를 하는 것은 서비스 DownTime 이 발생하게 되고 그만큼 운영에 힘들게 됩니다.    우리 Docker Swarm 과 함께라면 운영중에 서비스를 업데이트가 가능합니다!!!    먼저 기존에 만들어진 Docker Image 를 업데이트하도록 하겠습니다. [root@manager1 docker-swarm-demo]# cat htdocs/index.php \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt; \u0026lt;b\u0026gt; \u0026lt;?php $host=gethostname(); echo \"Container Name : \"; echo $host; ?\u0026gt; \u0026lt;p\u0026gt; Image Version : Update Version v2\u0026lt;/p\u0026gt; \u0026lt;/b\u0026gt; \u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;    핵심 파일인 Index.php 를 수정을 합니다.   [root@manager1 docker-swarm-demo]# docker build -t phpdemo:v2 . Sending build context to Docker daemon 4.608 kB Step 1/4 : FROM php:7.2-apache ---\u0026gt; 2424d6c5e6b9 Step 2/4 : MAINTAINER chhanz \u0026lt;chhan@osci.kr\u0026gt; ---\u0026gt; Using cache ---\u0026gt; 2beeadfdb912 Step 3/4 : ADD htdocs/index.php /var/www/html/index.php ---\u0026gt; 723bb4020994 Removing intermediate container ef8133b39a77 Step 4/4 : EXPOSE 80 ---\u0026gt; Running in 14a08f850b38 ---\u0026gt; 99574ad1473c Removing intermediate container 14a08f850b38 Successfully built 99574ad1473c [root@manager1 docker-swarm-demo]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE phpdemo v2 99574ad1473c 5 seconds ago 378 MB manager1.example.com:5000/phpdemo v1 c62e0ad19807 16 minutes ago 378 MB phpdemo v1 c62e0ad19807 16 minutes ago 378 MB docker.io/php 7.2-apache 2424d6c5e6b9 3 days ago 378 MB docker.io/registry \u0026lt;none\u0026gt; d0eed8dad114 12 days ago 25.8 MB [root@manager1 docker-swarm-demo]#     phpdemo:v2 로 Tag 를 지정하고 신규로 생성된 Image 를 Push 합니다. [root@manager1 docker-swarm-demo]# docker tag phpdemo:v2 manager1.example.com:5000/phpdemo:v2 [root@manager1 docker-swarm-demo]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE manager1.example.com:5000/phpdemo v2 99574ad1473c About a minute ago 378 MB phpdemo v1 c62e0ad19807 17 minutes ago 378 MB phpdemo v2 99574ad1473c About a minute ago 378 MB manager1.example.com:5000/phpdemo v1 c62e0ad19807 18 minutes ago 378 MB docker.io/php 7.2-apache 2424d6c5e6b9 3 days ago 378 MB docker.io/registry \u0026lt;none\u0026gt; d0eed8dad114 12 days ago 25.8 MB [root@manager1 docker-swarm-demo]# [root@manager1 docker-swarm-demo]# docker push manager1.example.com:5000/phpdemo:v2 The push refers to a repository [manager1.example.com:5000/phpdemo] b246f39fc10a: Pushed 29f6f251b4d2: Layer already exists 28255a6692d8: Layer already exists d9b14cb17d8b: Layer already exists 725c91d33681: Layer already exists 005a87a63ac9: Layer already exists 66fd43b3ea3b: Layer already exists 20d941ba3638: Layer already exists eb3e3e0ec224: Layer already exists 3843f6b0eab9: Layer already exists 63fc1837f67c: Layer already exists c68025fbc229: Layer already exists ec6f4f0a90dc: Layer already exists 0a07e81f5da3: Layer already exists v2: digest: sha256:1542620ce99456e9cc6b8e55998f08707e68d0f7aa8c84a17457e20fd5623caa size: 3242 [root@manager1 docker-swarm-demo]#     그럼 본격적으로 서비스를 운영하면서 변경된 Image 로 서비스 Rolling Update 를 하겠습니다.  방법은 아래와 같습니다.    # docker service update --update-parallelism 1 --image manager1.example.com:5000/phpdemo:v2 phpdemo    --update-parallelism 옵션은 컨테이너 이미지가 한번에 얼마나 변경될지 결정합니다. 0일 경우, 한번에 변경합니다.    [root@manager1 docker-swarm-demo]# docker service update --update-parallelism 1 --image manager1.example.com:5000/phpdemo:v2 phpdemo phpdemo [root@manager1 docker-swarm-demo]# docker service ps phpdemo ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS qo0dxm16hly5 phpdemo.1 manager1.example.com:5000/phpdemo:v2 manager2.example.com Running Running about a minute ago m82bq5rgmk7z \\_ phpdemo.1 manager1.example.com:5000/phpdemo:v1 manager2.example.com Shutdown Shutdown about a minute ago z7782l71gnum phpdemo.2 manager1.example.com:5000/phpdemo:v2 manager3.example.com Running Running about a minute ago msiyu6y2gjxn \\_ phpdemo.2 manager1.example.com:5000/phpdemo:v1 manager3.example.com Shutdown Shutdown about a minute ago sxp3cs1vmn96 phpdemo.3 manager1.example.com:5000/phpdemo:v2 manager1.example.com Running Running about a minute ago 8x0pcftmzbzw \\_ phpdemo.3 manager1.example.com:5000/phpdemo:v1 manager1.example.com Shutdown Shutdown about a minute ago      # docker service ps   명령으로 보면 각 노드에 phpdemo:v2 로 이미지들이 교체가 된 것을 확인 할 수 있습니다.    실제로 아래 그림을 보시면 이해에 도움이 됩니다.  하나씩 신규로 이미지를 교체하면서 서비스를 운영하면서 신규 이미지로 배포가 되는 것을 볼 수 있습니다.    (상기 GIF 파일은 용량이 커서, 출력이 느릴수도 있습니다.)   서비스 Rollback    서비스 Rolling Update 를 진행하였는데 문제가 발생되어 원복을 해야되는 상황이 발생 할 수 있습니다.  Docker Swarm 은 Rollback 기능도 지원하고 있습니다.  Rollback 은 아래와 같이 수행 합니다.    # docker service update --rollback phpdemo   [root@manager1 docker-swarm-demo]# docker service update --rollback phpdemo phpdemo [root@manager1 docker-swarm-demo]# docker service ps phpdemo ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS u1w5rry6wvog phpdemo.1 manager1.example.com:5000/phpdemo:v1 manager1.example.com Running Running 24 seconds ago so0h6oyfdy65 \\_ phpdemo.1 manager1.example.com:5000/phpdemo:v2 manager1.example.com Shutdown Shutdown 26 seconds ago yzfysbktouw9 phpdemo.2 manager1.example.com:5000/phpdemo:v1 manager2.example.com Running Running 23 seconds ago i3lqa52jspx4 \\_ phpdemo.2 manager1.example.com:5000/phpdemo:v2 manager2.example.com Shutdown Shutdown 24 seconds ago whoukoq2hwor \\_ phpdemo.2 manager1.example.com:5000/phpdemo:v1 manager2.example.com Shutdown Shutdown 2 minutes ago z7782l71gnum \\_ phpdemo.2 manager1.example.com:5000/phpdemo:v2 manager3.example.com Shutdown Shutdown 3 minutes ago msiyu6y2gjxn \\_ phpdemo.2 manager1.example.com:5000/phpdemo:v1 manager3.example.com Shutdown Shutdown 7 minutes ago 9ju17obshxlp phpdemo.3 manager1.example.com:5000/phpdemo:v1 manager3.example.com Running Running 25 seconds ago 26s3p5fthc0f \\_ phpdemo.3 manager1.example.com:5000/phpdemo:v2 manager3.example.com Shutdown Shutdown 26 seconds ago 92g40nhfm15n \\_ phpdemo.3 manager1.example.com:5000/phpdemo:v1 manager3.example.com Shutdown Shutdown 3 minutes ago sxp3cs1vmn96 \\_ phpdemo.3 manager1.example.com:5000/phpdemo:v2 manager1.example.com Shutdown Shutdown 3 minutes ago 8x0pcftmzbzw \\_ phpdemo.3 manager1.example.com:5000/phpdemo:v1 manager1.example.com Shutdown Shutdown 7 minutes ago [root@manager1 docker-swarm-demo]#      # docker service ps   명령을 통해 확인해보면 Rollback 된 것을 자세히 확인 할 수 있습니다.    실제로 서비스는 어떻게 Rollback 되는지는 아래 그림을 보면 됩니다.  컨테이너 하나씩 기존 이미지로 Rollback 을 진행 하는 것을 확인 할 수 있습니다.    (상기 GIF 파일은 용량이 커서, 출력이 느릴수도 있습니다.)   Docker Host 장애 발생으로 인한 복구 시나리오    서비스를 운영하다보면 어떠한 이유로 시스템에 장애가 발생되어 서비스가 중단되는 경우가 발생합니다.  Docker Swarm 을 이용하면 서비스 장애에 대해 감지하고 장애가 발생된 노드의 컨테이너를 다른 노드로 이관하여 장애 복구를 진행합니다.    인위적으로 manager2.example.com 의 시스템을 강제로 중지시켜 장애를 발생시키고 어떻게 Docker Swarm 에서 복구하는지 확인 해보겠습니다.   [root@manager1 docker-swarm-demo]# docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS lrt89xwkugty162qk8c2av5ek manager2.example.com Ready Active Leader y8ul9r3jq0rgt9k3vbvrayeyg * manager1.example.com Ready Active Reachable yqerq5ujds38t0izzlp03dbhd manager3.example.com Ready Active Reachable [root@manager1 docker-swarm-demo]# docker service ps phpdemo ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS a50p5gs82hop phpdemo.1 manager1.example.com:5000/phpdemo:v2 manager2.example.com Running Running about a minute ago 4vwicf8zt7j4 phpdemo.2 manager1.example.com:5000/phpdemo:v2 manager3.example.com Running Running 46 seconds ago 7ik901zrb0ob phpdemo.3 manager1.example.com:5000/phpdemo:v2 manager1.example.com Running Running 51 seconds ago // 노드 2번 시스템 인위적인 장애 발생 [root@manager1 docker-swarm-demo]# docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS lrt89xwkugty162qk8c2av5ek manager2.example.com Down Active Unreachable y8ul9r3jq0rgt9k3vbvrayeyg * manager1.example.com Ready Active Leader yqerq5ujds38t0izzlp03dbhd manager3.example.com Ready Active Reachable [root@manager1 docker-swarm-demo]# [root@manager1 docker-swarm-demo]# docker service ps phpdemo ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS o32y0ozm5wpz phpdemo.1 manager1.example.com:5000/phpdemo:v2 manager1.example.com Running Running about a minute ago a50p5gs82hop \\_ phpdemo.1 manager1.example.com:5000/phpdemo:v2 manager2.example.com Shutdown Running 5 minutes ago 4vwicf8zt7j4 phpdemo.2 manager1.example.com:5000/phpdemo:v2 manager3.example.com Running Running about a minute ago 7ik901zrb0ob phpdemo.3 manager1.example.com:5000/phpdemo:v2 manager1.example.com Running Running about a minute ago [root@manager1 docker-swarm-demo]#        (상기 GIF 파일은 용량이 커서, 출력이 느릴수도 있습니다.)  위 그림을 보시면 장애 감지 및 장애 복구까지 소요된 시간이 약 40~50초 정도가 걸렸습니다.  manager2.example.com 시스템에 장애가 감지되고 해당 시스템에서 실행 중이던 컨테이너는 manager1.example.com 시스템에서 복구가 되는 것을 볼 수 있습니다.   마치며    컨테이너 오케스트레이션 도구 중에 하나인 Docker Swarm 에 대해서 알아보는 시간이 였습니다.  구성이 간단하고 기존에 Docker 를 잘 사용하셨다면 쉽게 운영에 적용 할 수 있을 것 같습니다!!  또한 컨테이너 하나만으로는 부족한 느낌이 많이 없어진 것 같습니다. ^ㅡ^    언제나 쉽게 오픈소스를 사용할 수 있도록 노력하겠습니다.  감사합니다.   참고 자료     https://docs.docker.com/engine/swarm/    https://docs.docker.com/engine/reference/commandline/service_update/    https://www.xenonstack.com/blog/top-trends-ci-cd-devops-tools/    https://success.docker.com/article/networking    https://tech.osci.kr/2019/02/13/59736201/    https://chhanz.github.io/docker/2018/09/10/45749387/    ","permalink":"https://chhanz88.github.io/post/2019-02-13-containerorchestration-docker-swarm/","summary":"회사 기술블로그에 작성한 내용입니다.  오픈소스컨설팅 기술블로그  Author. chhanz \n   안녕하세요? 오픈소스컨설팅 한철희 과장입니다.    이전 \"  Docker 이해하기  \" 를 포스팅에 이어, \"  Docker Swarm 을 이용한 Container Orchestration 환경 만들기  \" 라는 포스팅을 작성하게 되었습니다.   (Review - Docker 이해하기   )    이전 포스팅에서 Docker 를 직접 사용하면서 여러 장점을 확인했습니다.  하지만 과연 실무에 적용하면 안정적으로 서비스를 유지하고 운영할 수 있을지에 대해서는 의문을 가지고 있었습니다.","title":"Docker Swarm 을 이용한 Container Orchestration 환경 만들기"},{"content":"[ansible] ansible-vault 를 이용하여 암호화 하기  아래는 Ansible 을 이용하여 특정 node 에 httpd 를 설치하는 간단한 ansible 구문 입니다.\n--- - hosts: node  tasks:  - name: install httpd  yum:  name=httpd  state=present  - name: Start web server  service:  name=httpd  state=started ansible 을 이용하여, node host 에 접근해서 httpd 설치 명령을 실행하기 위해서는 ansible 에서 사용될 User 정보 및 Password 정보, ssh 접근에 대한 정보 등을 사전에 ansible 시스템에 추가하거나 vars 혹은 inventory 에 설정을 해야됩니다.\n하지만 위와 같이 vars 혹은 inventory 에 등록하는 것은 보안에 취약하므로, ansible-vault 를 이용해서 중요 정보를 암호화 할 수 있습니다.\nansible-playboot 문서 구문  ansible-playbook 문서 구성은 아래와 같습니다.\n# tree . ├── apache.yml // Playbook ├── group_vars // Playbook 전역 변수 Directory │ └── all └── inventory // Host Inventory 1 directory, 3 files ~/group_vars/all 암화화  ~/group_vars/all 을 암호화 합니다.\n# mkdir ~/group_vars # ansible-vault create ~/group_vars/all \u0026gt;\u0026gt; ansible-vault passwd 입력 // Start of File --- ansible_connection: ssh ansible_ssh_user: root ansible_port: 22 ansible_ssh_pass: rootpasswd // End of File Playbook 실행  ansible-vault 로 암호화된 파일을 로드하면서 Playbook 을 시작하기 위해서는 아래와 명령 수행을 합니다.\n# ansible-playbook -i inventory --ask-vault-pass apache.yml Vault password: \u0026lt;Password 입력\u0026gt; 위와 같이 ~/group_vars/all 전역 변수에 선언된 내용들을 이용해서, 해당 명령을 수행 할 수 있습니다.\nansible-vault 로 암호화 된 파일을 읽기 위해서는 아래와 같이 명령으로 해야됩니다.\n# ansible-vault view ~/group_vars/all ansible_connection: ssh ansible_ssh_user: root ansible_port: 22 ansible_ssh_pass: rootpasswd // ansible-vault 미 사용일 경우 # cat ~/group_vars/all $ANSIBLE_VAULT;1.1;AES256 37306166613966396265353335663565383733323336616335323232336632346637326239306364 3964373733376262623639373936373636613964656130340a336537613134663837663136346165 39626166623738613938326665386336653231643538383932653333663832633032303539313866 3437363637613334360a633730336264353764323536353637663338656239386432386434373162 66306162353562633734393731386233333261376461353362656666633337393931646432636263 37663866346233383165386430663265363932336362326132356666376138643338313565333033 39306135306638653434636332663662653265393464336335646366633135636261636630313961 30366231396562336338376137623461353066323264643836393065363432343833653531633263 3261 ","permalink":"https://chhanz88.github.io/post/2019-01-30-ansible-vault-create-groupvars/","summary":"[ansible] ansible-vault 를 이용하여 암호화 하기  아래는 Ansible 을 이용하여 특정 node 에 httpd 를 설치하는 간단한 ansible 구문 입니다.\n--- - hosts: node  tasks:  - name: install httpd  yum:  name=httpd  state=present  - name: Start web server  service:  name=httpd  state=started ansible 을 이용하여, node host 에 접근해서 httpd 설치 명령을 실행하기 위해서는 ansible 에서 사용될 User 정보 및 Password 정보, ssh 접근에 대한 정보 등을 사전에 ansible 시스템에 추가하거나 vars 혹은 inventory 에 설정을 해야됩니다.","title":"[ansible] ansible-vault 를 이용하여 암호화 하기"},{"content":"[Linux] logger 를 이용한 로그 관리  /var/log/messages 는 syslogd 를 이용하여 로그를 기록합니다.\n주로 시스템의 핵심 로그가 작성되고, 해당 파일을 모니터링하여 시스템의 장애에 대해 파악하기가 좋습니다.\n시스템을 운영하면서 특별히 /var/log/messages 에 별도의 메시지를 기록하기 위해서는 logger 라는 명령을 사용하면 쉽게 적용이 가능합니다.\n아래는 이번 포스팅을 위해 httpd Web 서비스가 정상인지 체크하는 간단하게 제작된 스크립트입니다.\n해당 스크립트에서 발생되는 메시지를 /var/log/messages 에 기록 하도록 하겠습니다.\nCheck Web  #!/bin/bash echo -e \u0026#34; \u0026#34; echo -e \u0026#34; Service Check : httpd \u0026#34; WEBSTAT=`systemctl status httpd | grep \u0026#34;Active:\u0026#34; | grep running | wc -l` echo -e \u0026#34; \u0026#34; if [[ \u0026#34;$WEBSTAT\u0026#34; -eq \u0026#34;0\u0026#34; ]]; then echo -e \u0026#34; ########### Web Service Stop ###########\u0026#34; echo -e \u0026#34; ########### RESTART HTTPD SERVER ########### \u0026#34; systemctl start httpd sleep 5 echo -e \u0026#34; ########### CHECK HTTPD SERVER ########### \u0026#34; systemctl -l status httpd fi echo -e \u0026#34; ########### CHECK HTTPD SERVER ########### \u0026#34; echo -e \u0026#34; * * * * Web Server : Running * * * * \u0026#34; logger 를 이용하여 messages 에 등록 아래 명령어를 통해 /var/log/messages 에 로그를 기록합니다.\n# ./chkweb.sh | logger -t ServiceCHKDeamon # tail -n 10 /var/log/messages Jan 19 21:43:50 localhost ServiceCHKDeamon: Service Check : httpd Jan 19 21:43:50 localhost ServiceCHKDeamon: httpd Jan 19 21:43:50 localhost ServiceCHKDeamon: ########### CHECK HTTPD SERVER ########### Jan 19 21:43:50 localhost ServiceCHKDeamon: * * * * Web Server : Running * * * * // httpd 가 중지 되었을 때 # ./chkweb.sh | logger -t ServiceCHKDeamon # cat /var/log/messages Jan 19 21:45:27 localhost ServiceCHKDeamon: Service Check : httpd Jan 19 21:45:27 localhost ServiceCHKDeamon: httpd Jan 19 21:45:27 localhost ServiceCHKDeamon: ########### Web Service Stop ########### Jan 19 21:45:27 localhost ServiceCHKDeamon: ########### RESTART HTTPD SERVER ########### Jan 19 21:45:27 localhost systemd: Starting The Apache HTTP Server... Jan 19 21:45:27 localhost systemd: Started The Apache HTTP Server. Jan 19 21:45:32 localhost ServiceCHKDeamon: ########### CHECK HTTPD SERVER ########### Jan 19 21:45:32 localhost ServiceCHKDeamon: ● httpd.service - The Apache HTTP Server Jan 19 21:45:32 localhost ServiceCHKDeamon: Loaded: loaded (/usr/lib/systemd/system/httpd.service; disabled; vendor preset: disabled) Jan 19 21:45:32 localhost ServiceCHKDeamon: Drop-In: /etc/systemd/system/httpd.service.d Jan 19 21:45:32 localhost ServiceCHKDeamon: └─test.conf Jan 19 21:45:32 localhost ServiceCHKDeamon: Active: active (running) since Sat 2019-01-19 21:45:27 KST; 5s ago Jan 19 21:45:32 localhost ServiceCHKDeamon: Docs: man:httpd(8) Jan 19 21:45:32 localhost ServiceCHKDeamon: man:apachectl(8) Jan 19 21:45:32 localhost ServiceCHKDeamon: Main PID: 22565 (httpd) Jan 19 21:45:32 localhost ServiceCHKDeamon: Status: \u0026#34;Processing requests...\u0026#34; Jan 19 21:45:32 localhost ServiceCHKDeamon: Tasks: 6 Jan 19 21:45:32 localhost ServiceCHKDeamon: CGroup: /system.slice/httpd.service Jan 19 21:45:32 localhost ServiceCHKDeamon: ├─22565 /usr/sbin/httpd -DFOREGROUND Jan 19 21:45:32 localhost ServiceCHKDeamon: ├─22566 /usr/sbin/httpd -DFOREGROUND Jan 19 21:45:32 localhost ServiceCHKDeamon: ├─22567 /usr/sbin/httpd -DFOREGROUND Jan 19 21:45:32 localhost ServiceCHKDeamon: ├─22568 /usr/sbin/httpd -DFOREGROUND Jan 19 21:45:32 localhost ServiceCHKDeamon: ├─22569 /usr/sbin/httpd -DFOREGROUND Jan 19 21:45:32 localhost ServiceCHKDeamon: └─22570 /usr/sbin/httpd -DFOREGROUND Jan 19 21:45:32 localhost ServiceCHKDeamon: UND Jan 19 21:45:32 localhost ServiceCHKDeamon: Jan 19 21:45:27 localhost.local systemd[1]: Starting The Apache HTTP Server... Jan 19 21:45:32 localhost ServiceCHKDeamon: Jan 19 21:45:27 localhost.local systemd[1]: Started The Apache HTTP Server. Jan 19 21:45:32 localhost ServiceCHKDeamon: ########### CHECK HTTPD SERVER ########### Jan 19 21:45:32 localhost ServiceCHKDeamon: * * * * Web Server : Running * * * * 위와 같이 특정 스크립트의 내용을 /var/log/messages 에 등록함으로 특정 모니터링 소프트웨어에 등록하여 서비스 상태 체크를 손쉽게 설정 할 수 있습니다.\n참고 자료   man logger : https://linux.die.net/man/1/logger Oracle Document : https://docs.oracle.com/cd/E19957-01/820-3203/log_syslog/index.html  ","permalink":"https://chhanz88.github.io/post/2019-01-19-linux-how-to-use-logger/","summary":"[Linux] logger 를 이용한 로그 관리  /var/log/messages 는 syslogd 를 이용하여 로그를 기록합니다.\n주로 시스템의 핵심 로그가 작성되고, 해당 파일을 모니터링하여 시스템의 장애에 대해 파악하기가 좋습니다.\n시스템을 운영하면서 특별히 /var/log/messages 에 별도의 메시지를 기록하기 위해서는 logger 라는 명령을 사용하면 쉽게 적용이 가능합니다.\n아래는 이번 포스팅을 위해 httpd Web 서비스가 정상인지 체크하는 간단하게 제작된 스크립트입니다.\n해당 스크립트에서 발생되는 메시지를 /var/log/messages 에 기록 하도록 하겠습니다.\nCheck Web  #!/bin/bash echo -e \u0026#34; \u0026#34; echo -e \u0026#34; Service Check : httpd \u0026#34; WEBSTAT=`systemctl status httpd | grep \u0026#34;Active:\u0026#34; | grep running | wc -l` echo -e \u0026#34; \u0026#34; if [[ \u0026#34;$WEBSTAT\u0026#34; -eq \u0026#34;0\u0026#34; ]]; then echo -e \u0026#34; ########### Web Service Stop ###########\u0026#34; echo -e \u0026#34; ########### RESTART HTTPD SERVER ########### \u0026#34; systemctl start httpd sleep 5 echo -e \u0026#34; ########### CHECK HTTPD SERVER ########### \u0026#34; systemctl -l status httpd fi echo -e \u0026#34; ########### CHECK HTTPD SERVER ########### \u0026#34; echo -e \u0026#34; * * * * Web Server : Running * * * * \u0026#34; logger 를 이용하여 messages 에 등록 아래 명령어를 통해 /var/log/messages 에 로그를 기록합니다.","title":"[Linux] logger 를 이용한 로그 관리"},{"content":"[Linux][RHEL/CentOS] systemd 에 Service 등록  Systemd 에 사용자가 자주 사용하는 Service 를 등록하고 Systemd 를 통해 관리 할 수 있습니다.\n아래는 Systemd 에 등록할 Service Script 입니다.\nSystemd test 용 Script  #!/bin/bash echo -e \u0026#34; Start Systemd Test \u0026#34; | logger -t Testsystemd while : do echo -e \u0026#34;Running systemd\u0026#34; sleep 30 done systemd 에 Service 등록  systemd 에 서비스를 등록하기 위해 아래 경로에 아래와 같이 설정을 합니다.\n# vi /etc/systemd/system/testchk.service // /etc/systemd/system/testchk.service 내용 [Unit] Description=Systemd Test Daemon [Service] Type=simple ExecStart=/root/test-daemon.sh Restart=on-failure [Install] WantedBy=multi-user.target 서비스 등록 및 시작  + 등록된 서비스 시작 # systemctl start testchk // 서비스 상태 체크 # systemctl status testchk ● testchk.service - Systemd Test Daemon Loaded: loaded (/etc/systemd/system/testchk.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2019-01-19 22:20:57 KST; 5s ago Main PID: 12003 (test-daemon.sh) Tasks: 2 CGroup: /system.slice/testchk.service ├─12003 /bin/bash /root/test-daemon.sh └─12006 sleep 30 Jan 19 22:20:57 localhost.local systemd[1]: Started Systemd Test Daemon. Jan 19 22:20:57 localhost.local test-daemon.sh[12003]: Runniog systemd + 재부팅 후에도 서비스가 시작되도록 서비스 등록 # systemctl enable testchk Created symlink from /etc/systemd/system/multi-user.target.wants/testchk.service to /etc/systemd/system/testchk.service. 위와 같이 등록이 가능합니다.\n[Install] 항목에 설정된 내용에 맞게 서비스에 등록이 됩니다.\nService Restart Action 지정  위에 테스트로 만들어진 /etc/systemd/system/testchk.service 를 보면, Restart 설정이 있습니다.\non-failure 로 Restart 를 설정하면 서비스에 문제가 생기면 systemd 가 해당 서비스를 재시작 합니다.\n+ 서비스 상태 확인 # systemctl status testchk ● testchk.service - Systemd Test Daemon Loaded: loaded (/etc/systemd/system/testchk.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2019-01-19 22:20:57 KST; 5min ago Main PID: 12003 (test-daemon.sh) Tasks: 2 CGroup: /system.slice/testchk.service ├─12003 /bin/bash /root/test-daemon.sh └─12487 sleep 30 Jan 19 22:21:27 localhost.local test-daemon.sh[12003]: Runniog systemd Jan 19 22:21:57 localhost.local test-daemon.sh[12003]: Runniog systemd Jan 19 22:22:27 localhost.local test-daemon.sh[12003]: Runniog systemd Jan 19 22:22:57 localhost.local test-daemon.sh[12003]: Runniog systemd Jan 19 22:23:27 localhost.local test-daemon.sh[12003]: Runniog systemd Jan 19 22:23:57 localhost.local test-daemon.sh[12003]: Runniog systemd Jan 19 22:24:27 localhost.local test-daemon.sh[12003]: Runniog systemd Jan 19 22:24:57 localhost.local test-daemon.sh[12003]: Runniog systemd Jan 19 22:25:27 localhost.local test-daemon.sh[12003]: Runniog systemd Jan 19 22:25:57 localhost.local test-daemon.sh[12003]: Runniog systemd + 서비스 강제 중지 # ps -ef | grep test root 12003 1 0 22:20 ? 00:00:00 /bin/bash /root/test-daemon.sh root 12500 11911 0 22:26 pts/1 00:00:00 grep --color=auto test # kill -9 12003 + 서비스 재시작 확인 # systemctl status testchk ● testchk.service - Systemd Test Daemon Loaded: loaded (/etc/systemd/system/testchk.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2019-01-19 22:27:05 KST; 28s ago Main PID: 12509 (test-daemon.sh) Tasks: 2 CGroup: /system.slice/testchk.service ├─12509 /bin/bash /root/test-daemon.sh └─12512 sleep 30 Jan 19 22:27:05 localhost.local systemd[1]: testchk.service holdoff time over, scheduling restart. Jan 19 22:27:05 localhost.local systemd[1]: Stopped Systemd Test Daemon. Jan 19 22:27:05 localhost.local systemd[1]: Started Systemd Test Daemon. Jan 19 22:27:05 localhost.local test-daemon.sh[12509]: Runniog systemd 위와 같이 서비스가 문제가 발생하여, 서비스가 중지가 되고 해당 서비스가 재시작이 된 것을 볼 수 있습니다.\nsystemd 를 이용하면 손쉽게 Custom 서비스를 관리 할 수 있습니다.\n참고자료   systemd : https://www.freedesktop.org/wiki/Software/systemd/ on-failure Option : https://singlebrook.com/2017/10/23/auto-restart-crashed-service-systemd/  ","permalink":"https://chhanz88.github.io/post/2019-01-18-linux-how-to-create-custom-systemd-service/","summary":"[Linux][RHEL/CentOS] systemd 에 Service 등록  Systemd 에 사용자가 자주 사용하는 Service 를 등록하고 Systemd 를 통해 관리 할 수 있습니다.\n아래는 Systemd 에 등록할 Service Script 입니다.\nSystemd test 용 Script  #!/bin/bash echo -e \u0026#34; Start Systemd Test \u0026#34; | logger -t Testsystemd while : do echo -e \u0026#34;Running systemd\u0026#34; sleep 30 done systemd 에 Service 등록  systemd 에 서비스를 등록하기 위해 아래 경로에 아래와 같이 설정을 합니다.","title":"[Linux] systemd 에 Service 등록"},{"content":"[Linux] CentOS 6 Python 2.7 설치  CentOS 6 의 기본 Python 버전은 v2.6 입니다.\nCentOS 의 응용프로그램들은 Python 2.6 과 호환되도록 설정되어있고, 이를 Update 를 할 경우 다수의 응용프로그램들이 문제가 발생 할 수 있습니다. 하지만 일부 시스템은 몇몇 소프트웨어를 사용하기위해, 최신 버전의 Python 을 사용하기 원합니다.\n위와 같은 일부 시스템에 최신 버전의 Python 을 설치하기 위해서는 다음과 같은 절차로 설치를 진행하면 됩니다.\nSCL Repository 연결  yum 명령을 통해 SCL Repository 를 연결합니다.\n# yum install -y centos-release-scl SCL Repository 연결 확인  Repository 가 정살적으로 연결 되었는지 확인합니다.\n# yum repolist Loading mirror speeds from cached hostfile * base: centos.mirror.moack.net * epel: mirror.premi.st * extras: centos.mirror.moack.net * updates: centos.mirror.moack.net repo id repo name status base CentOS-6 - Base 6,713 centos-sclo-rh CentOS-6 - SCLo rh 2,880 centos-sclo-sclo CentOS-6 - SCLo sclo 406 epel Extra Packages for Enterprise Linux 6 - x86_64 12,517 extras CentOS-6 - Extras 35 updates CentOS-6 - Updates 257 repolist: 22,808 Python 2.7 설치  등록한 Repository 를 통해 설치가 가능한 Python 을 확인합니다.\n# yum list python27-python.x86_64 --showd Loaded plugins: fastestmirror, refresh-packagekit, security Loading mirror speeds from cached hostfile * base: centos.mirror.moack.net * epel: mirror.premi.st * extras: centos.mirror.moack.net * updates: centos.mirror.moack.net Available Packages python27-python.x86_64 2.7.8-3.el6 centos-sclo-rh python27-python.x86_64 2.7.8-16.el6 centos-sclo-rh python27-python.x86_64 2.7.8-18.el6 centos-sclo-rh python27-python.x86_64 2.7.13-3.el6 centos-sclo-rh 원하는 특정 버전을 확인하고 설치 진행합니다.\n테스트를 위해 v2.7.8-3 로 설치하도록 하겠습니다.\n# yum install python27-python-2.7.8-3.el6 Loaded plugins: fastestmirror, refresh-packagekit, security Setting up Install Process Loading mirror speeds from cached hostfile * base: centos.mirror.moack.net * epel: mirror.premi.st * extras: centos.mirror.moack.net * updates: centos.mirror.moack.net Resolving Dependencies --\u0026gt; Running transaction check ---\u0026gt; Package python27-python.x86_64 0:2.7.8-3.el6 will be installed --\u0026gt; Processing Dependency: python27-python-libs(x86-64) = 2.7.8-3.el6 for package: python27-python-2.7.8-3.el6.x86_64 --\u0026gt; Processing Dependency: python27-runtime for package: python27-python-2.7.8-3.el6.x86_64 --\u0026gt; Processing Dependency: libpython2.7.so.1.0()(64bit) for package: python27-python-2.7.8-3.el6.x86_64 --\u0026gt; Running transaction check ---\u0026gt; Package python27-python-libs.x86_64 0:2.7.8-3.el6 will be installed ---\u0026gt; Package python27-runtime.x86_64 0:1.1-25.el6 will be installed --\u0026gt; Processing Dependency: scl-utils for package: python27-runtime-1.1-25.el6.x86_64 --\u0026gt; Running transaction check ---\u0026gt; Package scl-utils.x86_64 0:20120927-29.el6_9 will be installed --\u0026gt; Finished Dependency Resolution Dependencies Resolved ================================================================================================================================== Package Arch Version Repository Size ================================================================================================================================== Installing: python27-python x86_64 2.7.8-3.el6 centos-sclo-rh 80 k Installing for dependencies: python27-python-libs x86_64 2.7.8-3.el6 centos-sclo-rh 5.6 M python27-runtime x86_64 1.1-25.el6 centos-sclo-rh 1.0 M scl-utils x86_64 20120927-29.el6_9 base 23 k Transaction Summary ================================================================================================================================== Install 4 Package(s) Total download size: 6.8 M Installed size: 24 M Is this ok [y/N]: ... 중략 ... Installed: python27-python.x86_64 0:2.7.8-3.el6 Dependency Installed: python27-python-libs.x86_64 0:2.7.8-3.el6 python27-runtime.x86_64 0:1.1-25.el6 scl-utils.x86_64 0:20120927-29.el6_9 Complete! 위와 같이 설치가 완료가 되면, scl Command 를 이용하여 Python 을 활성화 해야됩니다.\n// scl 설정 전 # python -V Python 2.6.6 // scl 을 이용하여 PATH 등록 # scl --list python27 # scl enable python27 bash # echo $PATH /opt/rh/python27/root/usr/bin:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin # python -V Python 2.7.8 PATH 가 등록이 되고 Python 이 최신 버전으로 Update 된 것을 볼 수 있습니다.\n.bash_profile 에 등록  Python 2.7 설치 경로에 있는 enable 파일을 .bash_profile 에 추가하여 shell 이 재시작하더라도 사용이 가능하게 설정을 할 수 있습니다.\n# vi .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs PATH=$PATH:$HOME/bin export PATH source /opt/rh/python27/enable 시스템 재접속 테스트 # exit logout Connection to 192.168.XX.XX closed. ~ $ ssh root@192.168.XX.XX root@192.168.XX.XX\u0026#39;s password: Last login: Wed Jan 9 00:00:00 2019 from 192.168.XX.XX # python -V Python 2.7.8 위와 같이 설치가 완료되었습니다.\n","permalink":"https://chhanz88.github.io/post/2019-01-09-rhel6-python27/","summary":"[Linux] CentOS 6 Python 2.7 설치  CentOS 6 의 기본 Python 버전은 v2.6 입니다.\nCentOS 의 응용프로그램들은 Python 2.6 과 호환되도록 설정되어있고, 이를 Update 를 할 경우 다수의 응용프로그램들이 문제가 발생 할 수 있습니다. 하지만 일부 시스템은 몇몇 소프트웨어를 사용하기위해, 최신 버전의 Python 을 사용하기 원합니다.\n위와 같은 일부 시스템에 최신 버전의 Python 을 설치하기 위해서는 다음과 같은 절차로 설치를 진행하면 됩니다.\nSCL Repository 연결  yum 명령을 통해 SCL Repository 를 연결합니다.","title":"[Linux] CentOS 6 Python 2.7 설치"},{"content":"Spacewalk latest Docker Image 제작기  안녕하세요? chhanz 입니다.\n고객사의 요청으로 Spacewalk 를 docker container 형태로 운영을 해야되는 요건이 생겨서,\nimage를 만들기 시작 하였습니다\u0026hellip;\u0026hellip; ;(\nSpacewalk 란?  Spacewalk는 오픈 소스 Linux 시스템 관리 솔루션입니다.\nRed Hat Satellite 제품이 파생 된 업스트림 커뮤니티 프로젝트입니다.\n쉽게 가자\u0026hellip;  docker 의 장점이 뭡니까!\ndocker hub의 많은 official image 아닙니까!\n열심히 docker image를 찾아보았습니다.\n하지만\u0026hellip;\n# docker search spacewalk INDEX NAME DESCRIPTION STARS OFFICIAL AUTOMATED docker.io docker.io/bashell/spacewalk Spacewalk Docker Image 7 [OK] docker.io docker.io/ruo91/spacewalk Spacewalk is an open source Linux systems ... 4 [OK] docker.io docker.io/pajinek/docker-spacewalk Spacewalk in Docker (without systemd) 3 [OK] docker.io docker.io/egonzalez90/spacewalk Spacewalk docker image 1 [OK] OFFICIAL 은 없고, 그나마 STARS 는 7개 뿐이고\u0026hellip;\n내가 만들어야겠다\u0026hellip; ;)\n이렇게 시작 되었습니다.\nDocker image 를 만들어보자!  먼저 참고할 Dockerfile 은 docker hub 에서 가져왔습니다.\n https://bitbucket.org/bashell-com/spacewalk\n Dockerfile 은 아래와 같이 수정하였습니다.\n# Dockerfile - Spacewalk # # - Build # # docker build --rm -t spacewalk . # # - Run # docker run --privileged=true -d --name=\u0026#34;spacewalk\u0026#34; -p 80:80 -p 443:443 spacewalk # 1. Base images FROM centos:6 MAINTAINER chhanz \u0026lt;chhan@osci.kr\u0026gt; # 2. Set the environment variable WORKDIR /opt # 3. Add the epel, spacewalk, jpackage, supervisord ADD conf/group_spacewalkproject-epel6-addons-epel-6.repo /etc/yum.repos.d/epel6-addons-epel-6.repo ADD conf/group_spacewalkproject-java-packages-epel-7.repo /etc/yum.repos.d/java-packages-epel-7.repo RUN yum install -y epel-release \\ \u0026amp;\u0026amp; rpm -Uvh https://copr-be.cloud.fedoraproject.org/results/@spacewalkproject/spacewalk-2.8/epel-6-x86_64/00736372-spacewalk-repo/spacewalk-repo-2.8-11.el6.noarch.rpm \\ \u0026amp;\u0026amp; yum check-update ; yum upgrade -y \\ \u0026amp;\u0026amp; yum install -y spacewalk-setup-postgresql spacewalk-postgresql tomcat-native python-setuptools python-pip \\ \u0026amp;\u0026amp; yum clean all \\ \u0026amp;\u0026amp; pip install supervisor \u0026amp;\u0026amp; pip install --upgrade meld3==0.6.10 \u0026amp;\u0026amp; mkdir /etc/supervisord.d \\ \u0026amp;\u0026amp; rm -rf /root/.cache \\ \u0026amp;\u0026amp; cat /usr/share/zoneinfo/Asia/Seoul \u0026gt; /etc/localtime # 4. Install supervisord config ADD conf/supervisord.conf /etc/supervisord.d/supervisord.conf # 5. Install spacewalk initial and running scripts ADD conf/answer.txt /opt/answer.txt ADD conf/spacewalk.sh /opt/spacewalk.sh # 6. Start supervisord CMD [\u0026#34;/usr/bin/supervisord\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;/etc/supervisord.d/supervisord.conf\u0026#34;] # System Log VOLUME /var/log # PostgreSQL Data VOLUME /var/lib/pgsql/data # RPM repository VOLUME /var/satellite # Bootstrap directory VOLUME /var/www/html/pub # Port EXPOSE 80 443 수정된 내용은 아래와 같습니다.\n  Update Spacewalk (Spacewalk 2.6 \u0026gt; Spacewalk 2.8)   Spacewalk 2.8 Repository 로 변경 spacewalk-setup 에 필요한 Answer file 을 최신버전의 요건에 맞게 수정하였습니다. Container 로 실행할때, Spacewalk 가 정상적으로 시작되는지 확인이 가능하도록 내부 Script 를 수정하였습니다. supervisord 를 통해, Spacewalk 를 시작하는데 해당 Log 를 debug 수준으로 설정해서 상태를 확인하는 것이 더욱 쉽게 만들었습니다. localtime 을 Asia/Seoul 으로 설정 하였습니다.  위와 같이 수정한 Dockerfile 를 Build 합니다.\nBuild  Dockerfile 및 image 에 추가될 file 이 있는 Directory 에서 Build 를 진행합니다.\n# docker build --rm -t spacewalk . 많은 시간이 소요되면서 Build 가 진행 됩니다.\n# docker images REPOSITORY TAG IMAGE ID CREATED SIZE spacewalk latest f0e26b8230ad 13 days ago 1.04 GB 위와 같이 Build 가 완료 되었습니다.\nRun  Build 가 된 이미지를 실행합니다.\n먼저 Container 가 종료되고, 제거가 되더라도 Spacewalk 데이터를 보존 할 수 있게 Volume 으로 사용될 경로를 생성합니다.\nmkdir -p /data/var/lib/pgsql/data mkdir -p /data/var/satellite mkdir -p /data/var/www/html/pub mkdir -p /data/var/log/tomcat6 mkdir -p /data/var/log/httpd mkdir -p /data/var/log/cobbler chown 26:26 /data/var/lib/pgsql/data chmod 777 -R /data/var Volume 생성이 다 되었다면,\n# docker run --privileged=true -d --name=\u0026#34;spacewalk\u0026#34; -p 80:80 -p 443:443 -v /data/var/log:/var/log -v /data/var/lib/pgsql/data:/var/lib/pgsql/data -v /data/var/satellite:/var/satellite -v /data/var/www/html/pub:/var/www/html/pub spacewalk 위 명령으로 Container 를 시작합니다.\nhttps://(Docker Host IP) 로 접속합니다.\n아래와 같이 Spacewalk 화면을 확인 할 수 있습니다.\n마치며  위와 같이 나만의 특별한 Docker Image 를 생성 할 수 있었습니다. 해당 Dockerfile 및 기타 필요한 file, 쉽게 Spacewalk 를 운영 할 수 있도록 만든 Script 들을 github 에 올려놨습니다.\n# git clone https://github.com/chhanz/spacewalk-docker-img.git 앞으로도 Opensource Software 를 손쉽게 사용 할 수 있도록 노력 하겠습니다.\n감사합니다.\n참고 자료   Base Dockerfile : https://bitbucket.org/bashell-com/spacewalk Spacewalk Document : https://spacewalkproject.github.io/documentation.html chhanz github : https://github.com/chhanz/spacewalk-docker-img 해당 포스팅은 오픈소스컨설팅 기술블로그에서도 확인하실 수 있습니다! : https://tech.osci.kr/spacewalk/2019/01/07/55869620/  ","permalink":"https://chhanz88.github.io/post/2019-01-07-spacewalkdocker/","summary":"Spacewalk latest Docker Image 제작기  안녕하세요? chhanz 입니다.\n고객사의 요청으로 Spacewalk 를 docker container 형태로 운영을 해야되는 요건이 생겨서,\nimage를 만들기 시작 하였습니다\u0026hellip;\u0026hellip; ;(\nSpacewalk 란?  Spacewalk는 오픈 소스 Linux 시스템 관리 솔루션입니다.\nRed Hat Satellite 제품이 파생 된 업스트림 커뮤니티 프로젝트입니다.\n쉽게 가자\u0026hellip;  docker 의 장점이 뭡니까!\ndocker hub의 많은 official image 아닙니까!\n열심히 docker image를 찾아보았습니다.\n하지만\u0026hellip;\n# docker search spacewalk INDEX NAME DESCRIPTION STARS OFFICIAL AUTOMATED docker.","title":"[Docker] Spacewalk latest Docker Image 제작기"},{"content":"RHEL / CentOS 7 - Default boot kernel 변경 방법  시스템을 부팅을 하면서 GRUB Menu 에서 Kernel 을 특정 버전으로 선택하여, 부팅을 할 수 있습니다.\n하지만 시스템을 물리적으로 접근이 불가능하거나, 원격으로만 작업이 가능할 경우에는 아래와 같은 방법으로 Default boot Kernel 을 변경해서 특정 버전으로 선택하여 부팅을 할 수 있습니다.\n주로 Kernel Update 이후, Kernel Version 원복 등으로 사용 할 수 있습니다.\nKernel Entry 확인  GRUB Menu 에서 Kernel Entry 가 어떻게 나올지 확인을 합니다.\n# cat /etc/grub2.cfg | grep \u0026#34;menuentry \u0026#34; | awk -F\u0026#34;\u0026#39;\u0026#34; \u0026#39;{print $2}\u0026#39; CentOS Linux (3.10.0-957.1.3.el7.x86_64.debug) 7 (Core) CentOS Linux (3.10.0-957.1.3.el7.x86_64) 7 (Core) CentOS Linux (3.10.0-862.el7.x86_64) 7 (Core) CentOS Linux (0-rescue-300ad7a22ef1472facc28c7d606e6ca3) 7 (Core) 위와 같이 확인되면 Entry 는 아래와 같이 지정을 할 수 있습니다.\nCentOS Linux (3.10.0-957.1.3.el7.x86_64.debug) 7 (Core)\t\u0026gt;\u0026gt; Entry 0 CentOS Linux (3.10.0-957.1.3.el7.x86_64) 7 (Core) \u0026gt;\u0026gt; Entry 1 CentOS Linux (3.10.0-862.el7.x86_64) 7 (Core)\t\u0026gt;\u0026gt; Entry 2 CentOS Linux (0-rescue-300ad7a22ef1472facc28c7d606e6ca3) 7 (Core) \u0026gt;\u0026gt; Entry 3 Entry 변경  grub2-set-default 명령을 통해 변경이 가능합니다.\n기존 Kernel Version 은 아래와 같습니다.\n# cat /boot/grub2/grubenv # GRUB Environment Block saved_entry=CentOS Linux (3.10.0-957.1.3.el7.x86_64) 7 (Core) # uname -a Linux localhost.local 3.10.0-957.1.3.el7.x86_64 #1 SMP Thu Nov 29 14:49:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux  Entry 변경  # grub2-set-default 2 # cat /boot/grub2/grubenv # GRUB Environment Block saved_entry=2 위와 같이 변경을 하고 시스템을 재부팅 합니다.\nDefault Boot Kernel 변경 확인  시스템이 재부팅 되면 아래와 같이 Default Boot Kernel 이 변경된 것을 확인 할 수 있습니다.\n# uptime 22:11:20 up 1 min, 1 user, load average: 0.89, 0.37, 0.14 # uname -a Linux localhost.local 3.10.0-862.el7.x86_64 #1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux # cat /boot/grub2/grubenv # GRUB Environment Block saved_entry=2 ","permalink":"https://chhanz88.github.io/post/2018-12-14-linux-change-default-boot-kernel/","summary":"RHEL / CentOS 7 - Default boot kernel 변경 방법  시스템을 부팅을 하면서 GRUB Menu 에서 Kernel 을 특정 버전으로 선택하여, 부팅을 할 수 있습니다.\n하지만 시스템을 물리적으로 접근이 불가능하거나, 원격으로만 작업이 가능할 경우에는 아래와 같은 방법으로 Default boot Kernel 을 변경해서 특정 버전으로 선택하여 부팅을 할 수 있습니다.\n주로 Kernel Update 이후, Kernel Version 원복 등으로 사용 할 수 있습니다.\nKernel Entry 확인  GRUB Menu 에서 Kernel Entry 가 어떻게 나올지 확인을 합니다.","title":"[Linux] Default boot kernel 변경 방법"},{"content":"Docker 기초 실습 교육 자료  엔터프라이즈 고객사 IT 부서 담당자분들께 실습 교육을 위해 제작한 교육 자료 입니다.\n(해당 실습을 위한 자료는 Github 에 있습니다.)\n참고 자료  Slideshare : https://www.slideshare.net/ienvyou/docker-20181113v3\nchhanz Github : https://github.com/chhanz/docker_training\n","permalink":"https://chhanz88.github.io/post/2018-11-13-docker-training/","summary":"Docker 기초 실습 교육 자료  엔터프라이즈 고객사 IT 부서 담당자분들께 실습 교육을 위해 제작한 교육 자료 입니다.\n(해당 실습을 위한 자료는 Github 에 있습니다.)\n참고 자료  Slideshare : https://www.slideshare.net/ienvyou/docker-20181113v3\nchhanz Github : https://github.com/chhanz/docker_training","title":"[Docker] Docker 기초 실습 교육자료"},{"content":"[AIX] System Information collection Script  회사 U2C 솔루션 개발을 하는 중, AIX 에서 필수 시스템 정보를 수집하는 Script 가 필요로 해서 간단하게 제작한 Script 입니다.\nScript 내용  #! /bin/ksh # Make by. chhan  DATEC=$(date +%Y%m%d-%H%M) IdChk=`id | grep root | wc -l` OutFile=\u0026#34;`hostname`_\u0026#34;$DATEC\u0026#34;.txt\u0026#34;  #Checking ROOT  if [ $IdChk -eq 0 ]; then  echo  echo \u0026#34;You must login root... Try again...\u0026#34;  echo  exit  fi  echo \u0026#34; * * * * * Check AIX System Information * * * * * \u0026#34; echo \u0026#34; \u0026#34; echo \u0026#34; This Check Output File \u0026#34; echo \u0026#34; View ./\u0026#34;$OutFile echo \u0026#34; Date : \u0026#34; $(date) echo \u0026#34; \u0026#34; echo \u0026#34; # Gethering Information . . . . . \u0026#34;  prtconf \u0026gt; /tmp/prtconf-$DATEC.txt sleep 1  cputype=$(cat /tmp/prtconf-$DATEC.txt | grep \u0026#34;Processor Type\u0026#34; | awk -F \u0026#34;:\u0026#34; \u0026#39;{print $2}\u0026#39;) kerneltype=$(cat /tmp/prtconf-$DATEC.txt | grep \u0026#34;Kernel Type\u0026#34; | awk -F \u0026#34;:\u0026#34; \u0026#39;{print $2}\u0026#39;) sizemem=$(cat /tmp/prtconf-$DATEC.txt | grep \u0026#34;^Memory Size\u0026#34; | awk -F \u0026#34;:\u0026#34; \u0026#39;{print $2}\u0026#39;) cpucore=$(cat /tmp/prtconf-$DATEC.txt | grep \u0026#34;Number Of Processors\u0026#34; | awk -F \u0026#34;:\u0026#34; \u0026#39;{print $2}\u0026#39;) ipaddr=$(cat /tmp/prtconf-$DATEC.txt | grep \u0026#34;IP Address:\u0026#34; | awk -F \u0026#34;:\u0026#34; \u0026#39;{print $2}\u0026#39;) subnet=$(cat /tmp/prtconf-$DATEC.txt | grep \u0026#34;Sub Netmask:\u0026#34; | awk -F \u0026#34;:\u0026#34; \u0026#39;{print $2}\u0026#39;) gateway=$(cat /tmp/prtconf-$DATEC.txt | grep \u0026#34;Gateway:\u0026#34; | awk -F \u0026#34;:\u0026#34; \u0026#39;{print $2}\u0026#39;) totalps=$(cat /tmp/prtconf-$DATEC.txt | grep \u0026#34;Total Paging Space:\u0026#34; | awk -F \u0026#34;:\u0026#34; \u0026#39;{print $2}\u0026#39;)  echo \u0026#34; # Print \u0026amp; Save Information\u0026#34; echo \u0026#34;-------------------------------------------\u0026#34;| tee -a $OutFile echo \u0026#34; System Infomation \u0026#34;| tee -a $OutFile echo \u0026#34;-------------------------------------------\u0026#34;| tee -a $OutFile echo \u0026#34; Host Name :\u0026#34; $(hostname)| tee -a $OutFile echo \u0026#34; Vender :\u0026#34; $(uname -M | awk -F \u0026#34;,\u0026#34; \u0026#39;{print $1}\u0026#39;)| tee -a $OutFile echo \u0026#34; CPU Type :\u0026#34; $cputype| tee -a $OutFile #echo \u0026#34; Kernel Type :\u0026#34; $kerneltype| tee -a $OutFile echo \u0026#34; Kernel Bit :\u0026#34; $(getconf KERNEL_BITMODE)\u0026#34;-bit\u0026#34;| tee -a $OutFile echo \u0026#34; OS Version :\u0026#34; $(oslevel -s)| tee -a $OutFile echo \u0026#34; Number Of Processors :\u0026#34; $cpucore| tee -a $OutFile echo \u0026#34; Memory :\u0026#34; $sizemem | tee -a $OutFile echo \u0026#34; \u0026#34; | tee -a $OutFile echo \u0026#34; IP Address :\u0026#34; $ipaddr\u0026#34; /\u0026#34;$subnet| tee -a $OutFile echo \u0026#34; Gateway IP: \u0026#34; $gateway| tee -a $OutFile echo \u0026#34; \u0026#34; | tee -a $OutFile echo \u0026#34; Total Page Space Size :\u0026#34; $totalps| tee -a $OutFile echo \u0026#34; Detail Page Space :\u0026#34;| tee -a $OutFile lsps -a| tee -a $OutFile echo \u0026#34; \u0026#34; | tee -a $OutFile echo \u0026#34; LVM Information :\u0026#34;| tee -a $OutFile lsvg -l rootvg| tee -a $OutFile echo \u0026#34; \u0026#34; | tee -a $OutFile echo \u0026#34; Total df Size :\u0026#34; | tee -a $OutFile df -gt| tee -a $OutFile echo \u0026#34; \u0026#34;| tee -a $OutFile echo \u0026#34; rootvg Filesystem Size : \u0026#34;| tee -a $OutFile df -gt | grep \u0026#34;Mounted\u0026#34;| tee -a $OutFile lsvg -l rootvg | grep \u0026#34;/\u0026#34; | grep -v \u0026#34;N/A\u0026#34; | awk \u0026#39;{print \u0026#34;df -gt \u0026#34;$7}\u0026#39; | sh | grep -v Mounted| tee -a $OutFile SCRIPT Output   * * * * * Check AIX System Information * * * * * This Check Output File View ./aix_test_20181104-2237.txt Date : Wed Jan 9 22:37:52 KST 2019 # Gethering Information . . . . . # Print \u0026amp; Save Information ------------------------------------------- System Infomation ------------------------------------------- Host Name : aix_test Vender : IBM CPU Type : PowerPC_POWER7 Kernel Bit : 64-bit OS Version : 6100-09-03-1415 Number Of Processors : 2 Memory : 31744 MB IP Address : 192.168.00.000 / 255.255.255.0 Gateway IP: 192.168.00.00 Total Page Space Size : 16384MB Detail Page Space : Page Space Physical Volume Volume Group Size %Used Active Auto Type Chksum hd6 hdisk0 rootvg 16384MB 0 yes yes lv 0 LVM Information : rootvg: LV NAME TYPE LPs PPs PVs LV STATE MOUNT POINT hd5 boot 1 1 1 closed/syncd N/A hd6 paging 32 32 1 open/syncd N/A hd8 jfs2log 1 1 1 open/syncd N/A hd4 jfs2 8 8 1 open/syncd / hd2 jfs2 20 20 1 open/syncd /usr hd9var jfs2 2 2 1 open/syncd /var hd3 jfs2 10 10 1 open/syncd /tmp hd1 jfs2 2 2 1 open/syncd /home hd10opt jfs2 2 2 1 open/syncd /opt hd11admin jfs2 1 1 1 open/syncd /admin lg_dumplv sysdump 4 4 1 open/syncd N/A livedump jfs2 1 1 1 open/syncd /var/adm/ras/livedump loglv00 jfslog 1 1 1 open/syncd N/A lv00 jfs 1 1 1 open/syncd /var/adm/csd app_lv jfs2 180 180 1 open/syncd /app Total df Size : Filesystem GB blocks Used Free %Used Mounted on /dev/hd4 4.00 1.27 2.73 32% / /dev/hd2 10.00 6.48 3.52 65% /usr /dev/hd9var 1.00 0.40 0.60 41% /var /dev/hd3 5.00 0.90 4.10 19% /tmp /dev/hd1 1.00 0.14 0.86 15% /home /dev/hd11admin 0.50 0.00 0.50 1% /admin /proc - - - - /proc /dev/hd10opt 1.00 0.61 0.39 61% /opt /dev/livedump 0.50 0.00 0.50 1% /var/adm/ras/livedump /dev/lv00 0.50 0.02 0.48 4% /var/adm/csd /dev/app_lv 90.00 41.10 48.90 46% /app rootvg Filesystem Size : Filesystem GB blocks Used Free %Used Mounted on /dev/hd4 4.00 1.27 2.73 32% / /dev/hd2 10.00 6.48 3.52 65% /usr /dev/hd9var 1.00 0.40 0.60 41% /var /dev/hd3 5.00 0.90 4.10 19% /tmp /dev/hd1 1.00 0.14 0.86 15% /home /dev/hd10opt 1.00 0.61 0.39 61% /opt /dev/hd11admin 0.50 0.00 0.50 1% /admin /dev/livedump 0.50 0.00 0.50 1% /var/adm/ras/livedump /dev/lv00 0.50 0.02 0.48 4% /var/adm/csd /dev/app_lv 90.00 41.10 48.90 46% /app 해당 SCRIPT 의 일부가 사용된 솔루션  Playce RoRo : https://roro.play-ce.io/\n","permalink":"https://chhanz88.github.io/post/2018-11-04-aix-system-info-script/","summary":"[AIX] System Information collection Script  회사 U2C 솔루션 개발을 하는 중, AIX 에서 필수 시스템 정보를 수집하는 Script 가 필요로 해서 간단하게 제작한 Script 입니다.\nScript 내용  #! /bin/ksh # Make by. chhan  DATEC=$(date +%Y%m%d-%H%M) IdChk=`id | grep root | wc -l` OutFile=\u0026#34;`hostname`_\u0026#34;$DATEC\u0026#34;.txt\u0026#34;  #Checking ROOT  if [ $IdChk -eq 0 ]; then  echo  echo \u0026#34;You must login root... Try again...\u0026#34;  echo  exit  fi  echo \u0026#34; * * * * * Check AIX System Information * * * * * \u0026#34; echo \u0026#34; \u0026#34; echo \u0026#34; This Check Output File \u0026#34; echo \u0026#34; View .","title":"[AIX] System Information collection Script"},{"content":"ESXi - VM Clone Script  ESXi 는 VMware 에서 제공하는 Hypervisor 입니다.\nVMware 의 모든 가상화 기술을 이용하기 위해서는 vCenter 를 필수로 사용해야합니다.\n하지만 유료 라이센스라 제약 사항이 있습니다.(개인 사용자의 경우ㅠㅠ)\n제일 많이 사용되고 필요로 하는 기능중 하나가 바로 VM Clone 을 하는 기능입니다.\nvCenter 가 없이 ESXi 에서 VM Clone 을 하는 방법을 알아봅시다.\nTest 환경  VMware ESXi-6.7.0-8169922-standard (VMware, Inc.)\nESXi ssh enable  위와 같이 ESXi 에서 [작업] \u0026gt; [서비스] \u0026gt; [SSH 사용] 을 선택하여 ssh 를 활성화합니다.\nVM Clone Script 추가  추가 할 Script 의 내용은 아래와 같습니다.\n#!/bin/sh if [ $# -ne 2 ];then echo \u0026#34;USAGE: $0 SRC_DIR DEST_DIR\u0026#34;; exit; fi; ## remove / SRC=`basename \u0026#34;$1\u0026#34; /` DEST=`basename \u0026#34;$2\u0026#34; /` if [ ! -d \u0026#34;$SRC\u0026#34; ];then echo \u0026#34;Source Dir \\\u0026#34;$SRC\\\u0026#34; is not exist. Exit....\u0026#34;; exit; fi if [ -d \u0026#34;$DEST\u0026#34; ];then echo \u0026#34;Dest Dir \\\u0026#34;$DEST\\\u0026#34; is already exist. Exit....\u0026#34;; exit; fi echo \u0026#34;Soruce VM : $SRC\u0026#34; echo \u0026#34;Target VM : $DEST\u0026#34; mkdir \u0026#34;$DEST\u0026#34; vmkfstools -d thin -i \u0026#34;${SRC}/${SRC}\u0026#34;.vmdk \u0026#34;${DEST}/${DEST}\u0026#34;.vmdk sed \u0026#34;s/${SRC}/${DEST}/g\u0026#34; \u0026lt; \u0026#34;${SRC}/${SRC}\u0026#34;.vmx \u0026gt; \u0026#34;${DEST}/${DEST}\u0026#34;.vmx sed \u0026#34;s/${SRC}/${DEST}/g\u0026#34; \u0026lt; \u0026#34;${SRC}/${SRC}\u0026#34;.vmxf \u0026gt; \u0026#34;${DEST}/${DEST}\u0026#34;.vmxf echo \u0026#34;Done!\u0026#34;; 위 Script 는 다른 분이 제작한 Script 에서 vmdk 속성 변경 및 진행 과정 출력 부분을 수정하였습니다.\n(thin 옵션 추가)\nScript 추가 Script 를 추가하기 위해 ESXi 를 ssh 로 접속하고 아래 절차를 수행합니다.\n# vi /bin/vm-copy.sh # chmod 755 /bin/vm-copy.sh # ls -la /bin/vm-copy.sh -rwxr-xr-x 1 root root 640 Oct 23 07:32 /bin/vm-copy.sh VM Clone  VM Clone 을 하기 위해 Clone 할 VM 이 위치한 datastore 경로에 접근합니다.\n이후 vm-copy.sh 명령을 수행합니다.\n(VM Clone 을 위해서는 Source VM 이 Shutdown 상태에서 만 가능합니다.)\n# vm-copy.sh ./centos7.5_temp/ ./clone-test Destination disk format: VMFS thin-provisioned Cloning disk \u0026#39;centos7.5_temp/centos7.5_temp.vmdk\u0026#39;... Clone: 100% done. /bin/vm-copy.sh: line 22: can\u0026#39;t open centos7.5_temp/centos7.5_temp.vmxf: no such file Done! 위와 같이 Clone 이 완료 된 것을 확인 할 수 있습니다.\nVM 등록  ESXi 에서 [가상 시스템] \u0026gt; [VM 생성/등록] 메뉴를 선택합니다.\n [기존 가상 시스템 등록]을 선택합니다.   복제한 VM 의 데이터스토어 경로를 선택합니다.   등록할 VM 을 확인 후 다음을 선택합니다.   등록할 준비가 되었습니다. 완료를 선택합니다.   위와 같이 복제된 VM 이 정상 적으로 등록 된 것을 볼 수 있습니다.   해당 VM 을 [전원 켜기] 를 진행하면 위와 같은 질의가 나옵니다. [복사함] 을 선택하면 해당 VM 이 정상적으로 부팅 될 것입니다.  참고 자료  Script 참조 : @Go Document\n","permalink":"https://chhanz88.github.io/post/2018-10-23-esxi-vmclone/","summary":"ESXi - VM Clone Script  ESXi 는 VMware 에서 제공하는 Hypervisor 입니다.\nVMware 의 모든 가상화 기술을 이용하기 위해서는 vCenter 를 필수로 사용해야합니다.\n하지만 유료 라이센스라 제약 사항이 있습니다.(개인 사용자의 경우ㅠㅠ)\n제일 많이 사용되고 필요로 하는 기능중 하나가 바로 VM Clone 을 하는 기능입니다.\nvCenter 가 없이 ESXi 에서 VM Clone 을 하는 방법을 알아봅시다.\nTest 환경  VMware ESXi-6.7.0-8169922-standard (VMware, Inc.)\nESXi ssh enable  위와 같이 ESXi 에서 [작업] \u0026gt; [서비스] \u0026gt; [SSH 사용] 을 선택하여 ssh 를 활성화합니다.","title":"[VMware] ESXi VM Clone Script"},{"content":"[Linux] CentOS 7 raw device 생성   raw device 생성을 위해 아래 절차를 따라 /etc/udev/rules.d/60-raw.rules 을 수정합니다.\n  udev rules 을 사용하여 raw device 를 생성합니다.  # vi /etc/udev/rules.d/60-raw.rules SCSI Device 를 사용하는 경우,  ACTION==\u0026#34;add|change\u0026#34;, KERNEL==\u0026#34;sdc\u0026#34;, RUN+=\u0026#34;/usr/bin/raw /dev/raw/raw1 %N\u0026#34; multipath device 를 사용하는 경우,  ACTION==\u0026#34;add|change\u0026#34;, ENV{DM_NAME}==\u0026#34;mpath1\u0026#34;, RUN+=\u0026#34;/usr/bin/raw /dev/raw/raw1 %N\u0026#34; LVM device 를 사용하는 경우,  ACTION==\u0026#34;add|change\u0026#34;, ENV{DM_VG_NAME}==\u0026#34;vg_test\u0026#34;, ENV{DM_LV_NAME}==\u0026#34;lv_test1\u0026#34;, RUN+=\u0026#34;/bin/raw /dev/raw/raw1 %N\u0026#34; raw device 의 권한 설정  ACTION==\u0026#34;add\u0026#34;, KERNEL==\u0026#34;raw*\u0026#34;, OWNER=\u0026#34;oracle\u0026#34;, GROUP=\u0026#34;dba\u0026#34;, MODE=\u0026#34;0660\u0026#34; udev rules 갱신  # udevadm trigger --action=add raw device 설정 확인  # raw -qa /dev/raw/raw1: bound to major 8, minor 17 # ls -l /dev/raw total 0 crw-rw---- 1 oracle dba 162, 2 Jan 21 05:21 raw1 ","permalink":"https://chhanz88.github.io/post/2018-10-01-linux_rawdevice/","summary":"[Linux] CentOS 7 raw device 생성   raw device 생성을 위해 아래 절차를 따라 /etc/udev/rules.d/60-raw.rules 을 수정합니다.\n  udev rules 을 사용하여 raw device 를 생성합니다.  # vi /etc/udev/rules.d/60-raw.rules SCSI Device 를 사용하는 경우,  ACTION==\u0026#34;add|change\u0026#34;, KERNEL==\u0026#34;sdc\u0026#34;, RUN+=\u0026#34;/usr/bin/raw /dev/raw/raw1 %N\u0026#34; multipath device 를 사용하는 경우,  ACTION==\u0026#34;add|change\u0026#34;, ENV{DM_NAME}==\u0026#34;mpath1\u0026#34;, RUN+=\u0026#34;/usr/bin/raw /dev/raw/raw1 %N\u0026#34; LVM device 를 사용하는 경우,  ACTION==\u0026#34;add|change\u0026#34;, ENV{DM_VG_NAME}==\u0026#34;vg_test\u0026#34;, ENV{DM_LV_NAME}==\u0026#34;lv_test1\u0026#34;, RUN+=\u0026#34;/bin/raw /dev/raw/raw1 %N\u0026#34; raw device 의 권한 설정  ACTION==\u0026#34;add\u0026#34;, KERNEL==\u0026#34;raw*\u0026#34;, OWNER=\u0026#34;oracle\u0026#34;, GROUP=\u0026#34;dba\u0026#34;, MODE=\u0026#34;0660\u0026#34; udev rules 갱신  # udevadm trigger --action=add raw device 설정 확인  # raw -qa /dev/raw/raw1: bound to major 8, minor 17 # ls -l /dev/raw total 0 crw-rw---- 1 oracle dba 162, 2 Jan 21 05:21 raw1 ","title":"[Linux] CentOS 7 raw device 생성"},{"content":"  회사 기술블로그에 작성한 내용입니다.  오픈소스컨설팅 기술블로그 \n  안녕하세요 오픈소스컨설팅 한철희 과장입니다.  이번에는 개발자, 시스템 운영자 등등 IT 업계에 계신다면 많이 들어본 Docker 에 대해 포스팅 해보려고 합니다.  Docker 의 기초적인 내용부터 활용까지 알아보도록 하겠습니다.    이미지 출처 : flickr     위 사진을 보면 항구에 정박되있는 배가 있습니다. 해외 수출, 수입을 위해 많은 컨테이너를 적재한 모습입니다.  위키백과에서는 컨테이너를 이렇게 정의 하고 있습니다.   컨테이너  (  영어  :  Container  )는 철판으로 만들어져 재사용  이 가능한 규격화된 통으로 화물을 옮길 때 쓴다.  1950년대 상용화 되고 그후 점차 널리 쓰게 되었으며, 짐 꾸리기에 편하고 운반이 쉬우며 보관에도 좋은 점 때문에 전 세계적으로 널리 퍼지게 되었다. [1]      재미있게도 지금부터 알아볼 컨테이너의 기술은 위키백과에 설명된 내용과 비슷합니다! Container 란?   컨테이너란 어플리케이션이 동작하기 위해서 필요한 요소(실행 파일, 어플리케이션 엔진등) 을 패키지화하고 격리 하는 기술을 말합니다.  이를 통해 전체 인프라를 쉽고 빠르게 관리 할 수 있게 됩니다.  아래 동영상은 컨테이너에 대해 쉽게 설명된 동영상입니다.  (자막이 포함되어 있습니다!)     출처 : RedHat Videos    Container 의 작동 원리     컨테이너는 Cgroup 와 namespace 와 같은 커널 기반의 기술을 이용해서 프로세스를 완벽하게 격리하여 분리된 환경을 만들고 실행하도록 만듭니다.  컨테이너의 사용법을 알기전에 Cgroup 과 namespace에 대해 먼저 보도록 하겠습니다.   Cgroup   Cgroup 이란 Control Group 의 약자로, 시스템의 CPU 시간, 시스템 메모리, 네트워크 대역폭과 같은 자원을 제한하고 격리 할 수 있는 커널 기능입니다.  Cgroup   \" CentOS 7 - Cgroup 내용\" # /sys/fs/cgroup # ls -la 합계 0 drwxr-xr-x 13 root root 340 7월 6 23:23 . drwxr-xr-x 5 root root 0 7월 6 23:23 .. drwxr-xr-x 5 root root 0 7월 6 23:23 blkio lrwxrwxrwx 1 root root 11 7월 6 23:23 cpu -\u0026gt; cpu,cpuacct drwxr-xr-x 5 root root 0 7월 6 23:23 cpu,cpuacct lrwxrwxrwx 1 root root 11 7월 6 23:23 cpuacct -\u0026gt; cpu,cpuacct drwxr-xr-x 3 root root 0 7월 6 23:23 cpuset drwxr-xr-x 5 root root 0 7월 6 23:23 devices drwxr-xr-x 3 root root 0 7월 6 23:23 freezer drwxr-xr-x 3 root root 0 7월 6 23:23 hugetlb drwxr-xr-x 5 root root 0 7월 6 23:23 memory lrwxrwxrwx 1 root root 16 7월 6 23:23 net_cls -\u0026gt; net_cls,net_prio drwxr-xr-x 3 root root 0 7월 6 23:23 net_cls,net_prio lrwxrwxrwx 1 root root 16 7월 6 23:23 net_prio -\u0026gt; net_cls,net_prio drwxr-xr-x 3 root root 0 7월 6 23:23 perf_event drwxr-xr-x 3 root root 0 7월 6 23:23 pids drwxr-xr-x 5 root root 0 7월 6 23:23 systemd    위 내용을 보면 Cgroup은 많은 시스템 자원을 제한하고 격리를 할 수 있습니다.  해당 서브시스템에 대한 설명은 아래와 같습니다.     서브시스템  설 명     blkio  Block Device 의 입출력 접근 제한     cpu  CPU에 cgroup 작업 액세스를 제공하기 위해 스케줄러     cpuacct  cgroup의 작업에 사용된 CPU 자원에 대한 보고서를 자동으로 생성     cpuset  개별 CPU (멀티코어 시스템에서) 및 메모리 노드를 cgroup의 작업에 할당    devices  cgroup의 작업 단위로 장치에 대한 액세스를 허용하거나 거부    freezer  cgroup의 작업을 일시 중지하거나 다시 시작    net_cls  특정 cgroup에서 발생하는 패킷을 식별하기 위해 태그를 지정    net_prio  cgroup의 작업에서 생성되는 네트워크 트래픽의 우선순위 지정    memory  cgroup의 작업에서 사용되는 메모리에 대한 제한을 설정         cgroup를 쉽게 이해하기 위해서 devices 서브시스템 자원을 제한하는 것을 보여드리겠습니다. # cd /sys/fs/cgroup/devices # mkdir shell # cd shell/ # ls cgroup.clone_children cgroup.procs devices.deny notify_on_release cgroup.event_control devices.allow devices.list tasks # cat tasks # cat devices.list a *:* rwm \" 모든 권한 활성화 \" # cat tasks # \" 다른 세션 PID: 8403 을 cgroup으로 지정 \" # echo \"8403\" \u0026gt; tasks # cat tasks 8403 # echo \"cgroup test \\ \u0026gt; Hello Cgroup! \\ \u0026gt; end\" \u0026gt; /dev/pts/2 #   # echo $$ 8403 # w 15:24:14 up 54 days, 16:00, 4 users, load average: 0.00, 0.01, 0.05 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root tty1 06 7월18 16days 0.04s 0.04s -bash root pts/0 192.168.0.83 14:38 14.00s 0.04s 0.02s ssh root@192.168.13.131 root pts/2 192.168.0.83 15:24 4.00s 0.00s 0.00s w root pts/3 192.168.13.131 15:23 14.00s 0.01s 0.01s -bash # cgroup test Hello Cgroup! end     위와 같이 pid : 8403 세션에 echo 명령으로 넣은 내용이 나오는 것을 보실 수 있습니다.    Cgroup을 이용해서 시스템 자원을 제한 해보도록 하겠습니다. # echo \"a *:* rwm\" \u0026gt; devices.deny    pid : 8403 세선에 모든 장치에 대해 deny 하는 내용을 선언 합니다. # echo $$ 8403 # echo \"Dent test\" \u0026gt; /dev/pts/2 -bash: /dev/pts/2: 명령을 허용하지 않음 # echo \"Dent test\" \u0026gt; /dev/pts/2 -bash: /dev/pts/2: 명령을 허용하지 않음    위와 같이 pid : 8403 세션에 대해 모든 장치 가 deny 된 것을 확인 할 수 있습니다.  이처럼 프로세스의 장치를 제한하고 격리하는 것이 바로 Cgroup 입니다. namespace   namespace 란, 시스템 리소스를 프로세스의 전용 자원처럼 보이게 하고, 다른 프로세스와 격리시키는 기능입니다.  namespace 에는 총 6가지 namespace 가 있습니다.   Mount namespacaes : 파일시스템의 Mount 를 분할하고 격리합니다.   PID namespacaes : 프로세스를 분할 관리합니다.   Network namespacaes : Network 관련된 정보를 분할 관리합니다.   IPC namespacaes : 프로세스간 통신을 격리합니다.   UTS namespacaes : 독립적인 hostname 할당합니다   USER namespacaes : 독립적인 UID를 할당합니다.      이와 같이 namespace 를 이용하여 각 프로세스를 격리 할 수 있습니다.            간단하게 Mount namespaces 를 통해 namespace에 대해 알아보겠습니다.   # echo $$ 6467 # mkdir /imsi # ls -la /proc/6467/ns/mnt lrwxrwxrwx 1 root root 0 8월 30 16:48 /proc/6467/ns/mnt -\u0026gt; mnt:[4026531840] \"신규 Mount Namespace 생성\" # unshare -m /bin/bash # echo $$ 6523 # mount -t tmpfs tmpfs /imsi # mount | grep imsi tmpfs on /imsi type tmpfs (rw,relatime) # df | grep imsi tmpfs 1941000 0 1941000 0% /imsi # ls -la /proc/6523/ns/mnt lrwxrwxrwx 1 root root 0 8월 30 16:50 /proc/6523/ns/mnt -\u0026gt; mnt:[4026532457] \" 다른 세션에서 Mount 확인 \" # echo $$ 21889 # mount | grep imsi # df | grep imsi      namespace 를 통해 프로세스가 시스템 자원을 전용으로 사용하는것을 확인 할 수 있습니다.            이처럼 컨테이너는 Cgroup 와 namespace 의 기술을 이용한 프로세스 격리 기술입니다.      이를 이용하여 프로세스별로 각각의 가상머신을 운영하는 것과 같은 격리 효과를 볼 수 있는 것입니다.      이것이 바로 컨테이너 입니다!!!          그럼 Docker 는 뭔가요!?       Docker 란, 리눅스 컨테이너 기술을 기반으로 하는 오픈 소스 소프트웨어 플랫폼입니다.  로고를 보면 마치 항구에서 컨테이너를 관리하는 것처럼 Docker는 컨테이너를 환경에 구애받지 않고 애플리케이션을 신속하게 배포 및 확장 있는 플랫폼입니다.    컨테이너에서도 잠깐 설명한것처럼 Docker 는 가상화 환경과 비교를 많이하는데  위 비교 자료 및 성능 자료는 기존에 포스팅된 내용 참고 하시면 됩니다.   [ Docker 발표 및 VM과 성능 비교 ]      이렇게 좋은 Docker 한번 설치해보겠습니다. Docker 설치    Test Information    Test OS 정보 :  CentOS Linux release 7.5.1804 (Core)   Docker Install Version :  docker-ce-18.06.1.ce-3.el7   Install Repository   docker-ce Yum Repository를 등록합니다. # wget https://download.docker.com/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo # yum repolist   Install docker-ce   docker-ce 를 설치합니다. # yum install docker-ce   docker 서비스 시작   docker 서비스 시작 합니다. # systemctl enable docker;systemctl start docker   docker 서비스 확인   docker 가 정상적으로 설치가 되고 문제없이 사용이 가능한지 확인합니다. \" docker 컨테이너 확인 \" # docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES \" docker 컨테이너 실행 \" # docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 9db2ca6ccae0: Pull complete Digest: sha256:4b8ff392a12ed9ea17784bd3c9a8b1fa3299cac44aca35a85c90c5e3c7afacdc Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/engine/userguide/ \" dockerd 정보 \" # docker info Containers: 1 Running: 0 Paused: 0 Stopped: 1 Images: 1 Server Version: 18.06.1-ce Storage Driver: overlay2 Backing Filesystem: xfs Supports d_type: true Native Overlay Diff: true Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: bridge host macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog Swarm: inactive Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: 468a545b9edcd5932818eb9de8e72413e616e86e runc version: 69663f0bd4b60df09991c08812a60108003fa340 init version: fec3683 Security Options: seccomp Profile: default Kernel Version: 3.10.0-693.el7.x86_64 Operating System: CentOS Linux 7 (Core) OSType: linux Architecture: x86_64 CPUs: 2 Total Memory: 3.702GiB Name: container.local ID: FC6G:U5RT:3F6H:4KTL:MX7I:7NND:F42S:FYXI:OFH2:XDSE:DFFC:Z6F2 Docker Root Dir: /var/lib/docker Debug Mode (client): false Debug Mode (server): false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Live Restore Enabled: false      Docker Command   docker 사용을 위해 기본적인 명령어들을 알아보겠습니다.   1) List containers   현재 활성화되거나 중지된 컨테이너 목록을 보는 명령어입니다.  컨테이너의 상태 및 가동 시간등을 보여줍니다.    # docker ps -a    CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES  032759e31d4c hello-world \"/hello\" 10 days ago Exited (0) 10 days ago jovial_lovelace     2) List images   Local 에 저장된 image 목록을 보여줍니다.    # docker images    REPOSITORY TAG IMAGE ID CREATED SIZE  hello-world latest 2cb0d9787c4d 2 months ago 1.85kB     3) Pull image   Public Repository 혹은 Private Repository에 있는 container image 를 Local 로 pull 합니다. ( 일종의 다운로드 )    # docker pull [OPTIONS] NAME[:TAG|@DIGEST]    # docker pull httpd  \"pull http image \"   Using default tag: latest  latest: Pulling from library/httpd  f189db1b88b3: Pull complete  ba2d31d4e2e7: Pull complete  23a65f5e3746: Pull complete  5e8eccbd4bc6: Pull complete  4c145eec18d8: Pull complete  1c74ffd6a8a2: Pull complete  1421f0320e1b: Pull complete  Digest: sha256:8631904c6e92918b6c7dd82b72512714e7fbc3f1a1ace2de17cb2746c401b8fb  Status: Downloaded newer image for httpd:latest  # docker images  REPOSITORY TAG IMAGE ID CREATED SIZE   httpd latest d595a4011ae3 5 days ago 178MB   han0495/hello-world latest 2cb0d9787c4d 2 months ago 1.85kB  hello-world latest 2cb0d9787c4d 2 months ago 1.85kB   4) Push images   Local 에 있는 container image 를 Public Repository 혹은 Private Repository 로 push 합니다. ( 일종의 업로드 )      # docker push [OPTIONS] NAME[:TAG]    # docker push han0495/hello-world  The push refers to repository [ docker.io/han0495/hello-world  ]  ee83fc5847cb: Mounted from library/hello-world  latest: digest: sha256:aca41a608e5eb015f1ec6755f490f3be26b48010b178e78c00eac21ffbe246f1 size: 524  # docker images  REPOSITORY TAG IMAGE ID CREATED SIZE   han0495/hello-world latest 2cb0d9787c4d 2 months ago 1.85kB   hello-world latest 2cb0d9787c4d 2 months ago 1.85kB      위와 같이 docker hub 의 Public Repository 로 push 된 것을 확인 할 수 있습니다.   5) Tag image   Container image 에 태그를 작성합니다.    # docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]    # docker tag hello-world han0495/hello-world  # docker images  REPOSITORY TAG IMAGE ID CREATED SIZE   han0495/hello-world latest 2cb0d9787c4d 2 months ago 1.85kB   hello-world latest 2cb0d9787c4d 2 months ago 1.85kB     6) Container 실행   Container 를 실행 하는 명령 입니다.    # docker run [OPTIONS] IMAGE [COMMAND] [ARG...]    # docker run httpd  AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.17.0.2. Set the 'ServerName' directive globally to suppress this message  AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.17.0.2. Set the 'ServerName' directive globally to suppress this message  [Mon Sep 10 06:43:19.002515 2018] [mpm_event:notice] [pid 1:tid 140466418673536] AH00489: Apache/2.4.34 (Unix) configured -- resuming normal operations  [Mon Sep 10 06:43:19.002612 2018] [core:notice] [pid 1:tid 140466418673536] AH00094: Command line: 'httpd -D FOREGROUND'     7) Container 제거   현재 실행 중이거나 실행이 종료된 Container 를 제거하는 명령 입니다.  현재 실행 중인 Container 를 강제로 제거 하기 위해서는 -f 옵션을 사용합니다.    # docker rm [OPTIONS] CONTAINER [CONTAINER...]    # docker ps -a  CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES   c89632a2eede  httpd \"httpd-foreground\" 2 minutes ago Exited (0) 2 minutes ago fervent_wiles  032759e31d4c hello-world \"/hello\" 10 days ago Exited (0) 10 days ago jovial_lovelace   # docker rm c89632a2eede   c89632a2eede  # docker ps -a  CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES  032759e31d4c hello-world \"/hello\" 10 days ago Exited (0) 10 days ago jovial_lovelace     8) Containers Image 삭제   Local 에 저장된 Container Image 를 삭제합니다.    # docker rmi [OPTIONS] IMAGE [IMAGE...]    # docker images  REPOSITORY TAG IMAGE ID CREATED SIZE  httpd latest d595a4011ae3 5 days ago 178MB   hello-world latest 2cb0d9787c4d 2 months ago 1.85kB   han0495/hello-world latest 2cb0d9787c4d 2 months ago 1.85kB   # docker rmi hello-world   Untagged: hello-world:latest  Untagged: hello-world@sha256:4b8ff392a12ed9ea17784bd3c9a8b1fa3299cac44aca35a85c90c5e3c7afacdc  # docker images  REPOSITORY TAG IMAGE ID CREATED SIZE  httpd latest d595a4011ae3 5 days ago 178MB  han0495/hello-world latest 2cb0d9787c4d 2 months ago 1.85kB      이처럼 Docker 에서 사용되는 기본적인 명령들을 확인하였습니다.  추가적인 명령들은 아래 문서를 참고합니다.  ( https://docs.docker.com/engine/reference/commandline/cli/  ) Docker를 이용해서 Web 서비스를 실행해보자!   위 명령어 예제에서 httpd image 를 이용하여 contanier를 실행하는 예제를 보여드렸습니다.  그런데 container 만 작동된다고 Web 서비스가 구동되는 것은 아니죠!  docker 를 이용해서 Web 서비스를 해보도록 하겠습니다.    먼저, httpd container image 를 pull 합니다. # docker pull httpd Using default tag: latest latest: Pulling from library/httpd Digest: sha256:8631904c6e92918b6c7dd82b72512714e7fbc3f1a1ace2de17cb2746c401b8fb Status: Image is up to date for httpd:latest # docker images REPOSITORY TAG IMAGE ID CREATED SIZE httpd latest d595a4011ae3 5 days ago 178MB han0495/hello-world latest 2cb0d9787c4d 2 months ago 1.85kB    http container 를 위에서 배운것처럼 실행해봅니다.    Container 가 Foreground 로 작동하면서 Shell 을 사용을 못할 뿐더러, Shell 이 종료가 되면 httpd Container 도 중지가 됩니다.  위와 같이 되면, 전혀 서비스에 적용 할 수가 없습니다.  그리하여 아래와 같이 background 로 container 를 실행하면 됩니다. # docker run -d httpd 3c9764e9b79a058b0dedda07312679d6cafc0779c3e66ae04d9d1034a2b29ee1 # docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3c9764e9b79a httpd \"httpd-foreground\" 11 seconds ago Up 10 seconds 80/tcp stupefied_rama    위와 같이 Shell 에서 다른 명령도 가능하고 서비스가 계속 실행되는 것을 확인 할 수 있습니다.  그럼 실제로 서비스가 작동하는지 확인해 보겠습니다. # curl http://127.0.0.1 curl: (7) Failed connect to 127.0.0.1; 연결이 거부됨    서비스가 안되고 있습니다! 이유가 뭘까요?    현재 container 가 어떤 상황인지 이해 하시면 왜 네트워크가 안되는지 이해가 쉽습니다.  위에 도식화된 내용은 Host OS 위에 docker 엔진이 설치가 되고 container 들의 네트워크가 어떤식으로 연결되어 있는지 쉽게 볼 수 있습니다.  지금 보면 HTTP 는 Host Network 와 연결이 안되어 있습니다. 이러면 docker 내부에서 container 간 통신은 되지만 docker 외부와 통신이 불가능합니다. 그래서 외부 서비스가 안되는 것입니다.  그럼 서비스가 되기 위해서는 아래와 같은 연결이 필요합니다. 그 연결은 port mapping 을 통해 진행합니다.    그리하면 위와 같은 구성이 될 것입니다. 실제로 적용해보겠습니다. # docker stop 3c9764e9b79a \" 기존에 실행중이던 docker 중지 \" 3c9764e9b79a # docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3c9764e9b79a httpd \"httpd-foreground\" 18 minutes ago Exited (0) 3 seconds ago stupefied_raman # docker run -d -p 80:80 httpd dee5fb60c083564d6095b1b9811b3e634c017caf9788f5fce57a0dcb309e4e76 # docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dee5fb60c083 httpd \"httpd-foreground\" 3 seconds ago Up 2 seconds 0.0.0.0:80-\u0026gt;80/tcp goofy_sammet 3c9764e9b79a httpd \"httpd-foreground\" 19 minutes ago Exited (0) 18 seconds ago stupefied_raman # curl http://127.0.0.1 \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;It works!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;    위와 같이 포트 80 을 통해 외부로 서비스를 하는 것을 확인 할 수 있습니다.  웹 브라우져에서 확인해보겠습니다.    이렇게 쉽게 Web 서비스를 구성 할 수 있습니다.    하지만 이런 Web Page 를 쓸수는 없습니다. Web Page 를 바꿔보겠습니다. # docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dee5fb60c083 httpd \"httpd-foreground\" 3 seconds ago Up 2 seconds 0.0.0.0:80-\u0026gt;80/tcp goofy_sammet 3c9764e9b79a httpd \"httpd-foreground\" 19 minutes ago Exited (0) 18 seconds ago stupefied_raman # docker exec -ti dee5fb60c083 /bin/bash \"container 내부로 들어가서 http index.html 을 수정합니다.\" root@dee5fb60c083:/usr/local/apache2# root@dee5fb60c083:/usr/local/apache2# ls bin build cgi-bin conf error htdocs icons include logs modules root@dee5fb60c083:/usr/local/apache2# cd htdocs/ root@dee5fb60c083:/usr/local/apache2/htdocs# ls index.html root@dee5fb60c083:/usr/local/apache2/htdocs# cat index.html \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;It works!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; root@dee5fb60c083:/usr/local/apache2/htdocs# echo \"\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Docker Test Page\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\" \u0026gt; index.html root@dee5fb60c083:/usr/local/apache2/htdocs# cat index.html \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Docker Test Page\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; root@dee5fb60c083:/usr/local/apache2/htdocs# exit exit # curl http://192.168.13.131 \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Docker Test Page\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;      이처럼 container 내부의 index.html 을 수정하면 Web Page를 변경 할 수 있습니다.    그럼 이제 완벽하게 Web 서비스를 할 수 있게 되었습니다.  하지만 이런 방식은 운영자 입장에서 보면 container 가 종료가 되거나, 삭제가 되면 매번 container를 실행하기 위해 Port 를 결정해서 입력해야되고, 수정된 index 파일을 다시 수정해야됩니다.  또한 운영하면서 발생한 로그 및 기타 증분 데이터는 container 가 종료가 되면 전부 삭제가 됩니다.    이런 귀차니즘을 해결하기 위해서  두가지의 방법을 적용해야 합니다.    바로      DockerFile      Docker-compose      입니다. DockerFile 이란?   DockerFile 이란, 나만의 Container image를 Build 할 수 있게 해주는 Docker image 파일 입니다.  Docker Hub 에 없는 이미지도 DockerFile 을 이용하면 특별한 image 도 생성 할 수 있습니다.  DockerFile   # cat dockerfile FROM httpd:latest MAINTAINER chhan \u0026lt;chhan@osci.kr\u0026gt; RUN echo \"\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Docker File Test Page\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\" \u0026gt; /usr/local/apache2/htdocs/index.html EXPOSE 80    간단히 위에 사용된 구문에 대해 설명하겠습니다.      FROM    Docker Hub 에서 어떤 이미지를 가지고와서 작업할지 선언합니다. 작성법은 \u0026lt;이미지명\u0026gt;:\u0026lt;태그\u0026gt; 로 작성합니다.    MAINTAINER    DockerFile 제작한 사람의 정보를 기입합니다.    RUN    docker image 가 실행되고 container 내에서 실행될 명령어입니다. 해당 내용은 적으면 적을수록 container label 이 적게 생성되어 image 를 compact 하게 생성 할 수 있습니다.    EXPOSE    Host 에 연결될 Port를 지정합니다.  위와 같은 DockerFile 구문을 자세히 확인하려면 아래 문서를 참고하시면 됩니다.  ( https://docs.docker.com/engine/reference/builder/  )  그럼 작성한 DockerFile 을 Build 하고 실행해 보겠습니다. # pwd /root/http # ls dockerfile # docker build -t myhttpd . Sending build context to Docker daemon 2.048kB Step 1/4 : FROM httpd:latest ---\u0026gt; d595a4011ae3 Step 2/4 : MAINTAINER chhan \u0026lt;chhan@osci.kr\u0026gt; ---\u0026gt; Running in 176d94e63272 Removing intermediate container 176d94e63272 ---\u0026gt; 282d30eba8fe Step 3/4 : RUN echo \"\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Docker File Test Page\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\" \u0026gt; /usr/local/apache2/htdocs/index.html ---\u0026gt; Running in 2f16f1ef9d1c Removing intermediate container 2f16f1ef9d1c ---\u0026gt; df135d6e6dbd Step 4/4 : EXPOSE 80 ---\u0026gt; Running in 12cdc41b8546 Removing intermediate container 12cdc41b8546 ---\u0026gt; 52c07f2bfb38 Successfully built 52c07f2bfb38 Successfully tagged myhttpd:latest # docker images REPOSITORY TAG IMAGE ID CREATED SIZE myhttpd latest 52c07f2bfb38 3 seconds ago 178MB httpd latest d595a4011ae3 5 days ago 178MB han0495/hello-world latest 2cb0d9787c4d 2 months ago 1.85kB # docker run -d -p 80:80 --name=myweb myhttpd 1afd260265923828f88eeae9bc7083985b66eef2d9c588d35ee08e05198470a0 # docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1afd26026592 myhttpd \"httpd-foreground\" 5 seconds ago Up 5 seconds 0.0.0.0:80-\u0026gt;80/tcp myweb # curl http://192.168.13.131 \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Docker File Test Page\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;    이처럼 DockerFile 을 통해 작성한 container image 를 통해 매번 index File 이 수정된 Web 서비스를 실행 할 수 있습니다. docker-compose 로 Container 통합 관리   docker-compose 란, 한번에 여러개의 container 을 통합 관리 할 수 있게 하는 툴입니다.    주로 서비스는 하나만으로 작동하는 것은 없습니다.  예를 들면  wordpress 같은 것이 있습니다.   DB 서비스와 wordpress  서비스가 동시에 실행되고 서로 연결되어 있습니다.  위와 같은 서비스를 편하게 통합 관리하기 위해 docker-compose 를 사용하는 것입니다.    그럼 docker-compose 를 사용해보겠습니다.   docker-compose 설치   아래와 같이 docker-compose 명령어를 설치해야합니다. # curl -L \"https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 617 0 617 0 0 558 0 --:--:-- 0:00:01 --:--:-- 559 100 11.2M 100 11.2M 0 0 2186k 0 0:00:05 0:00:05 --:--:-- 3668k # chmod +x /usr/local/bin/docker-compose # ls -l /usr/local/bin/docker-compose -rwxr-xr-x 1 root root 11750136 9월 10 17:26 /usr/local/bin/docker-compose # docker-compose --version docker-compose version 1.22.0, build f46880fe     docker-compose 사용법   아래 예제는 DB 서비스와 wordpress 서비스를 구동하는 docker-compose 파일입니다.  해당 docker-compose.yml 을 이용해서 wordpress 서비스를 구동해 보겠습니다.  docker-compose.yml   version: '3.3' services: db: image: mysql:5.7 volumes: - /var/lib/mysql:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: passwordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: depends_on: - db image: wordpress:latest ports: - \"8000:80\" restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress    * 위 예제는 docker docs 에서 발췌 했습니다. ( https://docs.docker.com/compose/overview/  )  * docker-compose 에 선언되는 각각의 environment는 아래 URL과 같이 docker docs 에서 확인이 가능합니다.  ( https://docs.docker.com/samples/library/mysql/  )    이제 위 docker-compose.yml 을 실행해보겠습니다. # docker-compose -f docker-compose.yml up -d (-f [File] , -d background 실행)     # ls -la /root/wordpress/docker-compose.yml -rw-r--r-- 1 root root 537 9월 10 17:35 /root/wordpress/docker-compose.yml # docker-compose -f docker-compose.yml up -d Creating network \"wordpress_default\" with the default driver Pulling db (mysql:5.7)... 5.7: Pulling from library/mysql 802b00ed6f79: Pull complete 30f19a05b898: Pull complete 3e43303be5e9: Pull complete 94b281824ae2: Pull complete 51eb397095b1: Pull complete 54567da6fdf0: Pull complete bc57ddb85cce: Pull complete c7c0a9c25d8a: Pull complete cce6c47ac3fc: Pull complete 499b9c7376c8: Pull complete 6c5e08e005ea: Pull complete Digest: sha256:1d8f471c7e2929ee1e2bfbc1d16fc8afccd2e070afed24805487e726ce601a6d Status: Downloaded newer image for mysql:5.7 Pulling wordpress (wordpress:latest)... latest: Pulling from library/wordpress 802b00ed6f79: Already exists 59f5a5a895f8: Pull complete 6898b2dbcfeb: Pull complete 8e0903aaa47e: Pull complete 2961af1e196a: Pull complete 71f7016f79a0: Pull complete 5e1a48e5719c: Pull complete 7ae5291984f3: Pull complete 725b65166f31: Pull complete 3823a607a5d4: Pull complete 1bcfa4198e39: Pull complete f1c79da21110: Pull complete 18903f439956: Pull complete 5eda25fffde3: Pull complete 3800dac98824: Pull complete 951fbb644962: Pull complete 5b91123e33c5: Pull complete 71250bb070e7: Pull complete 0363e75875b5: Pull complete 3bcb3cbf244a: Pull complete Digest: sha256:e30aed2d17b33758544f0eaebee763a452b41ff5bc926d723566338b0137dd81 Status: Downloaded newer image for wordpress:latest Creating wordpress_db_1 ... done Creating wordpress_wordpress_1 ... done # docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------- wordpress_db_1 docker-entrypoint.sh mysqld Up 3306/tcp, 33060/tcp wordpress_wordpress_1 docker-entrypoint.sh apach ... Up 0.0.0.0:8000-\u0026gt;80/tcp [root@container wordpress]# docker-compose top wordpress_db_1 UID PID PPID C STIME TTY TIME CMD ------------------------------------------------------------- polkitd 10737 10716 0 17:41 ? 00:00:00 mysqld wordpress_wordpress_1 UID PID PPID C STIME TTY TIME CMD ------------------------------------------------------------------------ root 10997 10977 0 17:41 ? 00:00:00 apache2 -DFOREGROUND 33 14530 10997 0 17:41 ? 00:00:00 apache2 -DFOREGROUND 33 14531 10997 0 17:41 ? 00:00:00 apache2 -DFOREGROUND 33 14533 10997 0 17:41 ? 00:00:00 apache2 -DFOREGROUND 33 14534 10997 0 17:41 ? 00:00:00 apache2 -DFOREGROUND 33 14535 10997 0 17:41 ? 00:00:00 apache2 -DFOREGROUND    이처럼 명령어 한줄로 두 종류의 서비스를 한번에 실행 할 수 있었습니다.  정상적으로 서비스가 작동하는지 확인해볼까요?    정상적으로 실행된 것을 확인 할 수 있었습니다.    지금까지 배운 명령어를 통해 docker-compose 가 어떤 내용을 실행했는지 한번 더 확인해 보겠습니다.   # docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 25c5e75bb07d wordpress:latest \"docker-entrypoint.s…\" 4 minutes ago Up 4 minutes 0.0.0.0:8000-\u0026gt;80/tcp wordpress_wordpress_1 f52a5d2495e4 mysql:5.7 \"docker-entrypoint.s…\" 4 minutes ago Up 4 minutes 3306/tcp, 33060/tcp wordpress_db_1 # docker images REPOSITORY TAG IMAGE ID CREATED SIZE myhttpd latest 52c07f2bfb38 About an hour ago 178MB wordpress latest 63b422244491 2 days ago 409MB mysql 5.7 563a026a1511 5 days ago 372MB httpd latest d595a4011ae3 5 days ago 178MB han0495/hello-world latest 2cb0d9787c4d 2 months ago 1.85k   Local 에 없던 image를 자동으로 Download 했습니다.  docker container 를 실행했습니다.  /var/lib/mysql 를 영구적 볼륨으로 할당하였습니다.  docker 내부 포트를 외부 포트와 Mapping 하였습니다.  container 를 순서대로 실행했습니다.      간단히 만든 docker-compose.yml 인데....  명령어 한줄인데....  많은 작업을 한번에 해줬습니다.  잘만든 docker-compose.yml ....  운영자들은 행복해합니다. ^ㅡ^     마치며   지금까지 docker 에 대한 기초부터 응용법까지 간단히 포스팅해보았습니다.  이 포스팅이 많은 운영자, 개발자, 엔지니어 분들이 도움이 되셨으면 합니다.      Docker 의 원리와 구조를 잘 이해하고, 고객사의 서비스 환경에 맞는 container 를 구성하여 쉽고 편한 운영을 할 수 있는 좋은 세상이 될 수 있었으면 합니다!!     참고 자료   Cgroup : https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/6/html/resource_management_guide/ch01   namespace : https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/overview_of_containers_in_red_hat_systems/introduction_to_linux_containers   docker : https://docs.docker.com/            --! ","permalink":"https://chhanz88.github.io/post/2018-09-10-45749387/","summary":"회사 기술블로그에 작성한 내용입니다.  오픈소스컨설팅 기술블로그 \n  안녕하세요 오픈소스컨설팅 한철희 과장입니다.  이번에는 개발자, 시스템 운영자 등등 IT 업계에 계신다면 많이 들어본 Docker 에 대해 포스팅 해보려고 합니다.  Docker 의 기초적인 내용부터 활용까지 알아보도록 하겠습니다.    이미지 출처 : flickr     위 사진을 보면 항구에 정박되있는 배가 있습니다. 해외 수출, 수입을 위해 많은 컨테이너를 적재한 모습입니다.  위키백과에서는 컨테이너를 이렇게 정의 하고 있습니다.","title":"[Docker] Docker 이해하기"},{"content":"RHEL / CentOS 7 Bonding Currently Active Slave 변경  Bonding(본딩) 는 이중화 구성을 통한 빠른 장애 대응 및 네트워크 대역폭 증가에 매우 유용합니다.\n아래 절차는 본딩 인터페이스를 중단하지 않고, Slave 인터페이스로 전환하는 방법을 설명하려고 합니다.\n# cat /proc/net/bonding/bond0 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) Bonding Mode: fault-tolerance (active-backup) Primary Slave: None Currently Active Slave: eth1 MII Status: up MII Polling Interval (ms): 100 Up Delay (ms): 0 Down Delay (ms): 0 Slave Interface: eth1 MII Status: up Speed: 1000 Mbps Duplex: full Link Failure Count: 0 Permanent HW addr: 00:2a:4a:16:01:63 Slave queue ID: 0 Slave Interface: eth2 MII Status: up Speed: 1000 Mbps Duplex: full Link Failure Count: 0 Permanent HW addr: 00:2a:4a:16:01:64 Slave queue ID: 0 위와 같이 본딩 구성이 되어 있습니다.\nCurrently Active Slave 를 eth2 인터페이스로 변경  ifenslave 명령을 통해 Slave 인터페이스를 추가, 제거 변경을 할 수가 있습니다.\n# ifenslave -c bond0 eth2 # cat /proc/net/bonding/bond0 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) Bonding Mode: fault-tolerance (active-backup) Primary Slave: None Currently Active Slave: eth2 MII Status: up MII Polling Interval (ms): 100 Up Delay (ms): 0 Down Delay (ms): 0 Slave Interface: eth1 MII Status: up Speed: 1000 Mbps Duplex: full Link Failure Count: 0 Permanent HW addr: 00:2a:4a:16:01:63 Slave queue ID: 0 Slave Interface: eth2 MII Status: up Speed: 1000 Mbps Duplex: full Link Failure Count: 0 Permanent HW addr: 00:2a:4a:16:01:64 Slave queue ID: 0 위와 같이 eth2 로 변경된 것을 확인 할 수 있습니다.\nSlave 인터페이스 제거  아래와 같이 bond device 에서 Slave 인터페이스를 제거 할 수 있습니다.\n# ifenslave -d bond0 eth1 # cat /proc/net/bonding/bond0 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) Bonding Mode: fault-tolerance (active-backup) Primary Slave: None Currently Active Slave: eth2 MII Status: up MII Polling Interval (ms): 100 Up Delay (ms): 0 Down Delay (ms): 0 Slave Interface: eth2 MII Status: up Speed: 1000 Mbps Duplex: full Link Failure Count: 0 Permanent HW addr: 00:2a:4a:16:01:64 Slave queue ID: 0 본딩에서 Slave 의 포트가 장애가 났거나 포트 변경을 할때 손쉽게 사용 할 수 있는 방법입니다.\nSlave 인터페이스 추가  아래와 같이 bond device 에 Slave 인터페이스를 추가 할 수 있습니다.\n# ifenslave bond0 eth1 # cat /proc/net/bonding/bond0 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) Bonding Mode: fault-tolerance (active-backup) Primary Slave: None Currently Active Slave: eth2 MII Status: up MII Polling Interval (ms): 100 Up Delay (ms): 0 Down Delay (ms): 0 Slave Interface: eth2 MII Status: up Speed: 1000 Mbps Duplex: full Link Failure Count: 0 Permanent HW addr: 00:2a:4a:16:01:64 Slave queue ID: 0 Slave Interface: eth1 MII Status: up Speed: 1000 Mbps Duplex: full Link Failure Count: 0 Permanent HW addr: 00:2a:4a:16:01:63 Slave queue ID: 0 위와 같이 정상적으로 추가된 것을 확인 할 수 있습니다.\n참고 자료  Red Hat Community : How to test nic bonding? - https://access.redhat.com/discussions/669983\n","permalink":"https://chhanz88.github.io/post/2018-08-16-linux-change-slave-bond-device/","summary":"RHEL / CentOS 7 Bonding Currently Active Slave 변경  Bonding(본딩) 는 이중화 구성을 통한 빠른 장애 대응 및 네트워크 대역폭 증가에 매우 유용합니다.\n아래 절차는 본딩 인터페이스를 중단하지 않고, Slave 인터페이스로 전환하는 방법을 설명하려고 합니다.\n# cat /proc/net/bonding/bond0 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) Bonding Mode: fault-tolerance (active-backup) Primary Slave: None Currently Active Slave: eth1 MII Status: up MII Polling Interval (ms): 100 Up Delay (ms): 0 Down Delay (ms): 0 Slave Interface: eth1 MII Status: up Speed: 1000 Mbps Duplex: full Link Failure Count: 0 Permanent HW addr: 00:2a:4a:16:01:63 Slave queue ID: 0 Slave Interface: eth2 MII Status: up Speed: 1000 Mbps Duplex: full Link Failure Count: 0 Permanent HW addr: 00:2a:4a:16:01:64 Slave queue ID: 0 위와 같이 본딩 구성이 되어 있습니다.","title":"[Linux] Bonding Network Currently Active Slave 변경"},{"content":"[PowerKVM] PowerKVM Install Guide  IBM PowerKVM Install Guide 입니다.\n 작성일 : 2018.08.01 배포일 : 2019.01.14  ","permalink":"https://chhanz88.github.io/post/2018-08-01-linux-powerkvm-install-guide/","summary":"[PowerKVM] PowerKVM Install Guide  IBM PowerKVM Install Guide 입니다.\n 작성일 : 2018.08.01 배포일 : 2019.01.14  ","title":"[PowerKVM] PowerKVM Install Guide"},{"content":"[Linux] CentOS 7 네트워크 Bonding 구성  CentOS 7 에서 bonding 구성을 위해서는 아래와 같은 절차가 필요합니다.\n CentOS 7 에서는 bonding 모듈이 기본적으로 로드가 되어 있지 않습니다. 아래 명령을 통해 boning 모듈을 로드합니다.  # modprobe --first-time bonding Bond Interface 생성 Bond Interface 생성을 하기위해서는 /etc/sysconfig/network-scripts/ 의 ifcfg-bond0 파일을 생성해야됩니다.  # cat /etc/sysconfig/network-scripts/ifcfg-bond0 DEVICE=bond0 NAME=bond0 TYPE=Bond IPADDR=10.0.0.1 NETMASK=255.255.255.0 ONBOOT=yes BOOTPROTO=none BONDING_OPTS=\u0026#34;mode=1 miimon=100\u0026#34; SLAVE Interface 생성 bond Interface 의 SLAVE Interface 파일을 생성합니다.\n/etc/sysconfig/network-scripts/ 의 ifcfg-eth1 과 ifcfg-eth2 를 수정합니다.  # cat ifcfg-eth1 DEVICE=eth1 NAME=eth1 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond0 SLAVE=yes # cat ifcfg-eth2 DEVICE=eth2 NAME=eth2 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond0 SLAVE=yes NetworkManager disable \u0026amp;\u0026amp; network restart network 재시작을 합니다.  # systemctl disable NetworkManager # systemctl stop NetworkManager # systemctl restart network # ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:2a:4a:16:01:5b brd ff:ff:ff:ff:ff:ff inet 192.168.13.63/16 brd 192.168.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::22a:4aff:fe16:15b/64 scope link valid_lft forever preferred_lft forever 3: eth1: \u0026lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000 link/ether 00:2a:4a:16:01:63 brd ff:ff:ff:ff:ff:ff 4: eth2: \u0026lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000 link/ether 00:2a:4a:16:01:63 brd ff:ff:ff:ff:ff:ff 5: bond0: \u0026lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 00:2a:4a:16:01:63 brd ff:ff:ff:ff:ff:ff inet 10.0.0.1/24 brd 10.0.0.255 scope global bond0 valid_lft forever preferred_lft forever inet6 fe80::22a:4aff:fe16:163/64 scope link valid_lft forever preferred_lft forever 위와 같이 bonding 구성이 된것을 확인 할 수 있습니다.\n참고자료  bonding Option : https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/sec-using_channel_bonding\n","permalink":"https://chhanz88.github.io/post/2018-03-02-linux-create-bond-device/","summary":"[Linux] CentOS 7 네트워크 Bonding 구성  CentOS 7 에서 bonding 구성을 위해서는 아래와 같은 절차가 필요합니다.\n CentOS 7 에서는 bonding 모듈이 기본적으로 로드가 되어 있지 않습니다. 아래 명령을 통해 boning 모듈을 로드합니다.  # modprobe --first-time bonding Bond Interface 생성 Bond Interface 생성을 하기위해서는 /etc/sysconfig/network-scripts/ 의 ifcfg-bond0 파일을 생성해야됩니다.  # cat /etc/sysconfig/network-scripts/ifcfg-bond0 DEVICE=bond0 NAME=bond0 TYPE=Bond IPADDR=10.0.0.1 NETMASK=255.255.255.0 ONBOOT=yes BOOTPROTO=none BONDING_OPTS=\u0026#34;mode=1 miimon=100\u0026#34; SLAVE Interface 생성 bond Interface 의 SLAVE Interface 파일을 생성합니다.","title":"[Linux] CentOS 7 네트워크 Bonding 구성"},{"content":"AIX - DISK FORMAT  주로 AIX 에서는 diag 를 통해 hdisk format 을 진행하고 데이터 삭제 검증을 합니다.\n하지만 동시에 여러개의 hdisk 를 format 을 진행 할 수는 없습니다.\n아래와 같이 명령어를 이용하면 동시에 hdisk format 이 가능합니다.\n/usr/lpp/diagnostics/bin/uformat -d hdisk1 -c -o format 동시에 여러 hdisk format 진행 하는 Command lspv | grep None | awk \u0026#39;{print \u0026#34;/usr/lpp/diagnostics/bin/uformat -d \u0026#34;$1\u0026#34; -c -o format \u0026amp;\u0026#34;}\u0026#39; | sh -x ","permalink":"https://chhanz88.github.io/post/2018-02-21-aixuformat/","summary":"AIX - DISK FORMAT  주로 AIX 에서는 diag 를 통해 hdisk format 을 진행하고 데이터 삭제 검증을 합니다.\n하지만 동시에 여러개의 hdisk 를 format 을 진행 할 수는 없습니다.\n아래와 같이 명령어를 이용하면 동시에 hdisk format 이 가능합니다.\n/usr/lpp/diagnostics/bin/uformat -d hdisk1 -c -o format 동시에 여러 hdisk format 진행 하는 Command lspv | grep None | awk \u0026#39;{print \u0026#34;/usr/lpp/diagnostics/bin/uformat -d \u0026#34;$1\u0026#34; -c -o format \u0026amp;\u0026#34;}\u0026#39; | sh -x ","title":"[AIX] hdisk format - diag command"},{"content":"[RHEL] Local Yum Repository 만들기  Red Hat Enterprise Linux 설치 ISO 및 물리 DVD 가 있다면 외부 rhn 을 사용하지 않고,\nbase yum Repository 를 만들 수 있습니다.\n구성 절차는 아래와 같습니다.\n DVD 혹은 ISO 파일 mount 진행   ISO 파일의 경우, 시스템에 Upload 진행합니다.  // DVD # mount /dev/sr0 /mnt // ISO # mount /root/rhel-server-7.5-x86_64-dvd.iso /mnt Local Disk 로 DVD 혹은 ISO 파일 데이터 복사  # cp -rpH /mnt /home/repo # vi /etc/yum.repos.d/media.repo  /etc/yum.repos.d/media.repo 내용  [media] name=media baseurl=file:///home/repo enable=1 gpgcheck=0 yum Repository 인식 Repository 가 정상적으로 인식하는지 확인합니다.  # yum repolist Loaded plugins: product-id, search-disabled-repos, subscription-manager This system is not registered with an entitlement server. You can use subscription-manager to register. media | 4.3 kB 00:00:00 (1/2): media/group_gz | 145 kB 00:00:00 (2/2): media/primary_db | 4.1 MB 00:00:00 repo id repo name status media media 5,099 repolist: 5,099 yum Test yum 이 정상적으로 작동되는지 확인 합니다.  # yum install gcc Loaded plugins: product-id, search-disabled-repos, subscription-manager This system is not registered with an entitlement server. You can use subscription-manager to register. Resolving Dependencies --\u0026gt; Running transaction check ---\u0026gt; Package gcc.x86_64 0:4.8.5-28.el7 will be installed --\u0026gt; Processing Dependency: cpp = 4.8.5-28.el7 for package: gcc-4.8.5-28.el7.x86_64 --\u0026gt; Processing Dependency: glibc-devel \u0026gt;= 2.2.90-12 for package: gcc-4.8.5-28.el7.x86_64 --\u0026gt; Processing Dependency: libmpc.so.3()(64bit) for package: gcc-4.8.5-28.el7.x86_64 --\u0026gt; Processing Dependency: libmpfr.so.4()(64bit) for package: gcc-4.8.5-28.el7.x86_64 --\u0026gt; Running transaction check ---\u0026gt; Package cpp.x86_64 0:4.8.5-28.el7 will be installed ---\u0026gt; Package glibc-devel.x86_64 0:2.17-222.el7 will be installed --\u0026gt; Processing Dependency: glibc-headers = 2.17-222.el7 for package: glibc-devel-2.17-222.el7.x86_64 --\u0026gt; Processing Dependency: glibc-headers for package: glibc-devel-2.17-222.el7.x86_64 ---\u0026gt; Package libmpc.x86_64 0:1.0.1-3.el7 will be installed ---\u0026gt; Package mpfr.x86_64 0:3.1.1-4.el7 will be installed --\u0026gt; Running transaction check ---\u0026gt; Package glibc-headers.x86_64 0:2.17-222.el7 will be installed --\u0026gt; Processing Dependency: kernel-headers \u0026gt;= 2.2.1 for package: glibc-headers-2.17-222.el7.x86_64 --\u0026gt; Processing Dependency: kernel-headers for package: glibc-headers-2.17-222.el7.x86_64 --\u0026gt; Running transaction check ---\u0026gt; Package kernel-headers.x86_64 0:3.10.0-862.el7 will be installed --\u0026gt; Finished Dependency Resolution  Dependencies Resolved  ==================================================================================================================================  Package Arch Version Repository Size ================================================================================================================================== Installing:  gcc x86_64 4.8.5-28.el7 media 16 M Installing for dependencies:  cpp x86_64 4.8.5-28.el7 media 5.9 M  glibc-devel x86_64 2.17-222.el7 media 1.1 M  glibc-headers x86_64 2.17-222.el7 media 678 k  kernel-headers x86_64 3.10.0-862.el7 media 7.1 M  libmpc x86_64 1.0.1-3.el7 media 51 k  mpfr x86_64 3.1.1-4.el7 media 203 k  Transaction Summary ================================================================================================================================== Install 1 Package (+6 Dependent packages)  Total download size: 31 M Installed size: 60 M Is this ok [y/d/N]: y Downloading packages: ---------------------------------------------------------------------------------------------------------------------------------- Total 43 MB/s | 31 MB 00:00:00 Running transaction check Running transaction test Transaction test succeeded Running transaction  Installing : mpfr-3.1.1-4.el7.x86_64 1/7  Installing : libmpc-1.0.1-3.el7.x86_64 2/7  Installing : cpp-4.8.5-28.el7.x86_64 3/7  Installing : kernel-headers-3.10.0-862.el7.x86_64 4/7  Installing : glibc-headers-2.17-222.el7.x86_64 5/7  Installing : glibc-devel-2.17-222.el7.x86_64 6/7  Installing : gcc-4.8.5-28.el7.x86_64 7/7  Verifying : gcc-4.8.5-28.el7.x86_64 1/7  Verifying : cpp-4.8.5-28.el7.x86_64 2/7  Verifying : mpfr-3.1.1-4.el7.x86_64 3/7  Verifying : glibc-devel-2.17-222.el7.x86_64 4/7  Verifying : kernel-headers-3.10.0-862.el7.x86_64 5/7  Verifying : glibc-headers-2.17-222.el7.x86_64 6/7  Verifying : libmpc-1.0.1-3.el7.x86_64 7/7  Installed:  gcc.x86_64 0:4.8.5-28.el7  Dependency Installed:  cpp.x86_64 0:4.8.5-28.el7 glibc-devel.x86_64 0:2.17-222.el7 glibc-headers.x86_64 0:2.17-222.el7  kernel-headers.x86_64 0:3.10.0-862.el7 libmpc.x86_64 0:1.0.1-3.el7 mpfr.x86_64 0:3.1.1-4.el7  Complete! 정상적으로 작동하는 것을 확인 할 수 있습니다.\n참고자료  Red Hat Document : https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/6/html/installation_guide/sn-switching-to-gui-login\n","permalink":"https://chhanz88.github.io/post/2018-02-02-rhel-create-yum-repo/","summary":"[RHEL] Local Yum Repository 만들기  Red Hat Enterprise Linux 설치 ISO 및 물리 DVD 가 있다면 외부 rhn 을 사용하지 않고,\nbase yum Repository 를 만들 수 있습니다.\n구성 절차는 아래와 같습니다.\n DVD 혹은 ISO 파일 mount 진행   ISO 파일의 경우, 시스템에 Upload 진행합니다.  // DVD # mount /dev/sr0 /mnt // ISO # mount /root/rhel-server-7.5-x86_64-dvd.iso /mnt Local Disk 로 DVD 혹은 ISO 파일 데이터 복사  # cp -rpH /mnt /home/repo # vi /etc/yum.","title":"[RHEL] Local Yum Repository 만들기"},{"content":"Create a raw Logical Volume for Oracle ASM On AIX  Raw Device 의 용량 산정은 VG 의 PP Size 를 기준으로 합니다.\n# lsvg RAWVG VOLUME GROUP: RAWVG VG IDENTIFIER: 00c25b6700004c00000001399f63219x VG STATE: active PP SIZE: 64 megabyte(s) VG PERMISSION: read/write TOTAL PPs: 30 (1920 megabytes) MAX LVs: 4096 FREE PPs: 6 (384 megabytes) LVs: 3 USED PPs: 24 (1536 megabytes) OPEN LVs: 3 QUORUM: 2 (Enabled) TOTAL PVs: 1 VG DESCRIPTORS: 2 STALE PVs: 0 STALE PPs: 0 ACTIVE PVs: 1 AUTO ON: no Concurrent: Enhanced-Capable Auto-Concurrent: Disabled VG Mode: Concurrent Node ID: 1 Active Nodes: 2 MAX PPs per VG: 2097152 MAX PVs: 1024 LTG size (Dynamic): 1024 kilobyte(s) AUTO SYNC: no HOT SPARE: no BB POLICY: relocatable MIRROR POOL STRICT: off PV RESTRICTION: none INFINITE RETRY: no 위 RAWVG 정보를 보면 PP SIZE 가 64 mb 로 확인 됩니다.\n그럼 10 GB 의 Raw Device 를 만들기 위해서는\n10240 = 64 * 160 160 개의 PP 가 필요합니다.\n용량 산정이 완료가 되었다면, 아래 명령을 통해 Raw Device 를 생성합니다.\nmklv -y rawlv1 -t raw RAWVG 160 mklv -y rawlv2 -t raw RAWVG 160 mklv -y rawlv3 -t raw RAWVG 160 ... ","permalink":"https://chhanz88.github.io/post/2017-05-25-aixasmraw/","summary":"Create a raw Logical Volume for Oracle ASM On AIX  Raw Device 의 용량 산정은 VG 의 PP Size 를 기준으로 합니다.\n# lsvg RAWVG VOLUME GROUP: RAWVG VG IDENTIFIER: 00c25b6700004c00000001399f63219x VG STATE: active PP SIZE: 64 megabyte(s) VG PERMISSION: read/write TOTAL PPs: 30 (1920 megabytes) MAX LVs: 4096 FREE PPs: 6 (384 megabytes) LVs: 3 USED PPs: 24 (1536 megabytes) OPEN LVs: 3 QUORUM: 2 (Enabled) TOTAL PVs: 1 VG DESCRIPTORS: 2 STALE PVs: 0 STALE PPs: 0 ACTIVE PVs: 1 AUTO ON: no Concurrent: Enhanced-Capable Auto-Concurrent: Disabled VG Mode: Concurrent Node ID: 1 Active Nodes: 2 MAX PPs per VG: 2097152 MAX PVs: 1024 LTG size (Dynamic): 1024 kilobyte(s) AUTO SYNC: no HOT SPARE: no BB POLICY: relocatable MIRROR POOL STRICT: off PV RESTRICTION: none INFINITE RETRY: no 위 RAWVG 정보를 보면 PP SIZE 가 64 mb 로 확인 됩니다.","title":"[AIX] Create a raw Logical Volume for Oracle ASM On AIX"},{"content":"Could not load program: Cannot run a 64-bit program until the 64-bit environment has been configured. aix maintenance mode 로 부팅 진행 후, shell 환경에서 아래와 같이 Error 가 발생하면서\n일부 명령어들이 정상 작동 하지 않습니다.\ncf. Booting AIX into Maintenance Mode\nError 내용 # ls Could not load program ls: Cannot run a 64-bit program until the 64-bit environment has been configured. See the system administrator. 해결 방법 # /etc/methods/cfg64 해결 이후 # ls test.log test.sh .profile ... \u0026lt; 하략 \u0026gt; ","permalink":"https://chhanz88.github.io/post/2017-01-16-aixerrorcfg64/","summary":"Could not load program: Cannot run a 64-bit program until the 64-bit environment has been configured. aix maintenance mode 로 부팅 진행 후, shell 환경에서 아래와 같이 Error 가 발생하면서\n일부 명령어들이 정상 작동 하지 않습니다.\ncf. Booting AIX into Maintenance Mode\nError 내용 # ls Could not load program ls: Cannot run a 64-bit program until the 64-bit environment has been configured. See the system administrator. 해결 방법 # /etc/methods/cfg64 해결 이후 # ls test.","title":"[AIX] Could not load program: Cannot run a 64-bit program until the 64-bit environment has been configured."},{"content":"SAN Switch - 운영 매뉴얼  SAN Switch 의 설정을 위해 주로 GUI - Web Console 을 접속합니다.\n하지만 장비 및 접속 PC 에 따라 JAVA Version 에 대한 제약 사항으로 인해 접속이 안되는 경우가 많습니다.\n그리하여 telnet , ssh 를 통한 CLI 접근을 통해 설정을 하면 편리하게 SAN Switch 설정을 할 수 있습니다.\n아래 명령들을 통해 SAN Switch 설정을 배워봅시다.\nSwitch configuration  Switch name 변경 switch_name \u0026gt; switchname \u0026#34;san_sw_new_name\u0026#34; Switch Domain ID 변경 Domain ID는 fabric내에서의 switch 구별을 위한 것으로 유일한 ID를 부여해야 합니다.\nswitch_name\u0026gt; switchdisable switch_name\u0026gt; configure Configure… Fabric parameters (yes, y, no, n): [no] y Domain: (1..239) [1] \u0026lt;new_id\u0026gt; \u0026lt;\u0026lt;\u0026lt; 신규 ID 입력 R_A_TOV: (4000..120000) [10000] E_D_TOV: (1000..5000) [2000] Data field size: (256..2112) [2112] Sequence Level Switching: (0..1) [0] Disable Device Probing: (0..1) [0] Suppress Class F Traffic: (0..1) [0] VC Encoded Address Mode: (0..1) [0] Per-frame Route Priority: (0..1) [0] Long Distance Fabric: (0..1) [0] BB credit: (1..16) [16] Virtual Channel parameters (yes, y, no, n): [no] Zoning Operation parameters (yes, y, no, n): [no] RSCN Transmission Mode (yes, y, no, n): [no] NS Operation Parameters (yes, y, no, n): [no] Arbitrated Loop parameters (yes, y, no, n): [no] System services (yes, y, no, n): [no] Portlog events enable (yes, y, no, n): [no] WARNING: The domain ID is changed. The port level zoning may be affected. done. switch_name\u0026gt; switchenable 위와 같이 설정을 하며, 주로 Domain ID 를 제외한 나머지 설정은 Enter 를 입력 하여 기본값으로 설정합니다.\nSwitch IP 변경 SAN Switch 의 기본 접속 IP 는 10.77.77.77 입니다.\n변경을 위해 아래와 같이 설정하여 변경을 합니다.\nswitch_name\u0026gt; ipaddrset Ethernet IP Address [10.77.77.77]: 192.168.0.23 Ethernet Subnetmask [255.255.255.0]: 255.255.255.0 Fibre Channel IP Address [0.0.0.0]: Fibre Channel Subnetmask [0.0.0.0]: Committing configuration...Done. switch_name\u0026gt; ipaddrshow ipaddrshow 명령을 통해 IP 설정을 확인 할 수 있습니다.\nPort Configuration  Port Speed 설정 설정 가능한 Port Speed 는 아래와 같습니다.\n 0 : Auto\n1 : 1G\n2 : 2G\n4 : 4G\n8 : 8G\n16 : 16G (16 G 지원 Switch)\n switch_name\u0026gt; portcfgspeed \u0026lt;port 번호\u0026gt; \u0026lt;port speed\u0026gt; Port 설정 확인 switch_name\u0026gt; portcfgshow Zone Configuration  Alias Switch 의 Port 번호 혹은 WWWN 을 알기쉽게 alias 로 지정하여, zone member 확인을 쉽게 합니다.\n Alias 생성  switch_name\u0026gt; alicreate \u0026lt;\u0026#34;alias name\u0026#34;\u0026gt;,\u0026lt;\u0026#34;member_1;…;member_n\u0026#34;\u0026gt; switch_name\u0026gt; alicreate \u0026#34;test_fc0\u0026#34;,\u0026#34;1,0\u0026#34; switch_name\u0026gt; alicreate \u0026#34;test_fc0\u0026#34;,\u0026#34;10:00:00:00:C9:4E:03:76\u0026#34;  Port Zoning : switch_name\u0026gt; alicreate \u0026ldquo;test_fc0\u0026rdquo;,\u0026ldquo;1,0\u0026rdquo;\nWWWN Zoning : switch_name\u0026gt; alicreate \u0026ldquo;test_fc0\u0026rdquo;,\u0026ldquo;10:00:00:00:C9:4E:03:76\u0026rdquo;\n  Alias 삭제  switch_name\u0026gt; alidelete \u0026lt;\u0026#34;alias name\u0026#34;\u0026gt; Zone Zone 은 일종의 Network Switch 의 VLAN 과 비슷한 역할을 하는 기능이며, member 들 사이의 통신을 제한 하는 기능입니다.\n Zone 생성  switch_name\u0026gt; zonecreate \u0026lt;\u0026#34;zone name\u0026#34;\u0026gt;,\u0026lt;\u0026#34;member_1;…;member_n\u0026#34;\u0026gt; switch_name\u0026gt; zonecreate \u0026#34;test_fc0__stg_fc1\u0026#34;,\u0026#34;test_fc0;stg_fc1\u0026#34; Zone 생성할 때 alias 대신 Port 번호 및 WWWN 을 바로 입력을 할 수 있습니다.\n하지만 관리의 편의를 위해 alias 를 사용하는 것을 권장합니다.\n Zone 삭제  switch_name\u0026gt; zonedelete \u0026lt;\u0026#34;zone name\u0026#34;\u0026gt; switch_name\u0026gt; zonedelete \u0026#34;test_fc0__stg_fc1\u0026#34; Config 적용 Zone 을 적용하기 위해서는 Config Group 을 생성하여 Zone 을 Group 에 추가 해야합니다.\n Config Group 생성 및 Zone 추가  switch_name\u0026gt; cfgcreate \u0026lt;\u0026#34;group name\u0026#34;\u0026gt;,\u0026lt;\u0026#34;member_1;…;member_n\u0026gt; switch_name\u0026gt; cfgcreate \u0026#34;test_zone\u0026#34;,\u0026#34;test_fc0__stg_fc1\u0026#34;  생성 되있는 Group 에 Zone 추가  switch_name\u0026gt; cfgadd \u0026lt;\u0026#34;group name\u0026#34;\u0026gt;,\u0026lt;\u0026#34;member_1;…;member_n\u0026gt; switch_name\u0026gt; cfgadd \u0026#34;test_zone\u0026#34;,\u0026#34;test_fc1__stg_fc2\u0026#34;  Zone Config 적용  switch_name\u0026gt; cfgenable \u0026lt;\u0026#34;group name\u0026#34;\u0026gt; switch_name\u0026gt; cfgenable \u0026#34;test_zone\u0026#34;  Zone Config 해제 및 삭제  switch_name\u0026gt; cfgdisable \u0026lt;\u0026#34;group name\u0026#34;\u0026gt; switch_name\u0026gt; cfgdisable \u0026#34;test_zone\u0026#34; switch_name\u0026gt; cfgdelete \u0026lt;\u0026#34;group name\u0026#34;\u0026gt; switch_name\u0026gt; cfgdelete \u0026#34;test_zone\u0026#34;  Zone Configuration 확인(예제)  zoneshow 명령을 통해 현재 Switch 에 설정된 Config 를 확인 할 수 있습니다.\nIBM_2498_B24:admin\u0026gt; zoneshow Defined configuration: cfg: ZONE_CFG Server1_fcs0__V7K3_A1; Server1_fcs0__V7K3_B1; Server1_fcs0__V7K3_A2; Server1_fcs0__V7K3_B2; Server2_fcs0__V7K3_A1; Server2_fcs0__V7K3_B1; Server2_fcs0__V7K3_A2; Server2_fcs0__V7K3_B2; Server3_fcs0__V7K4_A1; Server3_fcs0__V7K4_B1; Server3_fcs0__V7K4_A2; Server3_fcs0__V7K4_B2; Server4_fcs0__V7K4_A1; Server4_fcs0__V7K4_B1; Server4_fcs0__V7K4_A2; Server4_fcs0__V7K4_B2; Server5_fcs0__V7K5_A1; Server5_fcs0__V7K5_B1; Server5_fcs0__V7K5_A2; Server5_fcs0__V7K5_B2; Server6_fcs0__V7K5_A1; Server6_fcs0__V7K5_B1; Server6_fcs0__V7K5_A2; Server6_fcs0__V7K5_B2 zone: Server1_fcs0__V7K3_A1 Server1_fcs0; V7K3_A1 zone: Server1_fcs0__V7K3_A2 Server1_fcs0; V7K3_A2 zone: Server1_fcs0__V7K3_B1 Server1_fcs0; V7K3_B1 zone: Server1_fcs0__V7K3_B2 Server1_fcs0; V7K3_B2 zone: Server2_fcs0__V7K3_A1 Server2_fcs0; V7K3_A1 zone: Server2_fcs0__V7K3_A2 Server2_fcs0; V7K3_A2 .... 중략 .... alias: Server1_fcs0 1,12 alias: Server2_fcs0 1,13 alias: Server3_fcs0 1,14 alias: Server4_fcs0 1,15 alias: Server5_fcs0 1,16 alias: Server6_fcs0 1,17 alias: V7K3_A1 1,0 alias: V7K3_A2 1,2 alias: V7K3_B1 1,1 alias: V7K3_B2 1,3 Switch 정보 확인  Switch 의 모든 Port 의 연결 상태, WWWN 정보를 확인 할 수 있습니다.\n예제) IBM_2498_B24:admin\u0026gt; switchshow switchName: IBM_2498_B24 switchType: 71.2 switchState: Online switchMode: Native switchRole: Principal switchDomain: 1 switchId: 000000 switchWwn: 00:00:00:aa:aa:aa:aa:aa zoning: ON (ZONE_CFG) switchBeacon: OFF Index Port Address Media Speed State Proto ============================================== 0 0 010000 id N8 Online FC F-Port a0:01:07:00:04:00:00:00 1 1 010100 id N8 Online FC F-Port a0:01:07:00:04:00:00:00 2 2 010200 id N8 Online FC F-Port a0:01:07:00:04:00:00:00 3 3 010300 id N8 Online FC F-Port a0:01:07:00:04:00:00:00 4 4 010400 id N8 Online FC F-Port a0:01:07:00:04:00:00:00 5 5 010500 id N8 Online FC F-Port a0:01:07:00:04:00:00:00 6 6 010600 id N8 Online FC F-Port a0:01:07:00:04:00:00:00 7 7 010700 id N8 Online FC F-Port a0:01:07:00:04:00:00:00 8 8 010800 id N8 Online FC F-Port a0:01:07:00:04:00:00:00 9 9 010900 id N8 Online FC F-Port a0:01:07:00:04:00:00:00 10 10 010a00 id N8 Online FC F-Port a0:01:07:00:04:00:00:00 11 11 010b00 id N8 Online FC F-Port v0:01:07:00:04:20:20:20 12 12 010c00 id N8 Online FC F-Port v0:01:07:00:00:20:20:20 13 13 010d00 id N8 Online FC F-Port v0:01:07:00:00:20:20:20 14 14 010e00 id N8 Online FC F-Port v0:01:07:00:00:20:20:20 15 15 010f00 id N8 Online FC F-Port v0:01:07:00:00:20:20:20 16 16 011000 id N8 Online FC F-Port v0:01:07:00:00:20:20:20 17 17 011100 id N8 Online FC F-Port v0:01:07:00:00:20:20:20 18 18 011200 id N8 No_Light FC 19 19 011300 id N8 No_Light FC 20 20 011400 id N8 No_Light FC 21 21 011500 id N8 No_Light FC 22 22 011600 id N8 No_Light FC 23 23 011700 id N8 No_Light FC 위 명령어를 이용하면 쉽게 SAN Switch Configuration 이 가능합니다.\n","permalink":"https://chhanz88.github.io/post/2016-05-13-sanswitchops/","summary":"SAN Switch - 운영 매뉴얼  SAN Switch 의 설정을 위해 주로 GUI - Web Console 을 접속합니다.\n하지만 장비 및 접속 PC 에 따라 JAVA Version 에 대한 제약 사항으로 인해 접속이 안되는 경우가 많습니다.\n그리하여 telnet , ssh 를 통한 CLI 접근을 통해 설정을 하면 편리하게 SAN Switch 설정을 할 수 있습니다.\n아래 명령들을 통해 SAN Switch 설정을 배워봅시다.\nSwitch configuration  Switch name 변경 switch_name \u0026gt; switchname \u0026#34;san_sw_new_name\u0026#34; Switch Domain ID 변경 Domain ID는 fabric내에서의 switch 구별을 위한 것으로 유일한 ID를 부여해야 합니다.","title":"SAN Switch - 운영 매뉴얼"},{"content":"[AIX] RDX Device Backup Guide  AIX RDX Device Backup Guide 입니다.\n(RDX 를 이용한 백업 방법입니다.)\n2016년 프로젝트 참여하면서 작성한 가이드입니다.\n 작성일 : 2016.02.15 배포일 : 2019.01.11  ","permalink":"https://chhanz88.github.io/post/2016-02-15-aix-rdx-drive-backup-mksysb/","summary":"[AIX] RDX Device Backup Guide  AIX RDX Device Backup Guide 입니다.\n(RDX 를 이용한 백업 방법입니다.)\n2016년 프로젝트 참여하면서 작성한 가이드입니다.\n 작성일 : 2016.02.15 배포일 : 2019.01.11  ","title":"[AIX] RDX Device Backup Guide"},{"content":"Force remove cluster - PowerHA 7.1  HACMP 7.1 에서 CAAVG 가 삭제가 안되면서, Cluster 를 제거 할 수 없는 경우가 있습니다.\n해당 이슈가 발생 할 경우, 아래와 같이 진행 하면 됩니다.\n# lspv | grep caavg hdiskpower426 000000000r508909 caavg_private active There is caavg so customer failed to create new HA cluster. To remove CAA cluster, run below command at both nodes. # export CAA_FORCE_ENABLED=1; rmcluster -fr hdiskpower426 \u0026lt;\u0026lt;== hdisk no. which has CAAVG at each node. 만약 위 방법으로 Cluster 제거가 안되었다면,\n1. # dd if=/dev/zero of=/dev/hdiskpowerXXX bs=1024k count=100 (from one of the nodes. ex)node1 ) 2. reboot both nodes. 주로 첫번째 방법으로 해결되었습니다. :)\n","permalink":"https://chhanz88.github.io/post/2014-07-25-hacmpforceremove/","summary":"Force remove cluster - PowerHA 7.1  HACMP 7.1 에서 CAAVG 가 삭제가 안되면서, Cluster 를 제거 할 수 없는 경우가 있습니다.\n해당 이슈가 발생 할 경우, 아래와 같이 진행 하면 됩니다.\n# lspv | grep caavg hdiskpower426 000000000r508909 caavg_private active There is caavg so customer failed to create new HA cluster. To remove CAA cluster, run below command at both nodes. # export CAA_FORCE_ENABLED=1; rmcluster -fr hdiskpower426 \u0026lt;\u0026lt;== hdisk no.","title":"[HACMP71] Force remove cluster"},{"content":"LVM 이란?  사원 시절 작성한 문서입니다.\n  작성일 : 2014.01.12 배포일 : 2019.01.10  ","permalink":"https://chhanz88.github.io/post/2014-01-12-aix-lvm-informarion/","summary":"LVM 이란?  사원 시절 작성한 문서입니다.\n  작성일 : 2014.01.12 배포일 : 2019.01.10  ","title":"[AIX] LVM 이란?"},{"content":"[Windows] MSCS Cluster 구성 메뉴얼  사원 시절, 작성한 Windows MSCS 구성 메뉴얼 입니다.\n  작성일: 2013-04-27 배포일: 2019-01-10  ","permalink":"https://chhanz88.github.io/post/2013-04-27-windows-mscs-cluster/","summary":"[Windows] MSCS Cluster 구성 메뉴얼  사원 시절, 작성한 Windows MSCS 구성 메뉴얼 입니다.\n  작성일: 2013-04-27 배포일: 2019-01-10  ","title":"[Windows] MSCS Cluster 구성 메뉴얼"},{"content":"JFS 와 JFS2 차이점  JFS2 는 AIX V4.3. 에서 소개되었던 JFS 의 upgrade version\nJFS 와 JFS2 는 모두 AIX 에 기본적으로 탑재되어 있습니다. 다만 POWER system 에서는 JFS/JFS2 를 모두 사용할 수 있지만, IA64 system 에서는 JFS2 만 사용할 수 있습니다.\nJFS vs JFS2     Function JFS2 JFS     Fragments/Block Size 512~4096 Block Size 512~4096 Block Size   Architectural Maximum File (만들 수 있는 최대 File Size) 1PB(**1) 64GB   Architectural Maximum File System Size (만들 수 있는 최대 파일 시스템 Size) 4PB 1TB(**2)   Maximum File Size Tested (최대 파일 크기 테스트) 1TB 64GB   Maximum File System Size (최대 파일 시스템 크기) 1TB 1TB   Number of Inodes (Inodes 수) Dynamic, Limited by disk space Fixed, set at file system creation   Directory Organization (디렉토리 조직) B-tree Linear   Compresssion (압축) No Yes   Default Ownership at Creation(생성 시 기본 소유권) root.system sys.sys   SGID of Default File Mode(기본 파일 모드 SGID) SGID=off SGID=on   Quotas No Yes   Available on IA64 Architecture(IA64 아키텍처에서 사용 가능) Yes No   Available on POWER Architecture (POWER 아키텍처에서 사용 가능) Yes Yes    (**1) PB (PetaBytes) = 1,048,576 GB (**2) TB (TeraBytes) = 1,024 GB cf) SGID = Linux SetUID 와 같은 기능\nJFS2 의 대표적인 몇 가지 특징   Extent based addressing structure ( 범위 기반 주소 지정 구조 ) Dynamic disk inode allocation ( 동적 디스크 inode 할당 ) Directory organization ( 디렉토리 조직 ) On-line file system space defragmentation ( 온라인 파일 시스템 공간 조각 모음 ) Variable block size ( 가변 블록 크기 )  JFS 와 JFS2 의 addressing 구조에 많은 차이가 있습니다.\nJFS2 의 addressing structure 를 살펴보면, block allocation policy 가 변화하면서 좀 더 효율적이고, 확장성이 뛰어나면, compact 한 구조를 가지게 되있습니다. 특히 JFS2 에서는 더 이상 필요 없는 공간을 free 시켜서, 다른 공간에 할당하는 inode의 dynamic allocation 이 가능합니다. 기존의 JFS 는 file system 을 생성할 때 inode 가 고정되지만, JFS2 에서는 사용중인 file system 의 inode 를 변화시킬 수 있다.\n또 현재 mount 되어 사용 중인 file system 을 defragmentation(조각 모음) 이 가능합니다. File system 의 남은 공간을 다시 fragement 하면 I/O 가 그만큼 더 빨라질 수 있고, 공간 효율도 더 높일 수 있습니다.\n이와 같은 이유로 JFS2 에서는 “Large File enabled System ( 큰 용량의 파일 허용 )” 이라는 옵션이 없어졌다. JFS2 에는 두 가지의 directory 구조를 사용합니다. 작은 directory 와 큰 directory 에 사용하는 directory 구조가 다른데, 큰 directory 구조에는 기존의 linear 가 아닌 B-tree 형태를 사용합니다. 이 B-tree 구조는 좀 더 빠른 operation(directory look up, delete, insert\u0026hellip;) 이 가능합니다.\nJFS2 가 512, 1024, 2048, 4096 byte 의 block size 를 지원하는 것은 JFS 와 같습니다. Block size 가 작으면, disk 사용률은 높아지는 대신에, fragment(조각모음)율이 높아져서 path(경로) 가 길어지고, block size 가 커지면, path 가 짧기 때문에 performance(작업)는 좋아지지만, 디스크 사용률은 약간 낮아집니다. 이 밖에도 performance 를 높이기 위해서 vnode cache 가 NFS 에 추가되고, File system cache 가 좀 더 확장된 면이 있습니다.\n","permalink":"https://chhanz88.github.io/post/2012-09-06-aixjfsvsjfs2/","summary":"JFS 와 JFS2 차이점  JFS2 는 AIX V4.3. 에서 소개되었던 JFS 의 upgrade version\nJFS 와 JFS2 는 모두 AIX 에 기본적으로 탑재되어 있습니다. 다만 POWER system 에서는 JFS/JFS2 를 모두 사용할 수 있지만, IA64 system 에서는 JFS2 만 사용할 수 있습니다.\nJFS vs JFS2     Function JFS2 JFS     Fragments/Block Size 512~4096 Block Size 512~4096 Block Size   Architectural Maximum File (만들 수 있는 최대 File Size) 1PB(**1) 64GB   Architectural Maximum File System Size (만들 수 있는 최대 파일 시스템 Size) 4PB 1TB(**2)   Maximum File Size Tested (최대 파일 크기 테스트) 1TB 64GB   Maximum File System Size (최대 파일 시스템 크기) 1TB 1TB   Number of Inodes (Inodes 수) Dynamic, Limited by disk space Fixed, set at file system creation   Directory Organization (디렉토리 조직) B-tree Linear   Compresssion (압축) No Yes   Default Ownership at Creation(생성 시 기본 소유권) root.","title":"[AIX] JFS vs JFS2"},{"content":"Mirror VG, 한개의 PV 로 alt_disk_install 하는 방법  Mirror 된 VG 를 alt_disk_install 을 진행하면 target PV 는 2EA 가 필요합니다.\n따라서 Mirror 된 VG 는 한개의 target PV 에 하기 위해서는 -i 옵션을 사용합니다.\nimage.data 수정  -i 옵션에 사용될 image.data 를 수정합니다.\n# lspv hdisk0 000aaaa120011111 rootvg active hdisk1 000aaaa120222222 rootvg active hdisk2 none None # vi /image.data ## lv_data의 항목중 * LV_SOURCE_DISK_LIST= hdisk0 hdisk1 -\u0026gt; hdisk0로 변경 * COPIES= 2 -\u0026gt; 1로 변경 * PP= 2 -\u0026gt; 1로 변경 위와 같이 Source PV 하나를 제거하고, LV의 COPY 수량, LV 용량을 수정합니다.\nalt_disk_install 진행  # alt_disk_install -COB -i /image.data hdisk2 ","permalink":"https://chhanz88.github.io/post/2012-09-06-aixmirroraltdisk/","summary":"Mirror VG, 한개의 PV 로 alt_disk_install 하는 방법  Mirror 된 VG 를 alt_disk_install 을 진행하면 target PV 는 2EA 가 필요합니다.\n따라서 Mirror 된 VG 는 한개의 target PV 에 하기 위해서는 -i 옵션을 사용합니다.\nimage.data 수정  -i 옵션에 사용될 image.data 를 수정합니다.\n# lspv hdisk0 000aaaa120011111 rootvg active hdisk1 000aaaa120222222 rootvg active hdisk2 none None # vi /image.data ## lv_data의 항목중 * LV_SOURCE_DISK_LIST= hdisk0 hdisk1 -\u0026gt; hdisk0로 변경 * COPIES= 2 -\u0026gt; 1로 변경 * PP= 2 -\u0026gt; 1로 변경 위와 같이 Source PV 하나를 제거하고, LV의 COPY 수량, LV 용량을 수정합니다.","title":"[AIX] Mirror 상태인 VG, alt_disk_install 하는 법"},{"content":"Mirror Disk 교체  Mirroring 으로 구성된 rootvg 에서 장애난 DISK 교체 절차는 아래와 같습니다.\n 장애 디스크 확인  # errpt 613E5F38 1212232603 P H LVDD I/O ERROR DETECTED BY LVM A668F553 1212092003 P H hdisk1 DISK OPERATION ERROR bootlist 확인  # bootlist -om normal bootlist 재설정  # unmirrorvg rootvg hdisk1 Unmirror VG 수행  # unmirrorvg rootvg hdisk1 각 DISK 의 LV 상태 확인  # lspv -l hdisk0 # lspv -l hdisk1 제거가 안되거나, Mirroring 이 아닌 LV 가 남아 있는 경우  # migratepv -l hd6 hdisk1 hdisk0 # lspv -l hdisk0 // 확인 # lspv -l hdisk1 // 확인 VG 에서 DISK 제거  # reducevg rootvg hdisk1 시스템에서 DISK 제거  rmdev -Rdl hdisk1 bosboot 수행  # bosboot -ad hdisk0 신규 디스크 추가(물리 DISK 교체 및 장착)  # diag 혹은 # cfgmgr -v VG 에 신규 DISK 추가  # extendvg rootvg hdisk1 Mirror VG 수행  # mirrorvg -S rootvg hdisk0 hdisk1 rootvg Mirror 확인  # lsvg -l rootvg bosboot 수행  # bosboot -ad hdisk0 bootlist 신규 등록  # bootlist -om normal hdisk0 hdisk1 bootlist 확인  # bootlist -om normal ","permalink":"https://chhanz88.github.io/post/2012-08-26-aix-mirror-disk-change/","summary":"Mirror Disk 교체  Mirroring 으로 구성된 rootvg 에서 장애난 DISK 교체 절차는 아래와 같습니다.\n 장애 디스크 확인  # errpt 613E5F38 1212232603 P H LVDD I/O ERROR DETECTED BY LVM A668F553 1212092003 P H hdisk1 DISK OPERATION ERROR bootlist 확인  # bootlist -om normal bootlist 재설정  # unmirrorvg rootvg hdisk1 Unmirror VG 수행  # unmirrorvg rootvg hdisk1 각 DISK 의 LV 상태 확인  # lspv -l hdisk0 # lspv -l hdisk1 제거가 안되거나, Mirroring 이 아닌 LV 가 남아 있는 경우  # migratepv -l hd6 hdisk1 hdisk0 # lspv -l hdisk0 // 확인 # lspv -l hdisk1 // 확인 VG 에서 DISK 제거  # reducevg rootvg hdisk1 시스템에서 DISK 제거  rmdev -Rdl hdisk1 bosboot 수행  # bosboot -ad hdisk0 신규 디스크 추가(물리 DISK 교체 및 장착)  # diag 혹은 # cfgmgr -v VG 에 신규 DISK 추가  # extendvg rootvg hdisk1 Mirror VG 수행  # mirrorvg -S rootvg hdisk0 hdisk1 rootvg Mirror 확인  # lsvg -l rootvg bosboot 수행  # bosboot -ad hdisk0 bootlist 신규 등록  # bootlist -om normal hdisk0 hdisk1 bootlist 확인  # bootlist -om normal ","title":"[AIX] Mirror Disk 교체"},{"content":"[AIX] Mount CD/DVD \u0026amp; ISO Image  CD 혹은 DVD 를 CLI Command 로 Mount : # mount -V cdrfs -o ro /dev/cd0 /mnt ISO Image 를 CLI Command 로 Mount : # loopmount -i aix-6100-09-02-icd.iso -o \u0026#34;-V cdrfs -o ro\u0026#34; -m /mnt ","permalink":"https://chhanz88.github.io/post/2012-08-24-aix-mount-cd-iso/","summary":"[AIX] Mount CD/DVD \u0026amp; ISO Image  CD 혹은 DVD 를 CLI Command 로 Mount : # mount -V cdrfs -o ro /dev/cd0 /mnt ISO Image 를 CLI Command 로 Mount : # loopmount -i aix-6100-09-02-icd.iso -o \u0026#34;-V cdrfs -o ro\u0026#34; -m /mnt ","title":"[AIX] Mount CD/DVD \u0026 ISO Image"},{"content":"  Cheolhee Han RED HAT CERTIFIED ARCHITECT IN INFRASTRUCTURE CKA: Certified Kubernetes Administrator Cloud Engineer (han0495@gmail.com) \u0026nbsp;   안녕하세요. chhanz 블로그에 방문해주셔서 감사합니다. :)\n현재 Cloud Engineer 로 Cloud Infrastructure 를 설계하고 구축/운영 하는 업무를 하고 있습니다.\n보유기술  Red Hat Certified Architect in Infrastructure Level III as of 06/2021   Ansible 2.3, Ansible 2.8 GFS2 for Red Hat Enterprise Linux 7 Red Hat Enterprise Linux 7, Red Hat Enterprise Linux 8 Red Hat Enterprise Linux High Availability Add-On 7 Red Hat Certified Specialist in Linux Performance Tuning Red Hat Certified Specialist in Virtualization Red Hat Enterprise Linux OpenStack Platform 6.0 Red Hat OpenStack Platform 13 Red Hat OpenShift Container Platform 3.5, Red Hat OpenShift Container Platform 4.5 Red Hat Ceph Storage 3.0\n@ Go to Red Hat Certification Central     Opensource   CentOS KVM Pacemaker Spacewalk OpenStack Docker OpenShift Kubernetes Ansible oVirt   UNIX   IBM AIX IBM PowerVM (Virtualization Solution) IBM PowerHA (High-Availability Solution) IBM GPFS (General Parallel File System)   H/W   IBM Power System IBM X System Product IBM V Series Storage IBM DS Series Storage(Midrange Storage) Lenovo System Product HP Server Product Dell Server Product Brocade SAN Switch IBM Tape Library    ","permalink":"https://chhanz88.github.io/about/","summary":"About chhanz","title":"About"},{"content":"Recommend Links  chhanz: https://chhanz.github.io/ Cyuu: http://cyuu.tistory.com/ Jacobbaek: http://jacobbaek.com/ Ondrej Faměra: https://www.famera.cz/blog/index.html  ","permalink":"https://chhanz88.github.io/links/","summary":"Recommend Links  chhanz: https://chhanz.github.io/ Cyuu: http://cyuu.tistory.com/ Jacobbaek: http://jacobbaek.com/ Ondrej Faměra: https://www.famera.cz/blog/index.html  ","title":"Links"}]